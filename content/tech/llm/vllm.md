---
title: "vLLM(1)"
date: 2024-05-05
lastmod: 2024-05-19
draft: ture
description: ""
tags: ["NLP", "Transformer", "LLM", "vLLM"]
series: ["vLLM"]
series_order: 1
# layout: "simple"
showDate: true
---

{{<katex>}}

Reference
- [vLLM核心技术 PagedAttention原理](https://mp.weixin.qq.com/s?__biz=Mzg2NjcwNjcxNQ==&mid=2247485614&idx=1&sn=5600ea665d942b7ff290caded1e2252f&chksm=ce47fcdaf93075cc4582bfb15eb822840c332d56122df5b59bd8722426d07ac5b097cf032ba4&scene=21#wechat_redirect)
- [大模型推理框架vLLM源码解析](https://www.53ai.com/news/qianyanjishu/1249.html)