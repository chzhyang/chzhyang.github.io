---
title: "奇绩创坛：Efficient Long-context Generation"
date: 2024-08-12
lastmod: 2024-08-13
draft: false
description: ""
tags: ["Diary", "LLM"]
# series: ["Diary"]
series_order: 1
# layout: "simple"
showDate: true
---
![Betty](https://s2.loli.net/2024/08/12/ahkGKNUMenZmAJH.jpg)

今天在奇绩创坛参加了 Betty 关于 Long Context 的讨论，越听越开心，如果不是场合不合适，已经拍案叫绝了，简直同道中人。

Betty Chen @CMU Infinite AI Lab 助理教授，近期几篇 paper 都围绕 Efficient Long Context Generation 进行算法和系统的 Co-design:

- [TriForce](https://arxiv.org/abs/2404.11912): Dynamic attention compression + Top-k sparse KV cache
- MagicPIG 解决了 TriForce 的两个缺陷:
    - 一个是使用 locality sensitive hashing (LSH，这个真不懂 :< ) 精确查找 top k sparse KV
    - 另一个是设计了 CPU+GPU 异构计算的 LLM System 来解决 Memory Limit 问题，私以为这是它**最大的亮点**。由于 CPU 与 GPU 的算力差距在千倍数量级，但 memory BW 差距仅在十倍数量级，而 Decoding Attention 的计算强度在 1 左右，Why not use cheap and infinite memory? 很自然地要在 CPU 上实现 Sparse Attention 计算

这次论坛的几个收获:

- Top-4：RAG 和 Long Context 并不冲突，虽然多层次 Retrieval 系统、Graph RAG 等能缓解在 long context 上逻辑提取能力不足的问题，但都没有从根本上解决 LLM 对深层次逻辑理解不足的问题，这件最难的事大概率还是要从 model 层面解决
- Top-3：LLM 要商业化落地，推理的算法和系统（软件、硬件）的协同设计是大势所趋。非常赞同 Betty 的观点：**AI 探索不应该被硬件牵着鼻子走，丧失 diversity 是难以想象的**。现有的 LLM system 需要从模型和硬件两个层面 co-design，不能让特定硬件成为 locker，一定要有人兼顾 GPU 之外的硬件，CPU、NPU、TPU等都有机会（期待 Google 和 Groq :-）
- Top-2：Memory, more memory and more cheap memory are all you need
- Top-1：看到通过美国学术体系和工业界培养出来的教授对 diversity 和 innovation 的坚持和鼓励，内心触动，很有力量，这样科学的传承，很难不向往吧 :) 