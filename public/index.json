
[{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/posts/","section":"Blogs","summary":"","title":"Blogs","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/tags/cuda/","section":"Tags","summary":"","title":"CUDA","type":"tags"},{"content":"[Todo]\n","date":"2024-05-24","externalUrl":null,"permalink":"/posts/cuda/cuda_optimize/","section":"Blogs","summary":"[Todo]","title":"CUDA Optimization","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/series/cuda-parallel-programming/","section":"Series","summary":"","title":"CUDA Parallel Programming","type":"series"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/posts/cuda/","section":"Blogs","summary":"","title":"CUDA Parallel Programming","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"Hi, welcome to my blog.\n","date":"2024-05-24","externalUrl":null,"permalink":"/","section":"Welcome to My Blog","summary":"Hi, welcome to my blog.","title":"Welcome to My Blog","type":"page"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/attention/","section":"Tags","summary":"","title":"Attention","type":"tags"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/series/attention-and-optimization/","section":"Series","summary":"","title":"Attention and Optimization","type":"series"},{"content":"[WIP]\nFlashAttention V2:\nåœ¨V1çš„åŸºç¡€ä¸Šå‡å°‘äº†éçŸ©é˜µä¹˜æ³•è¿ç®—çš„FLOPsã€‚ é€šè¿‡å¹¶è¡ŒåŒ–å’Œä»»åŠ¡åˆ†é…ä¼˜åŒ–æé«˜äº†è®¡ç®—é€Ÿåº¦å’ŒGPUåˆ©ç”¨ç‡ï¼Œæ€§èƒ½æå‡äº†2-3å€ã€‚ Flash-Decodingå€Ÿé‰´äº†FlashAttentionçš„ä¼˜ç‚¹ï¼Œå°†å¹¶è¡ŒåŒ–ç»´åº¦æ‰©å±•åˆ°keys/valuesåºåˆ—é•¿åº¦ï¼Œæé«˜äº†æ¨ç†é€Ÿåº¦ã€‚ Flash-Decodingå‡ ä¹ä¸ç”¨é¢å¤–å­˜å‚¨å¤§é‡æ•°æ®åˆ°å…¨å±€å†…å­˜ä¸­ï¼Œå‡å°‘äº†å†…å­˜å¼€é”€ã€‚ Flash-Decoding++é€šè¿‡å¼‚æ­¥softmaxå’Œç»Ÿä¸€æœ€å¤§å€¼ã€flat GEMMä¼˜åŒ–å’ŒåŒç¼“å†²ã€å¯å‘å¼æ•°æ®æµå’Œç¡¬ä»¶èµ„æºé€‚åº”ç­‰æ–¹æ³•è¿›ä¸€æ­¥æé«˜äº†LLMæ¨ç†çš„æ€§èƒ½ã€‚ ","date":"2024-05-23","externalUrl":null,"permalink":"/posts/llm/flash_attention_2/","section":"Blogs","summary":"[WIP] FlashAttention V2: åœ¨V1çš„åŸºç¡€ä¸Šå‡å°‘äº†éçŸ©é˜µä¹˜æ³•è¿ç®—","title":"Flash Attention V2","type":"posts"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/posts/llm/","section":"Blogs","summary":"","title":"LLM","type":"posts"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/nlp/","section":"Tags","summary":"","title":"NLP","type":"tags"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/transformer/","section":"Tags","summary":"","title":"Transformer","type":"tags"},{"content":"","date":"2024-05-21","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"2024-05-21","externalUrl":null,"permalink":"/tags/pytorch/","section":"Tags","summary":"","title":"Pytorch","type":"tags"},{"content":"","date":"2024-05-21","externalUrl":null,"permalink":"/series/pytorch/","section":"Series","summary":"","title":"PyTorch","type":"series"},{"content":"","date":"2024-05-21","externalUrl":null,"permalink":"/posts/pytorch/","section":"Blogs","summary":"","title":"PyTorch","type":"posts"},{"content":" PyTorch ä»£ç ç»“æ„ # PyTroch ä¸»è¦ç”±C10ã€ATenã€torchä¸‰å¤§éƒ¨åˆ†ç»„æˆï¼š\ntorch/Â ä¸‹åŒ…å« import å’Œä½¿ç”¨çš„ Python æ¨¡å— torch/csrc/Â åŒ…å«äº† PyTorch å‰ç«¯çš„ C++ ä»£ç åŠC++å‰ç«¯ä»£ç ã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒåŒ…å«äº† Python å’Œ C++ ä¹‹é—´è½¬æ¢çš„bindingä»£ç ï¼Œ autograd å¼•æ“å’Œ JIT ç¼–è¯‘å™¨ç­‰ã€‚ c10(CaffeÂ Tensor Library), åŒ…å« PyTorch çš„æ ¸å¿ƒæŠ½è±¡ï¼Œå­˜æ”¾æœ€åŸºç¡€çš„Tensoråº“ä»£ç ï¼ŒåŒ…æ‹¬ Tensor å’Œ Storage æ•°æ®ç»“æ„çš„å®é™…å®ç°ï¼Œå¯ä»¥è¿è¡Œåœ¨æœåŠ¡ç«¯å’Œç§»åŠ¨ç«¯ã€‚ æœ€å…·ä»£è¡¨æ€§çš„classæ˜¯ TensorImpl ï¼Œå®ç°äº†Tensorçš„æœ€åŸºç¡€æ¡†æ¶ã€‚ç»§æ‰¿è€…å’Œä½¿ç”¨è€…æœ‰ï¼š Variableçš„Variable::Impl SparseTensorImpl detail::make_tensor(storage_impl, CUDATensorId(), false) Tensor(c10::intrusive_ptr\u0026lt;TensorImpl, UndefinedTensorImpl\u0026gt; tensor_impl) c10::make_intrusive\u0026lt;at::TensorImpl, at::UndefinedTensorImpl\u0026gt; ATen(A Tensor library for C++11)ï¼ŒåŒ…å«å£°æ˜å’Œå®šä¹‰ Tensor è¿ç®—ç›¸å…³é€»è¾‘çš„ä»£ç ï¼Œæ˜¯å®ç°å¼ é‡è¿ç®—çš„ C++ åº“ï¼Œkernelä»£ç å¤§å¤šåœ¨è¿™é‡Œ åŒ…å« C++ å®ç°çš„nativeç®—å­å’Œ C å®ç°çš„legacyç®—å­(TH, THC, THNN, THCUNN) . aten/src/ATen/gen.py ç”¨æ¥åŠ¨æ€ç”Ÿæˆä¸€äº›ATenç›¸å…³çš„ä»£ç  PyTroch çš„ç¼–è¯‘è¿‡ç¨‹ # å…¥å£ setup.pyï¼› æå‰æ£€æŸ¥ä¾èµ–é¡¹ï¼› ä½¿ç”¨ cmake ç”Ÿæˆ Makefile Make: äº§ç”Ÿä¸­é—´æºæ–‡ä»¶ Make: ç¼–è¯‘ä¸‰æ–¹åº“ Make: ç”Ÿæˆé™æ€åº“ã€åŠ¨æ€åº“ã€å¯æ‰§è¡Œæ–‡ä»¶ Make: Copyæ–‡ä»¶åˆ°åˆé€‚è·¯å¾„ setuptools, build_py setuptools, build_ext setuptools, install_lib PyTorch å·¥ä½œæµå’Œè®¡ç®—å›¾ # PyTorch 1.0 æ•´ä½“å·¥ä½œæµï¼š\nä½¿ç”¨ imperative / eager çš„èŒƒå¼ï¼Œæ¯ä¸€è¡Œä»£ç éƒ½æ„å»ºä¸€ä¸ªå›¾ä½œä¸ºå®Œæ•´è®¡ç®—å›¾çš„ä¸€éƒ¨åˆ†ã€‚å³ä½¿å®Œæ•´çš„è®¡ç®—å›¾è¿˜æ²¡æœ‰å®Œæˆæ„å»ºï¼Œä¹Ÿå¯ä»¥ç‹¬ç«‹æ‰§è¡Œè¿™äº›ä½œä¸ºç»„ä»¶çš„å°è®¡ç®—å›¾ï¼Œè¿™ç§åŠ¨æ€è®¡ç®—å›¾è¢«ç§°ä¸ºdefine-by-run Eager æ¨¡å¼é€‚åˆå—åšåŸå‹ã€å®éªŒã€debugï¼ŒScript æ¨¡å¼(torch.jit)é€‚åˆåšä¼˜åŒ–ä¸éƒ¨ç½² åŠ¨æ€å›¾ # å‡è®¾PyTorchçš„autogradç³»ç»Ÿæ˜¯ä¸€ä¸ª graphï¼Œé‚£ä¹ˆæ¯ä¸ª Function å®ä¾‹å°±æ˜¯ graph ä¸­çš„èŠ‚ç‚¹ï¼Œå„ä¸ª Function å®ä¾‹ä¹‹é—´é€šè¿‡ Edge è¿æ¥ã€‚Edge æ˜¯ä¸ª structï¼Œ(Function, input_nr) ç»„åˆå¯ä»¥ä»£è¡¨ä¸€ä¸ª edge\nstruct Edge { ... std::shared_ptr\u0026lt;Function\u0026gt; function; uint32_t input_nr; }; Function çš„æˆå‘˜å˜é‡ next_edges_ å°±æ˜¯ä¸€ç»„ Edge å®ä¾‹ï¼Œä»£è¡¨å½“å‰Functionå®ä¾‹çš„è¿”å›å€¼è¦è¾“å‡ºåˆ°å“ªä¸ªFunction\nFunction çš„ input, ouput éƒ½æ˜¯ Variableå®ä¾‹ï¼Œå› æ­¤ï¼Œå½“ä¸€ä¸ª graph è¢«æ‰§è¡Œæ—¶ï¼ŒVariable å®ä¾‹å°±åœ¨è¿™äº› edge ä¹‹é—´æ¥æµåŠ¨ï¼Œä¼ è¾“ä¿¡æ¯\nFunction çš„æˆå‘˜å˜é‡ sequence numberï¼Œéšç€Functionå®ä¾‹çš„ä¸æ–­æ„å»ºè€Œå•è°ƒå¢é•¿\nJIT # Code/AST -\u0026gt; Parsing-\u0026gt; Checking -\u0026gt; Optimization -\u0026gt; Translation -\u0026gt; Execution\nJIT ä¸»è¦ä¼šè¾“å…¥ä»£ç æˆ– Python çš„æŠ½è±¡å¥æ³•æ ‘ï¼ˆASTï¼‰ï¼Œå…¶ä¸­ AST ä¼šç”¨æ ‘ç»“æ„è¡¨å¾ Python æºä»£ç çš„å¥æ³•ç»“æ„ã€‚ Parsingå¯èƒ½æ˜¯è§£æå¥æ³•ç»“æ„å’Œè®¡ç®—å›¾ï¼Œç„¶åè¯­æ³•æ£€æµ‹æ¥è¿ç€ä»£ç ä¼˜åŒ–è¿‡ç¨‹ï¼Œæœ€ååªè¦ç¼–è¯‘å¹¶æ‰§è¡Œå°±å¯ä»¥ ä¼˜åŒ–è®¡ç®—å›¾ï¼Œå¦‚å±•å¼€å¾ªç¯ã€æŒ‡ä»¤è½¬æ¢ç­‰ æ‰§è¡Œï¼Œä¸ Python è§£é‡Šå™¨å¯ä»¥æ‰§è¡Œä»£ç ä¸€æ ·ï¼ŒPyTorch JIT è¿‡ç¨‹ä¸­ä¹Ÿæœ‰ä¸€ä¸ªè§£é‡Šå™¨æ‰§è¡Œä¸­é—´è¡¨å¾æŒ‡ä»¤ PyTorch ä» Python ä»£ç åˆ° kernel # PyTorch ä» Python ä»£ç åˆ° kernel çš„ä¸­é—´è¿‡ç¨‹ååˆ†å¤æ‚, åœ¨è¿›å…¥å†…æ ¸ä¹‹å‰ï¼Œæ‰€æœ‰ä»£ç éƒ½æ˜¯è‡ªåŠ¨ç”Ÿæˆçš„\nå‡è®¾è°ƒç”¨ torch.add()ï¼Œæµç¨‹å¦‚ä¸‹ï¼š\nPython åŸŸè½¬æ¢åˆ° C++ åŸŸï¼ˆPython å‚æ•°è§£æï¼‰ å¤„ç† VariableType dispatch å¤„ç† DeviceType/å¸ƒå±€ dispatch æ‰§è¡Œkernel(native kernel æˆ– TH kernel) ATen åŠ¨æ€ç”Ÿæˆçš„ä»£ç  # Typeç»§æ‰¿ä½“ç³»ï¼ŒåŒ…å«å¤´æ–‡ä»¶å’Œæºæ–‡ä»¶ Typeç»§æ‰¿ä½“ç³»æ˜¯è”ç³» Tensor op ä¸ legacy çš„ TH æˆ– native kernel çš„çº½å¸¦ Typeç»§æ‰¿ä½“ç³»ç»´æŠ¤äº†2/3çº§åˆ†å‘æœºåˆ¶ Declarations.yamlï¼Œä¼šè¢«Torchæ¨¡å—åŠ¨æ€ç”Ÿæˆä»£ç è°ƒç”¨ ç”Ÿæˆ Tensor ç±» ç”ŸæˆTypeå®¶æ—æ³¨å†Œåˆå§‹åŒ–çš„ä»£ç  ç”Ÿæˆ legacy çš„ TH/THC çš„kernelå£°æ˜ ç”Ÿæˆ native kernel çš„å£°æ˜ PyTroch Tensor # #åœ¨pythonä¸­å®šä¹‰äº†Parameterç±» class Parameter(torch.Tensor) #åœ¨pythonä¸­å®šä¹‰äº†torch.Tensorç±» class Tensor(torch._C._TensorBase) #åœ¨C++ä¸­å®šä¹‰äº†Variableç±» struct TORCH_API Variable : public at::Tensor //PyObject* Py_InitModule(char *name, PyMethodDef *methods) //åˆ›å»ºtorch._C Py_InitModule(\u0026#34;torch._C\u0026#34;, methods.data()ï¼‰ //åˆ›å»º torch._C._TensorBase PyModule_AddObject(module, \u0026#34;_TensorBase\u0026#34;, (PyObject *)\u0026amp;THPVariableType); Tensorè¿ç®— Dispatch æœºåˆ¶ä¸­çš„ Type ç»§æ‰¿ä½“ç³» # Typeç±»æ´¾ç”Ÿå‡ºäº†TypeExtendedInterfaceï¼ŒTypeExtendedInterfaceåˆæ´¾ç”Ÿäº†TypeDefaultã€‚TypeDefaultåˆæ´¾ç”Ÿäº†CUDATypeDefaultã€CPUTypeDefaultã€VariableTypeï¼ˆå®ç°äº†autogradï¼‰ã€UndefinedTypeç­‰ã€‚å…¶ä¸­ï¼Œæ ¹æ® density å’Œ scaler type çš„ä¸åŒï¼š\nCUDATypeDefaultæ´¾ç”Ÿäº†ï¼š\nCUDAIntType CUDAShortType SparseCUDACharType CUDADoubleType CUDAByteType CUDACharType SparseCUDAByteType CUDAFloatType SparseCUDALongType CUDALongType CUDAHalfType SparseCUDAShortType SparseCUDADoubleType SparseCUDAIntType SparseCUDAFloatType CPUTypeDefaultæ´¾ç”Ÿäº†ï¼š\nSparseCPUShortType CPUFloatType CPUHalfType CPUDoubleType CPUByteType SparseCPUFloatType SparseCPUIntType SparseCPUDoubleType CPUCharType SparseCPUByteType CPUIntType CPULongType SparseCPULongType SparseCPUCharType CPUShortType Typeç»§æ‰¿ä½“ç³»çš„ä½œç”¨\nPyTorch Kernel ç»„æˆ # Error checking, TORCH CHECK Output allocation Dtype dispatch Parallelization Data access æœªå®Œå¾…ç»­\u0026hellip;\n","date":"2024-05-21","externalUrl":null,"permalink":"/posts/pytorch/pytorch/","section":"Blogs","summary":"PyTorch ä»£ç ç»“æ„ # PyTroch ä¸»è¦ç”±C10ã€ATenã€t","title":"PyTorch Architecture","type":"posts"},{"content":"","date":"2024-05-20","externalUrl":null,"permalink":"/tags/paged-attention/","section":"Tags","summary":"","title":"Paged Attention","type":"tags"},{"content":" vLLM # vLLMæ˜¯ååæ€§èƒ½å“è¶Šçš„å¤§æ¨¡å‹æ¨ç†æ¡†æ¶ï¼ŒPagedAttentionæ˜¯vLLMæœ€å¤§çš„åˆ›æ–°ç‚¹ï¼š Efficient Memory Management for Large Language Model Serving with PagedAttention\nvLLMä¸­çš„attentionè®¡ç®—ï¼Œåœ¨æ¨ç†çš„prefillé˜¶æ®µ, ä½¿ç”¨ç¬¬ä¸‰æ–¹åº“xformersçš„ä¼˜åŒ–å®ç°ï¼Œdecodingé˜¶æ®µä½¿ç”¨ CUDA kernel å®ç°(csrc/attention/attention_kernels.cuï¼Œå¤§çº¦800å¤šè¡Œ)ã€‚\nAttentionè®¡ç®—æ—¶ä½¿ç”¨é¡µå¼ç®¡ç† KV Cache æ¥æé«˜å†…å­˜åˆ©ç”¨ç‡ï¼Œè¿›è€Œæé«˜ååé‡ã€‚\nPaged Attention(PA) # vLLMä¸­æœ‰ä¸¤ä¸ªç‰ˆæœ¬çš„ PAï¼Œå…¶ä¸­ï¼š\nV1 æºäº FasterTransformers çš„ MHAï¼Œé€‚ç”¨äº len(seq) \u0026lt; 8192 æˆ– num_seqs * num_heads \u0026gt; 512 çš„æƒ…å†µã€‚ V2 å‚è€ƒäº† Flash Decodingæ–¹å¼ï¼Œå¯¹ sequence çš„ç»´åº¦è¿›è¡Œåˆ‡åˆ†æ¥å¢åŠ å¹¶è¡Œç²’åº¦ Paged Attention V1 # Block table in PA\nä¸€ä¸ª req ä¸­åŒ…å«å¤šä¸ª seq æ—¶ï¼Œå¯ä»¥å…±äº«blocks\nPaged Attention V1 CUDA Kernel(vLLM) # csrc/attention/attention_kernels.cu\nDispatché€»è¾‘ï¼š\nCALL_KERNEL_LAUNCHER_BLOCK_SIZE æ ¹æ®å­˜å‚¨çš„kv blocksizeè¿›è¡Œæ´¾å‘ï¼Œåˆ†åˆ«æ˜¯ 8ï¼Œ 16ï¼Œ 32 LAUNCH_ATTENTION_KERNEL æ ¹æ®æ³¨æ„åŠ›å¤´å¤§å°HEADSIZEé™æ€æ´¾å‘ å¹¶è¡Œä»»åŠ¡çš„åˆ’åˆ†ï¼š\ndim3 grid(num_heads, num_seqsï¼Œ 1) dim3 block(NUM_THREADS), çº¿ç¨‹æ•°æ˜¯128ï¼Œæ¯ä¸ª block è´Ÿè´£å®Œæˆ output çŸ©é˜µä¸€è¡Œï¼ˆhead_sizeä¸ªå…ƒç´ ï¼‰ç»“æœçš„ attention è®¡ç®— block çš„çº¿ç¨‹åˆ’åˆ†ä¸ºè‹¥å¹²ä¸ª Warp, æ¯ä¸ª Warp çš„32ä¸ªçº¿ç¨‹åˆ’åˆ†ä¸º blk_size ä¸ª thread group Kernel è¾“å…¥å‚æ•°\nout[num_seqs, num_heads, head_size] q[num_seqs, num_heads, head_size] k_cache[num_blocks, num_kv_heads, head_size/x, block_size, x] # xè¡¨ç¤ºä¸€ä¸ªå‘é‡åŒ–çš„å¤§å°ï¼Œå¦‚float16 -\u0026gt; 16 / sizeof(float16) = 8 v_cache[num_blocks, num_kv_heads, head_size, block_size] head_mapping[num_heads] # ä½¿ç”¨MQA, GQAæ—¶çš„kv_head block_tables[num_seqs, max_num_blocks_per_seq] # ç»´æŠ¤å„ä¸ªQå¯¹åº”KVCacheçš„å“ªäº›block context_lens[num_seqs] # ç”¨äºå˜é•¿ num_headï¼š Q çš„ head æ•° num_kv_headsï¼šK, V çš„ head æ•°ï¼ŒMHA çš„ num_kv_heads = num_headï¼ŒGQAã€MQA çš„ num_kv_heads \u0026lt; num_head blk_size # block_sizeï¼Œæ¯ä¸ªpage blockå­˜å‚¨çš„å…ƒç´ æ•°é‡ï¼Œæ¯ä¸ªpageå­˜(blk_size, num_headï¼Œhead_size)ä¸ªKã€Vçš„å…ƒç´ \nKernel çš„å¸¸é‡å®šä¹‰ï¼š\nTHREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1) é€šè¿‡WARPSIZE / BLOCKSIZE å¾—åˆ°ä¸€ä¸ªthread_groupå¤§å°ã€‚æ³¨æ„è¿™é‡Œçš„BLOCKSIZEä¸æ˜¯cuda blocksizeï¼Œè€Œæ˜¯ä¸€ä¸ªkv blockçš„å¤§å°(é»˜è®¤å€¼16) NUM_TOKENS_PER_THREAD_GROUP = (BLOCK_SIZE + WARP_SIZE - 1) / - WARP_SIZE è¡¨ç¤ºæ¯ä¸ªthread_groupå¤„ç†å¤šå°‘ä¸ªtoken NUM_WARPS è¡¨ç¤ºä¸€ä¸ªthreadblockæœ‰å¤šå°‘ä¸ªwarp VEC_SIZE è¡¨ç¤ºå‘é‡åŒ–å¤§å°ï¼Œä¿è¯æ¯ä¸ªthread_groupä¸€æ¬¡æ€§è·å–16bytesï¼ŒMAX(16 / (THREAD_GROUP_SIZE * sizeof(scalar_t)), 1) NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE è¡¨ç¤ºæ¯ä¸ªthreadè¦è´Ÿè´£å¤šå°‘ä¸ªæ•°æ®è®¡ç®— NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE è¡¨ç¤ºæ¯ä¸ªthreadè´Ÿè´£çš„æ•°æ®ç»è¿‡å‘é‡åŒ–åï¼Œä¸€å…±æœ‰å¤šå°‘ä¸ªvec V_VEC_SIZE = MIN(16 / sizeof(scalar_t), BLOCK_SIZE) æ¯ä¸ªthreadä¸€æ¬¡æ€§è¯»å–16bytes NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZEã€‚å¯¹äºv_cache[head_size, block_size]ï¼Œè¡¨ç¤ºä¸€è¡Œéœ€è¦å‡ ä¸ªV_VEC NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW è¡¨ç¤ºä¸€ä¸ªwarpå¯ä»¥å¤„ç†å¤šå°‘è¡Œ NUM_ROWS_PER_THREAD è¡¨ç¤ºæ¯ä¸ªthreadéœ€è¦è´Ÿè´£å¤šå°‘è¡Œ Kernel ä»£ç é€»è¾‘ï¼š\nï¼ˆ1ï¼‰å¾ªç¯ä»æ˜¾å­˜è¯»å–\\(Q\\)åˆ° shared memoryï¼š\nè¿­ä»£è¯»å–ï¼Œæ¯ CUDA block è´Ÿè´£è¯»å–\\(Q\\)çš„ä¸€è¡Œï¼ˆhead_size ä¸ªå…ƒç´ ï¼‰å­˜å…¥ shared memoryã€‚å…¶ä¸­ï¼Œblock çš„æ¯ä¸ª Warp è´Ÿè´£è¯»å– 16blk_size å­—èŠ‚çš„ Qï¼Œå³æ¯ä¸ª thread group ä¼šè¯»å–16å­—èŠ‚çš„ Qï¼Œ16blk_size å­—èŠ‚çš„ Q å¯¹åº” sequence çš„ä¸€ä¸ª headã€‚\nconst int thread_group_idx = thread_idx / THREAD_GROUP_SIZE; const int thread_group_offset = thread_idx % THREAD_GROUP_SIZE; // Load the query to registers. // Each thread in a thread group has a different part of the query. // For example, if the the thread group size is 4, then the first thread in // the group has 0, 4, 8, ... th vectors of the query, and the second thread // has 1, 5, 9, ... th vectors of the query, and so on. NOTE(woosuk): Because // q is split from a qkv tensor, it may not be contiguous. const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE; __shared__ Q_vec q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD]; #pragma unroll for (int i = thread_group_idx; i \u0026lt; NUM_VECS_PER_THREAD; i += NUM_THREAD_GROUPS) { const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE; q_vecs[thread_group_offset][i] = *reinterpret_cast\u0026lt;const Q_vec*\u0026gt;(q_ptr + vec_idx * VEC_SIZE); } __syncthreads(); ï¼ˆ2ï¼‰å¾ªç¯ä»æ˜¾å­˜è¯»å–\\(K\\)åˆ° registerï¼Œå¹¶è®¡ç®—QKï¼š\næ¯ä¸ª seq åŒ…å« cxt_length * num_kv_heads * head_size ä¸ªå…ƒç´  æ¯ä¸ª CUDA block è´Ÿè´£è®¡ç®—ä¸€ä¸ª seq çš„ä¸€ä¸ª head çš„ \\(QK^T\\)ï¼Œ åªéœ€è¦è¯»å– ctx_length * head_size ä¸ª K çš„å…ƒç´  å› ä¸ºé¡µå¼å†…å­˜ç®¡ç†ï¼ŒK åœ¨ ctx_length ç»´åº¦çš„å­˜å‚¨ä¸è¿ç»­ï¼Œä»¥ blk_size ä¸ª token ä¸ºç²’åº¦åˆ†å¸ƒåœ¨ä¸åŒçš„å†…å­˜åœ°å€ï¼Œæ‰€ä»¥éœ€è¦æ ¹æ® Q çš„ head_idx å’Œ seq_idx è®¿é—® block_table æ‰¾åˆ° K çš„ physical_block_num K Cacheçš„å¸ƒå±€ä¸º [num_blocks, num_kv_heads, head_size/x, block_size, x]ï¼Œ ç›®çš„æ˜¯ä¼˜åŒ–å†™å…¥ shared memoryã€‚Qå’ŒKçš„åŒä¸€è¡Œå…ƒç´ è¢«è¯»å…¥å¯„å­˜å™¨å¹¶è¿›è¡Œç‚¹ä¹˜è¿ç®—åï¼Œç»“æœè¦å†™å…¥shared memoryã€‚å¦‚æœä¸€ä¸ª Warp ä¸­æ‰€æœ‰çº¿ç¨‹éƒ½è®¡ç®— Qã€K åŒä¸€è¡Œæ•°æ®ï¼Œä¼šå¯¼è‡´å†™å…¥ shared memory çš„åŒä¸€ä¸ªä½ç½®ï¼Œè¿™å°†é€ æˆ warp å†…ä¸åŒçº¿ç¨‹é¡ºåºåœ°å†™å…¥ã€‚æ‰€ä»¥ warp çš„çº¿ç¨‹æœ€å¥½è®¡ç®— Qå’ŒK çš„ä¸åŒè¡Œæ•°æ®ã€‚åœ¨è®¾è®¡ K å¸ƒå±€æ—¶ï¼Œå°† block_size æ”¾åœ¨æ¯” head_size æ›´ä½çš„ç»´åº¦ã€‚ç”±äºwarp sizeå¤§äºblock_sizeï¼Œæˆ‘ä»¬éœ€è¦å°†head_sizeæ‹†åˆ†ä¸ºhead_size/xå’Œxä¸¤ä¸ªç»´åº¦ï¼Œå€Ÿxåˆ°æœ€ä½ç»´åº¦ï¼Œä»¥ç¡®ä¿æ¯ä¸ªçº¿ç¨‹è¯»å…¥çš„æ•°æ®é‡å’Œè®¡ç®—é‡éƒ½è¶³å¤Ÿå¤§ã€‚æœ€åï¼Œæ¯ä¸ªçº¿ç¨‹ç»„æ´¾ä¸€ä¸ªçº¿ç¨‹å»å†™å…¥shared memoryï¼Œè¿™æ ·ä¸€ä¸ªwarpæœ‰blk_sizeä¸ªçº¿ç¨‹å¹¶è¡Œå†™å…¥shared memoryï¼Œä»è€Œå¢åŠ äº†shared memoryçš„è®¿é—®å¸¦å®½ã€‚è¿™ç§è®¾è®¡ç­–ç•¥æ˜¯ä¸ºäº†å®ç°é«˜æ•ˆçš„å¹¶è¡Œè®¡ç®—å’Œå†…å­˜è®¿é—®ï¼Œä»¥æé«˜æ•´ä½“çš„è®¡ç®—æ€§èƒ½ã€‚ è¯»å– K éœ€è¦ä¸€ä¸ªå¾ªç¯ï¼Œå¾ªç¯ä¸­æ¯ä¸ªCUDA blockä¸­çš„æ‰€æœ‰ warp ä¾æ¬¡è®¿é—®num_blocks ä¸ª page blockã€‚æ¯æ¬¡è¿­ä»£ï¼š æ¯ä¸ª warp è´Ÿè´£è®¿é—®è¿ç»­çš„ blk_size ä¸ª KCache çš„è¡Œæ•°æ®ï¼ˆblk_size * head_sizeä¸ªå…ƒç´ ï¼‰ã€‚æ¯ä¸ª thread group è´Ÿè´£è®¿é—® KCache çš„ä¸€è¡Œï¼Œå°†head_size ä¸ªå…ƒç´ è¯»å…¥å¯„å­˜å™¨ å¯„å­˜å™¨ä¸­çš„Qå’ŒKå…ƒç´ è¿›è¡Œç‚¹ä¹˜ï¼Œç»“æœå†™å…¥shared memoryã€‚ä¸€ä¸ª CUDA block çš„ shared memory å­˜å‚¨äº†ä¸€è¡Œ QK^T çš„ç»“æœï¼Œå…± ctx_length ä¸ªå…ƒç´  CUDA block å¯¹ shared memory ä¸­å…ƒç´ è¿›è¡Œ maxï¼Œsum æ–¹å¼ reductionï¼Œç„¶åè®¡ç®—å¾—åˆ° softmax çš„ç»“æœ ä»£ç æ­¥éª¤ï¼š\næ¯ä¸ªwarpè´Ÿè´£è®¡ç®—ä¸€ä¸ªblock keyï¼Œè€Œæ¯ä¸ªblock key shapeä¸º [block_size, num_head, head_size] æ¯ä¸ªthread_groupå–ä¸€ä¸ªkeyï¼Œå³num_headä¸ªå…ƒç´ ï¼Œè®¡ç®—QK dot åªæœ‰thread_groupçš„ç¬¬ä¸€ä¸ªthreadè´Ÿè´£å°†QKç»“æœå†™å…¥shared memory // æ¯ä¸ªwarpè´Ÿè´£ blocksize * headsizeä¸ªå…ƒç´  for (int block_idx = warp_idx; block_idx \u0026lt; num_blocks; block_idx += NUM_WARPS) { // TODO(Zhengzekang) const int physical_block_number = block_table[block_idx]; // ... K_vec k_vecs[NUM_VECS_PER_THREAD]; // éå†æ¯ä¸ªthread_groupå¤„ç†å¤šå°‘ä¸ªtoken for (int i = 0; i \u0026lt; NUM_TOKENS_PER_THREAD_GROUP; i++) { const int physical_block_offset = (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE; const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset; K_vec k_vecs[NUM_VECS_PER_THREAD]; // éå†æ¯ä¸ªthreadéœ€è¦å¤„ç†å¤šå°‘ä¸ªVEC for (int j = 0; j \u0026lt; NUM_VECS_PER_THREAD; j++) { // vectorizedå–åˆ°key k_vecs[j] = xxxx; } // è®¡ç®—QKdotï¼Œé‡Œé¢åŒ…å«äº†ä¸€ä¸ªthread_groupsizeçš„WarpReduceSumï¼Œ float qk = scale * Qk_dot\u0026lt;scalar_t, THREAD_GROUP_SIZE\u0026gt;::dot(q_vecs, k_vecs); // åªæœ‰thread_groupçš„ç¬¬ä¸€ä¸ªthreadè´Ÿè´£å°†QKç»“æœå†™å…¥shared memory // å¹¶ä¸”ç»´æŠ¤ä¸€ä¸ªqk_maxï¼Œç”¨äºåç»­softmax if (thread_group_offset == 0) { // Store the partial reductions to shared memory. // NOTE(woosuk): It is required to zero out the masked logits. const bool mask = token_idx \u0026gt;= context_len; logits[token_idx - start_token_idx] = mask ? 0.f : qk; // Update the max value. qk_max = mask ? qk_max : fmaxf(qk_max, qk); } } } æ­¤æ—¶å„ä¸ªthread_groupå·²ç»å®Œæˆäº†è‡ªå·±çš„qk_dotæ“ä½œï¼Œå¹¶ä¸”éƒ½ç»´æŠ¤äº†qk_maxã€‚ä¸‹é¢å°±éœ€è¦å’Œå…¶ä»–thread_groupåšwarp shuffleæ“ä½œï¼Œå¾—åˆ°ä¸€ä¸ªwarpå†…çš„qk maxå€¼ã€‚\nï¼ˆ3ï¼‰ä»æ˜¾å­˜è¯»å–\\(V\\)åˆ° registerï¼š\nå’ŒKCacheä¸€æ ·ï¼ŒCUDA thread blockä¾æ¬¡è®¿é—®num_blkä¸ªç‰©ç†å—åˆ°å¯„å­˜å™¨ï¼Œæ¯ä¸ªwarpè´Ÿè´£blk_sizeä¸ªtokençš„pageå†…å­˜ï¼Œpageçš„çœŸå®ç‰©ç†åœ°å€åŒæ ·éœ€è¦è¿›è¡Œç´¢å¼•ã€‚ä¸è¿‡è¿™é‡Œä¸éœ€è¦ä»¥thread groupä¸ºå•ä½è®¿é—®16å­—èŠ‚ï¼Œè€Œæ˜¯æ¯ä¸ªthreadè®¿é—®16å­—èŠ‚çš„å…ƒç´ ã€‚è®¿é—®å®Œå°±å¯ä»¥ä¸shared memoryçš„softmax(QK^T)ä¸­é—´ç»“æœå¯¹åº”ä½ç½®16å­—èŠ‚çš„æ•°æ®è¿›è¡Œç‚¹ä¹˜ï¼Œå¾—åˆ°ä¸€ä¸ªfloatç»“æœï¼Œå†™åˆ°outputå¯¹åº”ä½ç½®ä¸­ã€‚\nä¸ºä»€ä¹ˆ VCache çš„ layout æ˜¯ [num_blocks, num_kv_heads, head_size, block_size]ï¼Œå’Œ KCache layout ä¸ä¸€æ ·ï¼Ÿ å› ä¸º V è¦å»åšç‚¹ä¹˜çš„å¯¹è±¡åœ¨shared memoryï¼Œåªéœ€è¦è¯»ï¼Œä¸æ¶‰åŠå¹¶è¡Œå†™ã€‚\nPA V1 å’Œ Flash Attention çš„åŒºåˆ« # å¹¶è¡Œä»»åŠ¡çš„åˆ’åˆ†æ–¹å¼ä¸åŒ\nFlashAttention ç”¨äº†ä¸¤å±‚å¾ªç¯ï¼Œæ¯æ¬¡å†™ä¸€ä¸ª Tile çš„ output tensorï¼Œè€Œ PA åªæœ‰ä¸€å±‚å¾ªç¯ï¼Œæ¯æ¬¡å†™ä¸€è¡Œ output tensorã€‚å› ä¸ºæ¯æ¬¡è¿­ä»£éƒ½æœ‰æ•´è¡Œçš„ QK^T ä¸­é—´ç»“æœï¼Œä¸éœ€è¦online softmax PA V1 è®¾è®¡çš„ KCache layout å……åˆ†åˆ©ç”¨äº† shared memory å†™å¸¦å®½ PA V1 çš„ç¼ºé™· # ä¸è¶³ï¼š\nä¸é€‚åˆ seq å¾ˆé•¿çš„æƒ…å†µï¼Œå› ä¸ºæ²¡æœ‰æ²¿ç€ ctx_length æˆ–è€… batch ç»´åº¦åšåˆ‡åˆ† å’ŒMHAç›¸æ¯”ï¼ŒMQAå’ŒGAQæ²¡æœ‰å‡å°‘å¯¹KV Cacheçš„è¯»å†™æ¬¡æ•°ã€‚è¯»Kã€V Cacheæ—¶å€™åªæ˜¯åšäº†ä¸€ä¸ªhead_idxçš„è½¬æ¢ï¼Œä¼šé‡å¤ä»æ˜¾å­˜è¯»ç›¸åŒçš„head æœªå®Œå¾…ç»­\u0026hellip;\nReference:\nvllm Efficient Memory Management for Large Language Model Serving with PagedAttention ","date":"2024-05-20","externalUrl":null,"permalink":"/posts/llm/paged_attention_v1/","section":"Blogs","summary":"vLLM # vLLMæ˜¯ååæ€§èƒ½å“è¶Šçš„å¤§æ¨¡å‹æ¨ç†æ¡†","title":"Paged Attention V1(vLLM)","type":"posts"},{"content":"","date":"2024-05-20","externalUrl":null,"permalink":"/tags/vllm/","section":"Tags","summary":"","title":"Vllm","type":"tags"},{"content":"","date":"2024-05-20","externalUrl":null,"permalink":"/series/vllm/","section":"Series","summary":"","title":"VLLM","type":"series"},{"content":"","date":"2024-05-20","externalUrl":null,"permalink":"/tags/conv/","section":"Tags","summary":"","title":"Conv","type":"tags"},{"content":" CUDA Conv # åœ¨ PyTorch ä¸Šå®ç°Convå¾ˆç®€å•\nimport torch from torch.nn.functional import conv2d device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) import time width = 1000 height = 1000 img =torch.randn([width,height]) img = img.to(device) kernel = torch.tensor([[-1.0, 0.0, 1.0], [-1.0, 0.0, 1.0], [-1.0, 0.0, 1.0]]) img = torch.reshape(img, (1, 1, width, height)) kernel = torch.reshape(kernel, (1, 1, 3, 3)) kernel = kernel.to(device) start = time.perf_counter() output = F.conv2d(img, kernel, stride=1).to(device) end = time.perf_counter() print(f\u0026#39;total_cost: {end-start} ms\u0026#39;) print(f\u0026#39;output_size: {output.shape}\u0026#39;) print(f\u0026#39;output_tensor: {output}\u0026#39;) ç”¨CUDAå®ç°Convï¼ŒSteps:\nCheck Get thread info Memory allocation Copy data to device Call conv kernel Copy data to host Free memory //file: conv.cu #include\u0026lt;stdint.h\u0026gt; #include\u0026lt;cuda.h\u0026gt; #include \u0026#34;cuda_runtime.h\u0026#34; #include \u0026#34;device_launch_parameters.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;math.h\u0026gt; const int NUM_REPEATS = 10; #define CHECK(call) \\ do \\ { \\ const cudaError_t error_code = call; \\ if (error_code != cudaSuccess) \\ { \\ printf(\u0026#34;CUDA Error:\\n\u0026#34;); \\ printf(\u0026#34; File: %s\\n\u0026#34;, __FILE__); \\ printf(\u0026#34; Line: %d\\n\u0026#34;, __LINE__); \\ printf(\u0026#34; Error code: %d\\n\u0026#34;, error_code); \\ printf(\u0026#34; Error text: %s\\n\u0026#34;, \\ cudaGetErrorString(error_code)); \\ exit(1); \\ } \\ } while (0) static void HandleError(cudaError_t err, const char* file, int line) { if (err != cudaSuccess) { printf(\u0026#34;%s in %s at line %d\\n\u0026#34;, cudaGetErrorString(err), file, line); exit(EXIT_FAILURE); } } #define HANDLE_ERROR(err) (HandleError(err, __FILE__, __LINE__)) int getThreadNum() { cudaDeviceProp prop; int count; CHECK(cudaGetDeviceCount(\u0026amp;count)); printf(\u0026#34;GPU num: %d\\n\u0026#34;, count); CHECK(cudaGetDeviceProperties(\u0026amp;prop, 0)); printf(\u0026#34;Max thread num per block: %d\\n\u0026#34;, prop.maxThreadsPerBlock); printf(\u0026#34;Max grid dim: %d, %d, %d)\\n\u0026#34;, prop.maxGridSize[0], prop.maxGridSize[1], prop.maxGridSize[2]); return prop.maxThreadsPerBlock; } __global__ void conv(float* img, float* kernel, float* result, int width, int height, int kernelSize) { int ti = threadIdx.x; int bi = blockIdx.x; int id = (bi * blockDim.x + ti); if (id \u0026gt;= width * height) { return; } int row = id / width; int col = id % width; for (int i = 0; i \u0026lt; kernelSize; ++i) { for (int j = 0; j \u0026lt; kernelSize; ++j) { float imgValue = 0; int curRow = row - kernelSize / 2 + i; int curCol = col - kernelSize / 2 + j; if (curRow \u0026lt; 0 || curCol \u0026lt; 0 || curRow \u0026gt;= height || curCol \u0026gt;= width) { } else { imgValue = img[curRow * width + curCol]; } result[id] += kernel[i * kernelSize + j] * imgValue; } } } int main() { int width = 1000; int height = 1000; float* img = new float[width * height]; for (int row = 0; row \u0026lt; height; ++row) { for (int col = 0; col \u0026lt; width; ++col) { img[col + row * width] = (col + row) % 256; } } int kernelSize = 3; float* kernel = new float[kernelSize * kernelSize]; for (int i = 0; i \u0026lt; kernelSize * kernelSize; ++i) { kernel[i] = i % kernelSize - 1; } float* imgGpu; float* kernelGpu; float* resultGpu; CHECK(cudaMalloc((void**)\u0026amp;imgGpu, width * height * sizeof(float))); CHECK(cudaMalloc((void**)\u0026amp;kernelGpu, kernelSize * kernelSize * sizeof(float))); CHECK(cudaMalloc((void**)\u0026amp;resultGpu, width * height * sizeof(float))); CHECK(cudaMemcpy(imgGpu, img, width * height * sizeof(float), cudaMemcpyHostToDevice)); CHECK(cudaMemcpy(kernelGpu, kernel, kernelSize * kernelSize * sizeof(float), cudaMemcpyHostToDevice)); int threadNum = getThreadNum(); int blockNum = (width * height - 0.5) / threadNum + 1; float t_sum = 0; float t2_sum = 0; for (int repeat = 0; repeat \u0026lt;= NUM_REPEATS; ++repeat) { cudaEvent_t start, stop; CHECK(cudaEventCreate(\u0026amp;start)); CHECK(cudaEventCreate(\u0026amp;stop)); CHECK(cudaEventRecord(start)); cudaEventQuery(start); conv \u0026lt;\u0026lt; \u0026lt;blockNum, threadNum \u0026gt;\u0026gt; \u0026gt; (imgGpu, kernelGpu, resultGpu, width, height, kernelSize); CHECK(cudaEventRecord(stop)); CHECK(cudaEventSynchronize(stop)); float elapsed_time; CHECK(cudaEventElapsedTime(\u0026amp;elapsed_time, start, stop)); printf(\u0026#34;Time = %g ms.\\n\u0026#34;, elapsed_time); if (repeat \u0026gt; 0) { t_sum += elapsed_time; t2_sum += elapsed_time * elapsed_time; } CHECK(cudaEventDestroy(start)); CHECK(cudaEventDestroy(stop)); } const float t_ave = t_sum / NUM_REPEATS; const float t_err = sqrt(t2_sum / NUM_REPEATS - t_ave * t_ave); printf(\u0026#34;Time = %g +- %g ms.\\n\u0026#34;, t_ave, t_err); float* result = new float[width * height]; CHECK(cudaMemcpy(result, resultGpu, width * height * sizeof(float), cudaMemcpyDeviceToHost)); // visualization printf(\u0026#34;img\\n\u0026#34;); for (int row = 0; row \u0026lt; 10; ++row) { for (int col = 0; col \u0026lt; 10; ++col) { printf(\u0026#34;%2.0f \u0026#34;, img[col + row * width]); } printf(\u0026#34;\\n\u0026#34;); } printf(\u0026#34;kernel\\n\u0026#34;); for (int row = 0; row \u0026lt; kernelSize; ++row) { for (int col = 0; col \u0026lt; kernelSize; ++col) { printf(\u0026#34;%2.0f \u0026#34;, kernel[col + row * kernelSize]); } printf(\u0026#34;\\n\u0026#34;); } printf(\u0026#34;result\\n\u0026#34;); for (int row = 0; row \u0026lt; 10; ++row) { for (int col = 0; col \u0026lt; 10; ++col) { printf(\u0026#34;%2.0f \u0026#34;, result[col + row * width]); } printf(\u0026#34;\\n\u0026#34;); } return 0; } ","date":"2024-05-20","externalUrl":null,"permalink":"/posts/cuda/cuda_conv/","section":"Blogs","summary":"CUDA Conv # åœ¨ PyTorch ä¸Šå®ç°Convå¾ˆç®€å• import torch from torch.nn.functional import","title":"CUDA Conv","type":"posts"},{"content":"","date":"2024-05-18","externalUrl":null,"permalink":"/tags/gpu/","section":"Tags","summary":"","title":"GPU","type":"tags"},{"content":" CPU ä¸ GPU çš„ä¸åŒ # CPUï¼Œ4ä¸ª ALUï¼Œä¸»è¦è´Ÿè´£é€»è¾‘è®¡ç®—ï¼Œ1ä¸ªæ§åˆ¶å•å…ƒ Controlï¼Œ1ä¸ª DRAMï¼Œ1ä¸ª Cache GPUï¼Œç»¿è‰²å°æ–¹å—çœ‹ä½œ ALUï¼Œçº¢è‰²æ¡†çœ‹ä½œä¸€ä¸ª SMï¼ŒSM ä¸­çš„å¤šä¸ª ALU share ä¸€ä¸ªControl å’Œ Cacheï¼ŒSM å¯ä»¥çœ‹ä½œä¸€ä¸ªå¤šæ ¸ CPUï¼Œä½†æ˜¯ ALU æ›´å¤šï¼Œcontrol æ›´å°‘ï¼Œä¹Ÿå°±æ˜¯ç®—åŠ›æå‡ï¼Œæ§åˆ¶åŠ›å‡å¼± æ‰€ä»¥ï¼ŒCPU é€‚åˆæ§åˆ¶é€»è¾‘å¤æ‚çš„ä»»åŠ¡ï¼ŒGPU é€‚åˆé€»è¾‘ç®€å•ã€æ•°æ®é‡å¤§ã€è®¡ç®—é‡å¤§çš„ä»»åŠ¡ã€‚\nGPU, CUDA, AI Framework çš„å…³ç³» # Reference:\nNVIDIA CUDA Docs cudaç¼–ç¨‹å­¦ä¹  ä¸€å¼ å›¾äº†è§£GPUã€CUDAã€CUDA toolkitå’Œpytorchçš„å…³ç³» GPU å†…å­˜æ¦‚å¿µæµ…æ GPU å†…éƒ¨ç»“æ„ # æ¯ä¸€ä¸ª SM æœ‰è‡ªå·±çš„ Wrap scheduler ã€å¯„å­˜å™¨ï¼ˆRegisterï¼‰ã€æŒ‡ä»¤ç¼“å­˜ã€L1ç¼“å­˜ã€å…±äº«å†…å­˜ã€‚\nA100 ä¸­æ¯ä¸ª SM åŒ…æ‹¬ 4 ä¸ª SM partitionï¼ˆSMPï¼‰ï¼Œé‡Œè¾¹ç»¿è‰²çš„å°±æ˜¯ Streaming Processorï¼ˆSPï¼‰ï¼Œä¹Ÿå« CUDA coresï¼Œå®ƒä»¬æ˜¯å®é™…æ‰§è¡Œè®¡ç®—çš„åŸºæœ¬å•å…ƒã€‚\næ‰€æœ‰çš„ SM å…±äº« L2 ç¼“å­˜ã€‚æ•´ä¸ª GPU å†…å­˜ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤º\nGPU å†…å­˜ç»“æ„ # æŒ‰ç…§å­˜å‚¨åŠŸèƒ½è¿›è¡Œç»†åˆ†ï¼ŒGPU å†…å­˜å¯ä»¥åˆ†ä¸ºï¼šå±€éƒ¨å†…å­˜ï¼ˆlocal memoryï¼‰ã€å…¨å±€å†…å­˜ï¼ˆglobal memoryï¼‰ã€å¸¸é‡å†…å­˜ï¼ˆconstant memoryï¼‰ã€å…±äº«å†…å­˜ï¼ˆshared memoryï¼‰ã€å¯„å­˜å™¨ï¼ˆregisterï¼‰ã€L1/L2 ç¼“å­˜ç­‰ã€‚\nå…¶ä¸­å…¨å±€å†…å­˜ã€å±€éƒ¨å†…å­˜ã€å¸¸é‡å†…å­˜éƒ½æ˜¯ç‰‡ä¸‹å†…å­˜(off-chip)ï¼Œå‚¨å­˜åœ¨ HBM ä¸Šã€‚æ‰€ä»¥ HBM çš„å¤§éƒ¨åˆ†ä½œä¸ºå…¨å±€å†…å­˜ã€‚\non-chipï¼šL1/L2 cacheï¼šå¤šçº§ç¼“å­˜ï¼Œåœ¨ GPU èŠ¯ç‰‡å†…éƒ¨\noff-chipï¼šGPU DRAM/HBM, global memory\nL2 ç¼“å­˜å¯ä»¥è¢«æ‰€æœ‰ SM è®¿é—®ï¼Œé€Ÿåº¦æ¯”å…¨å±€å†…å­˜å¿«ã€‚Flash attention çš„æ€è·¯å°±æ˜¯å°½å¯èƒ½åœ°åˆ©ç”¨ L2 ç¼“å­˜ï¼Œå‡å°‘ HBM çš„æ•°æ®è¯»å†™æ—¶é—´\nL1 ç¼“å­˜ç”¨äºå­˜å‚¨ SM å†…çš„æ•°æ®ï¼Œè¢« SM å†…çš„ CUDA cores å…±äº«ï¼Œä½†æ˜¯è·¨ SM ä¹‹é—´çš„ L1 ä¸èƒ½ç›¸äº’è®¿é—®\nå±€éƒ¨å†…å­˜ (local memory) æ˜¯çº¿ç¨‹ç‹¬äº«çš„å†…å­˜èµ„æºï¼Œçº¿ç¨‹ä¹‹é—´ä¸å¯ä»¥ç›¸äº’è®¿é—®ã€‚å±€éƒ¨å†…å­˜å±äºoff-chipï¼Œæ‰€ä»¥è®¿é—®é€Ÿåº¦è·Ÿå…¨å±€å†…å­˜ä¸€æ ·ã€‚å®ƒä¸»è¦æ˜¯ç”¨æ¥åº”å¯¹å¯„å­˜å™¨ä¸è¶³æ—¶çš„åœºæ™¯ï¼Œå³åœ¨çº¿ç¨‹ç”³è¯·çš„å˜é‡è¶…è¿‡å¯ç”¨çš„å¯„å­˜å™¨å¤§å°æ—¶ï¼Œnvcc ä¼šè‡ªåŠ¨å°†ä¸€éƒ¨æ•°æ®æ”¾ç½®åˆ°ç‰‡ä¸‹å†…å­˜é‡Œã€‚\nå¯„å­˜å™¨ï¼ˆregisterï¼‰æ˜¯çº¿ç¨‹èƒ½ç‹¬ç«‹è®¿é—®çš„èµ„æºï¼Œå®ƒæ˜¯ç‰‡ä¸Šï¼ˆon chipï¼‰å­˜å‚¨ï¼Œç”¨æ¥å­˜å‚¨ä¸€äº›çº¿ç¨‹çš„æš‚å­˜æ•°æ®ã€‚å¯„å­˜å™¨çš„é€Ÿåº¦æ˜¯è®¿é—®ä¸­æœ€å¿«çš„ï¼Œä½†æ˜¯å®ƒçš„å®¹é‡è¾ƒå°ï¼Œåªæœ‰å‡ ç™¾ç”šè‡³å‡ å KBï¼Œè€Œä¸”è¦è¢«è®¸å¤šçº¿ç¨‹å‡åˆ†\nå…±äº«å†…å­˜ï¼ˆshared memoryï¼‰ æ˜¯ä¸€ç§åœ¨çº¿ç¨‹å—å†…èƒ½è®¿é—®çš„å†…å­˜ï¼Œæ˜¯ç‰‡ä¸Šï¼ˆon chipï¼‰å­˜å‚¨ï¼Œè®¿é—®é€Ÿåº¦è¾ƒå¿«ã€‚å…±äº«å†…å­˜ä¸»è¦æ˜¯ç¼“å­˜ä¸€äº›éœ€è¦åå¤è¯»å†™çš„æ•°æ®ã€‚å…±äº«å†…å­˜ä¸ L1 ç¼“å­˜çš„ä½ç½®ã€é€Ÿåº¦æå…¶ç±»ä¼¼ï¼ŒåŒºåˆ«åœ¨äºå…±äº«å†…å­˜çš„æ§åˆ¶ä¸ç”Ÿå‘½å‘¨æœŸç®¡ç†ä¸ L1 ä¸åŒï¼šå…±äº«å†…å­˜å—ç”¨æˆ·æ§åˆ¶ï¼ŒL1 å—ç³»ç»Ÿæ§åˆ¶ã€‚å…±äº«å†…å­˜æ›´åˆ©äºçº¿ç¨‹å—ä¹‹é—´æ•°æ®äº¤äº’ã€‚\nå¸¸é‡å†…å­˜ï¼ˆconstant memoryï¼‰æ˜¯ç‰‡ä¸‹ï¼ˆoff chipï¼‰å­˜å‚¨ï¼Œä½†æ˜¯é€šè¿‡ç‰¹æ®Šçš„å¸¸é‡å†…å­˜ç¼“å­˜ï¼ˆconstant cacheï¼‰è¿›è¡Œç¼“å­˜è¯»å–ï¼Œå®ƒæ˜¯åªè¯»å†…å­˜ã€‚å¸¸é‡å†…å­˜ä¸»è¦æ˜¯è§£å†³ä¸€ä¸ª warp scheduler å†…å¤šä¸ªçº¿ç¨‹è®¿é—®ç›¸åŒæ•°æ®æ—¶é€Ÿåº¦å¤ªæ…¢çš„é—®é¢˜ã€‚å‡è®¾æ‰€æœ‰çº¿ç¨‹éƒ½éœ€è¦è®¿é—®ä¸€ä¸ª constant_A çš„å¸¸é‡ï¼Œåœ¨å­˜å‚¨ä»‹è´¨ä¸Š constant_A çš„æ•°æ®åªä¿å­˜äº†ä¸€ä»½ï¼Œè€Œå†…å­˜çš„ç‰©ç†è¯»å–æ–¹å¼å†³å®šäº†å¤šä¸ªçº¿ç¨‹ä¸èƒ½åœ¨åŒä¸€æ—¶åˆ»è¯»å–åˆ°è¯¥å˜é‡ï¼Œæ‰€ä»¥ä¼šå‡ºç°å…ˆåè®¿é—®çš„é—®é¢˜ï¼Œè¿™æ ·ä½¿å¾—å¹¶è¡Œè®¡ç®—çš„çº¿ç¨‹å‡ºç°äº†è¿ç®—æ—¶å·®ã€‚å¸¸é‡å†…å­˜æ­£æ˜¯è§£å†³è¿™æ ·çš„é—®é¢˜è€Œè®¾ç½®çš„ï¼Œå®ƒæœ‰å¯¹åº”çš„ cache ä½ç½®äº§ç”Ÿå¤šä¸ªå‰¯æœ¬ï¼Œè®©çº¿ç¨‹è®¿é—®æ—¶ä¸å­˜åœ¨å†²çªï¼Œä»è€Œä¿è¯å¹¶è¡Œåº¦ã€‚\nTensor Core # CUDA core å’Œ Tensor core çš„åŒºåˆ«ï¼š\nTensor core æ˜¯åœ¨ Volta ä»¥åŠä¹‹åçš„æ¶æ„ä¸­æ‰æœ‰çš„, ç›¸æ¯”äºCUDA coreï¼Œå®ƒå¯ä»¥æä¾›æ›´é«˜æ•ˆçš„è¿ç®—ã€‚ æ¯ä¸ª GPU clockï¼ŒCUDA core å¯ä»¥è¿›è¡Œä¸€æ¬¡å•ç²¾åº¦ä¹˜åŠ è¿ç®—ï¼Œå³ï¼šin fp32: x += y * zã€‚ æ¯ä¸ª GPU clockï¼ŒTensor core å¯ä»¥å®Œæˆ 4 Ã— 4 çš„æ··åˆç²¾åº¦çŸ©é˜µä¹˜åŠ  (matrix multiply-accumulate, MMA)ï¼šD=A * B + Cï¼Œå…¶ä¸­ Aã€Bã€Cã€D éƒ½æ˜¯ 4 Ã— 4 çŸ©é˜µã€‚A å’Œ Bæ˜¯ FP16 çŸ©é˜µï¼Œè€Œç´¯åŠ çŸ©é˜µ C å’Œ D å¯ä»¥æ˜¯ FP16 æˆ– FP32 çŸ©é˜µï¼ˆFP16/FP16 æˆ– FP16/FP32 ä¸¤ç§æ¨¡å¼ã€‚æ‰€ä»¥æ¯ä¸ª GPU clockï¼ŒTensor core å¯ä»¥æ‰§è¡Œ 64 ä¸ªæµ®ç‚¹ FMA æ··åˆç²¾åº¦è¿ç®—ï¼ˆ4 Ã— 4 Ã— 4ï¼‰ã€‚ Turing æ¶æ„ä¸­æ–°å¢äº† INT8/INT32, INT4/INT32, INT1/INT32 ç­‰æ¨¡å¼ V100 ä¸­ï¼Œä¸€ä¸ª SM ä¸­æœ‰ 8 ä¸ª Tensor coreï¼Œæ¯ä¸ª GPU clock å…±å¯ä»¥æ‰§è¡Œ 1024 ä¸ªæµ®ç‚¹è¿ç®—ï¼ˆ64 Ã— 8 Ã— 2ï¼Œä¹˜ä»¥ 2 å› ä¸ºä¹˜åŠ æ˜¯ä¸¤ä¸ªæµ®ç‚¹è¿ç®—ï¼‰\nReference:\nTENSOR CORE DL PERFORMANCE GUIDE GPUå†…å­˜æ¦‚å¿µæµ…æ ","date":"2024-05-18","externalUrl":null,"permalink":"/posts/cuda/gpu/","section":"Blogs","summary":"CPU ä¸ GPU çš„ä¸åŒ # CPUï¼Œ4ä¸ª ALUï¼Œä¸»è¦è´Ÿ","title":"GPU ç»“æ„","type":"posts"},{"content":" Main Concepts # gridDim\ngridDim.xã€gridDim.yã€gridDim.zåˆ†åˆ«è¡¨ç¤º grid å„ä¸ªç»´åº¦çš„å¤§å°\nblockDim\nblockDim.xã€blockDim.yã€blockDim.zåˆ†åˆ«è¡¨ç¤º block å„ä¸ªç»´åº¦çš„å¤§å°\nblockIdx\nblockIdx.xã€blockIdx.yã€blockIdx.zåˆ†åˆ«è¡¨ç¤ºå½“å‰ block åœ¨ grid ä¸­çš„åæ ‡\nthreadIdx\nthreadIdx.xã€threadIdx.yã€threadIdx.zåˆ†åˆ«è¡¨ç¤ºå½“å‰ thread åœ¨ block çš„åæ ‡\ngrid é‡Œæ€»çš„çº¿ç¨‹ä¸ªæ•° N = gridDim.x * gridDim.y * gridDim.z * blockDim.x * blockDim.y * blockDim.z\né€šè¿‡ blockIdx.xã€blockIdx.yã€blockIdx.zã€threadIdx.xã€threadIdx.yã€threadIdx.z å¯ä»¥å®šä½ä¸€ä¸ªçº¿ç¨‹çš„åæ ‡ã€‚\nå°†æ‰€æœ‰çš„çº¿ç¨‹æ’æˆä¸€ä¸ªåºåˆ—ï¼Œåºåˆ—å·ä¸º 0 , 1 , 2 , â€¦ , N ï¼Œå¦‚ä½•æ‰¾åˆ°å½“å‰ thread çš„åºåˆ—å· ?\nå…ˆæ‰¾åˆ°è¯¥threadæ‰€åœ¨çš„ blockçš„åºå· blockId = blockIdx.x + blockIdx.ygridDim.x + blockIdx.zgridDim.x*gridDim.y ç„¶åæ‰¾åˆ°å½“å‰ thread åœ¨ block ä¸­çš„åºå· threadId = threadIdx.x + threadIdx.yblockDim.x + threadIdx.zblockDim.x*blockDim.y è®¡ç®—ä¸€ä¸ª block ä¸­ä¸€å…±æœ‰å¤šå°‘ä¸ª threadï¼Œ M = blockDim.xblockDim.yblockDim.z æ±‚å¾—å½“å‰çš„çº¿ç¨‹çš„åºåˆ—å· idx = threadId + M*blockId Steps of CUDA Program # Config GPU(device), cudaSetDevice() Allocate device memory, cudaMalloc(), cudaMallocManaged() Allocate CPU(Host) memory Copy data from host to device, cudaMemcpy() Run kernel on device Copy result from device to host, cudaMemcpy() Print result on host Release host and device memory, cudaFree(), free() CPU is always called host, the GPU is called device\nC Example:\n// file: sample.cu #include\u0026lt;stdint.h\u0026gt; #include\u0026lt;cuda.h\u0026gt; #include \u0026#34;cuda_runtime.h\u0026#34; #include \u0026#34;device_launch_parameters.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; // This is a sample CUDA kernel, called on host and execte on device __global__ void add(float* a) { a[threadIdx.x] = 1; } int main(int argc, char** argv) { // 1. Set device cudaSetDevice(0); // 2. Allocate device memory float* dx; cudaMalloc((void**)\u0026amp;dx, 16 * sizeof(float)); // 3. Allocate host memory float hx[16] = { 0 }; // 4. Copy data from host to device cudaMemcpy(dx, hx, 16 * sizeof(float), cudaMemcpyHostToDevice); // 5. Run kernel on device add \u0026lt;\u0026lt; \u0026lt;1, 16 \u0026gt;\u0026gt; \u0026gt; (dx); // 6. Copy result from device to host cudaMemcpy(hx, dx, 16 * sizeof(float), cudaMemcpyDeviceToHost); // 7. Print result on host for (int i = 0; i \u0026lt; 16; i++) { printf(\u0026#34;%f \\n\u0026#34;, hx[i]); } 8. Release host and device memory cudaFree(dx); free(hx); //cudaDeviceReset(); return 0; } ä½¿ç”¨nvccç¼–è¯‘,ç„¶åè¿è¡Œ\nnvcc sample.cu - o sample ./sample CUDA æä¾›äº†ç»Ÿä¸€å†…å­˜: gpuå’Œcpuå¯è®¿é—®çš„å•ä¸€å†…å­˜ç©ºé—´. è°ƒç”¨cudaMallocManaged()ï¼Œå®ƒè¿”å›ä¸€ä¸ªæŒ‡é’ˆï¼Œä»host codeæˆ–device codeéƒ½å¯ä»¥è®¿é—®ã€‚è¦é‡Šæ”¾æ•°æ®ï¼Œåªéœ€å°†æŒ‡é’ˆä¼ é€’ç»™cudaFree()ã€‚\nCUDA Kernel and Parrallel Computing # å‰ç½®çŸ¥è¯†: ç†è§£ GPU ç»“æ„, Grid, Block, Thread è¿™å‡ ä¸ªé€»è¾‘æ¦‚å¿µä¹‹é—´çš„å…³ç³»\nCUDA kernel çš„ç¼–ç¨‹æ¨¡å‹\n[Todo] Dim and size detail\nè°ƒç”¨kernel: add \u0026lt;\u0026lt; \u0026lt;blockNumber, threadNumber \u0026gt;\u0026gt; \u0026gt; (dx);\nç¼–å†™kernelï¼š\nç”¨å…³é”®å­—æè¿°ç¬¦ __global__ å£°æ˜kernel: __global__ void add(){} è°ƒç”¨ kernel æ—¶çš„å‚æ•° \u0026lt;\u0026lt;\u0026lt;blockNumber per grid, threadNumber per block\u0026gt;\u0026gt;\u0026gt; å†³å®šäº†å…±æœ‰ TotalThreadNum = blockNumber * threadNumber ä¸ªçº¿ç¨‹å¯ä»¥å¹¶è¡Œæ‰§è¡Œä»»åŠ¡ kernel å†…çš„æ¯ä¸€æ¬¡è¿­ä»£ï¼Œæ„å‘³ç€ TotalThreadNum ä¸ªçº¿ç¨‹å¹¶è¡Œæ‰§è¡Œäº†ä¸€æ¬¡å¾ªç¯ä½“ä¸­çš„ä»»åŠ¡ï¼ˆå³æ¯ä¸ªçº¿ç¨‹å®Œæˆå¯¹ä¸€ä»½æ•°æ®çš„å¤„ç†ï¼‰ï¼Œä¹Ÿå°±æ˜¯æ¯æ¬¡è¿­ä»£èƒ½å¤„ç† TotalThreadNum ä»½æ•°æ®ï¼ŒTotalThreadNum ä¹Ÿç­‰ä»·äºè·¨æ­¥(stride)çš„å¤§å° kernel ä¸­ threadIdx.x ä»£è¡¨ the index of the thread within the blockï¼Œ blockDim.x ä»£è¡¨ the size of blockï¼ˆnumber of threads in blockï¼ˆå‡è®¾ è¿™é‡Œçš„ grid å’Œ block çš„ dim åªæœ‰ä¸€ç»´ï¼‰ kernel å†… threadIdx.x å’Œ blockIdx.x çš„ç»„åˆå¯¹åº”çº¿ç¨‹çš„å”¯ä¸€æ ‡è¯† ä»¥add_3è¿™ä¸ª kernel ä¸ºä¾‹ï¼Œå¯ä»¥ç”¨ index = blockIdx.x * blockDim.x + threadIdx.x è·å¾—å½“å‰çº¿ç¨‹çš„è¦å¤„ç†çš„æ•°æ®çš„æ•°ç»„ä¸‹æ ‡ï¼ˆè§ä¸‹å›¾ï¼‰ï¼Œ\n__global__ void add_3(int n, float *x, float *y) { int index = blockIdx.x * blockDim.x + threadIdx.x; // strideä¸ºgridçš„çº¿ç¨‹æ€»æ•°:blockDim.x*gridDim.x int stride = blockDim.x * gridDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] = x[i] + y[i]; } Kernel examples(ä¸‹é¢çš„ C++ Example)\n1ä¸ªblock,1ä¸ªçº¿ç¨‹: add_1() 1ä¸ªblock,å¤šä¸ªçº¿ç¨‹: add_2() å¤šä¸ªblock,å¤šä¸ªçº¿ç¨‹: add_3() å¤šä¸ªblockï¼Œå¤šä¸ªçº¿ç¨‹ä¹Ÿç§°ä¸ºç½‘æ ¼è·¨æ­¥å¾ªç¯ï¼Œå…¶ä¸­æ¯æ¬¡å¾ªç¯çš„è·¨æ­¥(stride)ä¸º grid çš„çº¿ç¨‹æ€»æ•°: stride = blockDim.x * gridDim.x\nC++ Example:\n// file: add.cu #include \u0026lt;iostream\u0026gt; #include \u0026lt;math.h\u0026gt; // Kernel function to add the elements of two arrays // single thread __global__ void add_1(int n, float *x, float *y) { for (int i = 0; i \u0026lt; n; i++) y[i] = x[i] + y[i]; } // single block, multi threads __global__ void add_2(int n, float *x, float *y) { int index = threadIdx.x; int stride = blockDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] = x[i] + y[i]; } // multi block, multi threads // ç½‘æ ¼è·¨æ­¥(stride)å¾ªç¯ __global__ void add_3(int n, float *x, float *y) { int index = blockIdx.x * blockDim.x + threadIdx.x; // strideä¸ºgridçš„çº¿ç¨‹æ€»æ•°:blockDim.x*gridDim.x int stride = blockDim.x * gridDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] = x[i] + y[i]; } int main(void) { int N = 1\u0026lt;\u0026lt;20; float *x, *y; // Allocate Unified Memory â€“ accessible from CPU or GPU cudaMallocManaged(\u0026amp;x, N*sizeof(float)); cudaMallocManaged(\u0026amp;y, N*sizeof(float)); // initialize x and y arrays on the host for (int i = 0; i \u0026lt; N; i++) { x[i] = 1.0f; y[i] = 2.0f; } // Run kernel on 1M elements on the GPU // Just run with single thread :) add_1\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Run with 1 block and multi threads add_2\u0026lt;\u0026lt;\u0026lt;1, 256\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Run with multi block and multi threads int blockSize = 256;//å¹¶è¡Œçº¿ç¨‹æ•°é‡ int numBlocks = (N + blockSize - 1) / blockSize;//çº¿ç¨‹å—æ•°é‡ add_3\u0026lt;\u0026lt;\u0026lt;numBlocks, blockSize\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Wait for GPU to finish before accessing on host cudaDeviceSynchronize(); // Check for errors (all values should be 3.0f) float maxError = 0.0f; for (int i = 0; i \u0026lt; N; i++) maxError = fmax(maxError, fabs(y[i]-3.0f)); std::cout \u0026lt;\u0026lt; \u0026#34;Max error: \u0026#34; \u0026lt;\u0026lt; maxError \u0026lt;\u0026lt; std::endl; // Free memory cudaFree(x); cudaFree(y); return 0; } CUDA Kernel for Conv # CUDA Code Profiling # nvprofæ˜¯CUDAå·¥å…·åŒ…é™„å¸¦çš„å‘½ä»¤è¡ŒGPUåˆ†æå™¨\nReference:\nNVIDIA CUDA Docs ","date":"2024-05-10","externalUrl":null,"permalink":"/posts/cuda/cuda_1/","section":"Blogs","summary":"Main Concepts # gridDim gridDim.xã€gridDi","title":"CUDA Programming","type":"posts"},{"content":"","date":"2024-05-05","externalUrl":null,"permalink":"/tags/flash-attention/","section":"Tags","summary":"","title":"Flash Attention","type":"tags"},{"content":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\nä¸æ ‡å‡† attention ç›¸æ¯”ï¼ŒFlash Attention æœ‰ä»¥ä¸‹ä¸‰ç‚¹ç‰¹ç‚¹ï¼š\nè¿ç®—é€Ÿåº¦æ›´å¿« (Fast) æ›´èŠ‚çœæ˜¾å­˜ (Memory-Efficient) è®¡ç®—ç»“æœç›¸åŒ (Exact) FlashAttention ç›®çš„ä¸æ˜¯èŠ‚çº¦ FLOPsï¼Œè€Œæ˜¯å‡å°‘å¯¹HBMçš„è®¿é—®ã€‚å®ƒæ²¡æœ‰æ”¹å˜åŸæœ‰çš„è®¡ç®—å…¬å¼ï¼Œæ•´ä½“è®¡ç®—å¤æ‚åº¦å¹¶æœªé™ä½ã€‚\nèƒŒæ™¯ # GPUä¸­å­˜å‚¨å•å…ƒä¸»è¦æœ‰ HBM å’Œ SRAMï¼šHBM å®¹é‡å¤§ä½†æ˜¯è®¿é—®é€Ÿåº¦æ…¢ï¼ŒSRAMå®¹é‡å°å´æœ‰ç€è¾ƒé«˜çš„è®¿é—®é€Ÿåº¦ã€‚ä¾‹å¦‚ï¼šA100 GPUæœ‰40-80GBçš„HBMï¼Œå¸¦å®½ä¸º1.5-2.0TB/sï¼›æ¯108ä¸ªæµå¼å¤šæ ¸å¤„ç†å™¨å„æœ‰192KBçš„ç‰‡ä¸ŠSRAMï¼Œå¸¦å®½ä¼°è®¡çº¦ä¸º 19TB/sã€‚å¯ä»¥çœ‹å‡ºï¼Œç‰‡ä¸Šçš„SRAMæ¯”HBMå¿«ä¸€ä¸ªæ•°é‡çº§ï¼Œä½†å°ºå¯¸è¦å°è®¸å¤šæ•°é‡çº§ã€‚\nå½“è¾“å…¥åºåˆ—ï¼ˆsequence lengthï¼‰è¾ƒé•¿æ—¶ï¼ŒTransformerçš„è®¡ç®—è¿‡ç¨‹ç¼“æ…¢ä¸”è€—è´¹å†…å­˜ï¼Œè¿™æ˜¯å› ä¸º self-attention çš„ time å’Œ memory complexity ä¼šéšç€ sequence length çš„å¢åŠ æˆäºŒæ¬¡å¢é•¿ã€‚\næ ‡å‡†Attentionçš„è®¡ç®—è¿‡ç¨‹ï¼š $$ S=Q K^T \\in \\mathbb{R}^{N \\times N} $$ $$ P=\\operatorname{softmax}(S) \\in \\mathbb{R}^{N \\times N} $$ $$ O=P V \\in \\mathbb{R}^{N \\times N} $$\næ ‡å‡†Attentionçš„ä¸­é—´ç»“æœ ğ‘†, ğ‘ƒ é€šå¸¸éœ€è¦é€šè¿‡é«˜å¸¦å®½å†…å­˜ï¼ˆHBMï¼‰è¿›è¡Œå­˜å–ï¼Œä¸¤è€…æ‰€éœ€å†…å­˜ç©ºé—´å¤æ‚åº¦ä¸º\\(O(Nd+N^2)\\), å¯¹ HBM çš„é‡å¤è¯»å†™æ˜¯ä¸»è¦ç“¶é¢ˆã€‚è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œéœ€è¦åšä¸¤ä»¶äº‹ï¼š\nåœ¨ä¸è®¿é—®æ•´ä¸ªè¾“å…¥çš„æƒ…å†µä¸‹è®¡ç®— softmax ä¸ä¸ºåå‘ä¼ æ’­å­˜å‚¨å¤§çš„ä¸­é—´ attention çŸ©é˜µ(\\(N^2\\)) FlashAttention V1 # FlashAttention æå‡ºäº†ä¸¤ç§æ–¹æ³•æ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼štiling å’Œ recomputationã€‚\ntiling - æ³¨æ„åŠ›è®¡ç®—è¢«é‡æ–°æ„é€ ï¼Œå°†è¾“å…¥åˆ†å‰²æˆå—ï¼Œå¹¶é€šè¿‡åœ¨è¾“å…¥å—ä¸Šè¿›è¡Œå¤šæ¬¡ä¼ é€’æ¥é€’å¢åœ°æ‰§è¡Œsoftmaxæ“ä½œã€‚ recomputation - å­˜å‚¨æ¥è‡ªå‰å‘çš„ softmax å½’ä¸€åŒ–å› å­ï¼Œä»¥ä¾¿åœ¨åå‘ä¸­å¿«é€Ÿé‡æ–°è®¡ç®—èŠ¯ç‰‡ä¸Šçš„ attentionï¼Œè¿™æ¯”ä»HBMè¯»å–ä¸­é—´çŸ©é˜µçš„æ ‡å‡†æ³¨æ„åŠ›æ–¹æ³•æ›´å¿«ã€‚å¯ä»¥æŠŠå®ƒçœ‹ä½œåŸºäº tiling çš„ç‰¹æ®Šçš„ gradient checkpointing æ­£å¸¸çš„softmaxè®¡ç®—ï¼š\n$$m(x):=\\max _i x i$$ $$f(x):=\\left[e^{x_1-m(x)} \\ldots e^{x_B-m(x)}\\right]$$ $$\\ell(x):=\\sum_i f(x)_i$$ $$\\operatorname{softmax}(x):=\\frac{f(x)}{\\ell(x)}$$\nsoftmax ä¼ªä»£ç : softmax å‡½æ•°éœ€è¦ä¸‰ä¸ªå¾ªç¯ï¼Œç¬¬ä¸€ä¸ªå¾ªç¯è®¡ç®—æ•°ç»„çš„æœ€å¤§å€¼ï¼Œç¬¬äºŒä¸ªå¾ªç¯è®¡ç®— softmax çš„åˆ†æ¯ï¼Œç¬¬ä¸‰ä¸ªå¾ªç¯è®¡ç®— softmax è¾“å‡ºã€‚\nåˆ†å—çš„ softmax è®¡ç®—(å‡è®¾åˆ†2å—å¹¶è¡Œè®¡ç®—)ï¼š\n$$m(x)=m\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)=\\max \\left(m\\left(x^{(1)}\\right), m\\left(x^{(2)}\\right)\\right) $$ $$f(x)=\\left[e^{m\\left(x^{(1)}\\right)-m(x)} f\\left(x^{(1)}\\right) \\quad e^{m\\left(x^{(2)}\\right)-m(x)} f\\left(x^{(2)}\\right)\\right] $$ $$\\ell(x)=\\ell\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)=e^{m\\left(x^{(1)}\\right)-m(x)} \\ell\\left(x^{(1)}\\right)+e^{m\\left(x^{(2)}\\right)-m(x)} \\ell\\left(x^{(2)}\\right) $$ $$\\operatorname{softmax}(x)=\\frac{f(x)}{\\ell(x)}$$\nåˆ†å—çš„ softmax ä¼ªä»£ç : åœ¨ç¬¬ä¸€ä¸ªå¾ªç¯ä¸­åŒæ—¶å¯¹æœ€å¤§å€¼\\(m\\)ä»¥åŠ softmax çš„åˆ†æ¯\\(d\\)è¿›è¡Œæ›´æ–°ï¼Œä»è€Œå‡å°‘äº†ä¸€ä¸ªå¾ªç¯ã€‚é€šè¿‡ tiling çš„æ–¹å¼ï¼Œsoftmax çš„å¾ªç¯æ•°ä»ä¸‰ä¸ªå‡åˆ°äº†ä¸¤ä¸ªï¼Œä»è€Œå¯ä»¥é™ä½å†…å­˜æ¶ˆè€—ã€‚\nflashattention ä¼ªä»£ç ï¼š ä¸­é—´å˜é‡ï¼š\\(O_i\\)(æœ€ç»ˆä¹˜ç§¯)ã€\\(l_i\\)ï¼ˆsoftmaxçš„åˆ†æ¯ï¼Œå³ç´¯åŠ å’Œï¼‰ã€\\(m_i\\)ï¼ˆéå†åˆ°å½“å‰å—ä¸ºæ­¢çš„æœ€å¤§å€¼ï¼‰ï¼Œå†ä¹Ÿä¸ç”¨ä¿å­˜å…¨éƒ¨çš„Så’ŒPäº†ã€‚\nç”±äºé‡æ–°è®¡ç®—å¯¼è‡´ FLOPs å¢åŠ ï¼Œä½†æ˜¯ç”±äºå¤§é‡å‡å°‘HBMè®¿é—®ï¼ŒFlashAttention è¿è¡Œé€Ÿåº¦æ›´å¿«\nFlashAttentionçš„ FLOPs ä¸º \\(ğ‘‚(ğ‘^2ğ‘‘)\\)ï¼Œé™¤äº† input å’Œ outputï¼Œé¢å¤–éœ€è¦çš„å†…å­˜ä¸º \\(ğ‘‚(ğ‘)\\), å¯¹HBMè®¿é—®çš„æ¬¡æ•°ä¸º \\(ğ‘‚(ğ‘^2ğ‘‘^2ğ‘€^{âˆ’1})\\), æ¯”æ ‡å‡† Attention çš„ \\(O(Nd+N^2)\\)æ›´é«˜æ•ˆ\nPyTorch 2.0å·²å°† FlashAttention é›†æˆåˆ°å®˜æ–¹åº“ä¸­ï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨ torch.nn.functional.scaled_dot_product_attention\næ€»ç»“ # FlashAttention V1:\né€šè¿‡åˆ‡å—æŠ€æœ¯å‡å°‘äº†å†…å­˜è®¿é—®æ¬¡æ•°ï¼Œæé«˜äº†è®¡ç®—é€Ÿåº¦å’Œå†…å­˜åˆ©ç”¨ç‡ã€‚ å†…å­˜è®¿é—®å¤æ‚åº¦ä¸º \\(ğ‘‚(ğ‘^2ğ‘‘^2ğ‘€^{âˆ’1})\\), æ¯”æ ‡å‡† Attention çš„ \\(O(Nd+N^2)\\)æ›´é«˜æ•ˆ ","date":"2024-05-05","externalUrl":null,"permalink":"/posts/llm/flash_attention/","section":"Blogs","summary":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness ä¸æ ‡å‡† attention ç›¸æ¯”ï¼ŒFlash","title":"Flash Attention","type":"posts"},{"content":" Self-Attention # å¯¹äºself-attentionï¼Œç”±äº Q, K, V éƒ½æ¥è‡ªè¾“å…¥ X ï¼Œåœ¨è®¡ç®— \\(QT^T\\) æ—¶ï¼Œæ¨¡å‹å¾ˆå®¹æ˜“å…³æ³¨åˆ°è‡ªèº«çš„ä½ç½®ä¸Šï¼Œä¹Ÿå°±æ˜¯ \\(QT^T\\) å¯¹è§’çº¿ä¸Šçš„æ¿€æ´»å€¼ä¼šæ˜æ˜¾æ¯”è¾ƒå¤§, è¿™ä¼šå‰Šå¼±æ¨¡å‹å…³æ³¨å…¶ä»–é«˜ä»·å€¼ä½ç½®çš„èƒ½åŠ›ï¼Œä¹Ÿå°±é™åˆ¶æ¨¡å‹çš„ç†è§£å’Œè¡¨è¾¾èƒ½åŠ›ã€‚MHA å¯¹è¿™ä¸ªé—®é¢˜æœ‰ä¸€å®šçš„ç¼“è§£ä½œç”¨ã€‚\nMHA # MHAï¼ˆmulti-head attentionï¼‰\nQKV ç»è¿‡çº¿æ€§å˜æ¢åï¼Œå°†ä»–ä»¬åˆ†åˆ«åœ¨ hidden states ç»´åº¦ä¸Šåˆ‡åˆ†æˆ heads ä»½ã€‚\nMHA ç›¸æ¯”å•å¤´çš„æƒ…å†µï¼Œç›¸å½“äºåªæ˜¯æŠŠ QKV åˆ‡æˆå¤šä»½å¹¶è¡Œè®¡ç®—äº†ï¼Œå¯¹äºå®é™…éœ€è¦ç¼“å­˜çš„å¤§å°æ²¡æœ‰å½±å“\nKV Cache # Decodingé˜¶æ®µï¼Œä¸‹ä¸€ä¸ªstepçš„è¾“å…¥å…¶å®åŒ…å«äº†ä¸Šä¸€ä¸ªstepçš„å†…å®¹ï¼Œè€Œä¸”åªåœ¨æœ€åé¢å¤šäº†ä¸€ä¸ªtokençš„æ•°æ®ï¼Œä¸‹ä¸€ä¸ªstepçš„è®¡ç®—åº”è¯¥ä¹ŸåŒ…å«äº†ä¸Šä¸€ä¸ªstepçš„è®¡ç®—ã€‚\nKV Cache çš„ç›®çš„ï¼šç©ºé—´æ¢æ—¶é—´ï¼Œç”¨ç¼“å­˜æŠŠéœ€è¦é‡å¤åˆ©ç”¨çš„ä¸­é—´è®¡ç®—ç»“æœå­˜ä¸‹æ¥ï¼Œå‡å°‘é‡å¤è®¡ç®—ã€‚è€Œ K å’Œ V å°±æ˜¯è¦ç¼“å­˜çš„å¯¹è±¡ã€‚\nQ K V\nå¯¹äºè¾“å…¥é•¿åº¦ä¸º \\(s)\\ ï¼Œå±‚æ•°ä¸º \\(L\\) ï¼Œhidden sizeä¸º \\(d\\) çš„æ¨¡å‹:\nå½“ batch size=1 æ—¶ éœ€è¦ç¼“å­˜çš„å‚æ•°é‡ä¸º: \\(2Ls*d\\)ï¼Œå…¶ä¸­ 2 è¡¨ç¤º K + V éœ€è¦çš„ç©ºé—´ä¸ºï¼ˆä½¿ç”¨åŠç²¾åº¦æµ®ç‚¹æ•° float16ï¼‰ï¼š\\(22Lsd Bytes\\) ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ª 2 è¡¨ç¤º float16 å ç”¨ 2 Bytes å½“ batch size=B æ—¶ éœ€è¦ç¼“å­˜çš„å‚æ•°é‡ä¸º: \\(2LsdB\\) éœ€è¦çš„ç©ºé—´ä¸ºï¼ˆä½¿ç”¨åŠç²¾åº¦æµ®ç‚¹æ•° float16ï¼‰ï¼š\\(22Lsd*B Bytes\\) MHA ç›¸æ¯”å•å¤´çš„æƒ…å†µï¼Œç›¸å½“äºåªæ˜¯æŠŠ QKV åˆ‡æˆå¤šä»½å¹¶è¡Œè®¡ç®—äº†ï¼Œå¯¹äºå®é™…éœ€è¦ç¼“å­˜çš„å¤§å°æ²¡æœ‰å½±å“ GQAã€MQA [Todo] ä»¥Llama2 7Bä¸ºä¾‹ï¼Œ\\(L=32, d=4096\\)ï¼Œæ­¤æ—¶æ¯ä¸ª token éœ€è¦çš„ cache ç©ºé—´ä¸º 524,288 Bytes(512 KB)ï¼Œå½“ \\(s=1024, batch size=1)\\) æ—¶ï¼Œéœ€è¦ 512 MB\nä¸»æµæ˜¾å¡é…ç½®ï¼š\nNV A100(Ampere Arch)ï¼ŒHBM2e 40/80GBï¼ŒL2 Cache 40MB, CUDA Cores - 16896/14592 NV H100(Hopper Arch), HBM2e/HBM3 80GB, L2 Cache 50MB, CUDA Cores 6912 NV V100(Volta Arch), HBM2 16/32GB, L2 Cache 6MB, CUDA Cores 5120 H100ä¹Ÿåªæœ‰50Mçš„L2 Cacheï¼Œåªèƒ½æ”¯æŒLlama2 7Bå…±100ä¸ªtokensçš„seqï¼Œè¶…å‡ºL2 Cacheçš„éƒ¨åˆ†åªèƒ½èµ°åˆ°æ˜¾å­˜ä¸­å»äº†ï¼Œä½†æ˜¯ HBM çš„ bandwidth æ¯” L2 Cache å°å¤šäº†ï¼ŒA100 memory bandwidth å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\nMQA # ã€ŠFast Transformer Decoding: One Write-Head is All You Needã€‹\nQ ç»è¿‡çº¿æ€§å˜æ¢åï¼ŒMQA åªå¯¹ Q è¿›è¡Œ å¤šä¸ª head çš„åˆ‡åˆ†ï¼Œæ¯ä¸ª head çš„ Q çš„ç»´åº¦å˜ä¸º \\(Q_s*(d/heads)\\)ï¼Œ Kå’ŒVå¹¶ä¸åˆ‡åˆ†ï¼Œè€Œæ˜¯çº¿æ€§å˜æ¢æ—¶ç›´æ¥æŠŠhidden stateç»´åº¦é™ä½ä¸º d/headsï¼Œ ç„¶å heads ä¸ª Q åˆ†åˆ«äº åŒä¸€ä»½ Kï¼ŒV ç»§ç»­å® attention è®¡ç®—ï¼Œæœ€åå°†ç»“æœ concat èµ·æ¥ã€‚\næ¯”å¦‚åœ¨Llama2 7Bä¸­ç”¨äº†32ä¸ªå¤´ï¼ŒMQAåï¼Œ1024ä¸ª tokens éœ€è¦ KVCache å°±å˜æˆMHA çš„ 1/32ï¼Œå³ 512MB/32=16MBï¼ŒåŸºæœ¬å¯ä»¥å…¨éƒ¨æ”¾å…¥A100çš„L2 Cache\nç”±äºå…±äº«äº†å¤šä¸ªå¤´çš„å‚æ•°ï¼Œé™åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼ŒMQAè™½ç„¶èƒ½å¥½åœ°æ”¯æŒæ¨ç†åŠ é€Ÿï¼Œä½†æ˜¯åœ¨æ•ˆæœä¸Šç•¥ç•¥æ¯”MHAå·®ä¸€ç‚¹\nGQA # ã€ŠGQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpointsã€‹\nGQAï¼ˆGrouped-Query Attentionï¼‰æå‡ºäº†ä¸€ä¸ªæŠ˜ä¸­çš„åŠæ³•ï¼Œæ—¢èƒ½å‡å°‘ MQA çš„æŸå¤±ï¼Œåˆæ¯” MHA éœ€è¦æ›´å°‘çš„ç¼“å­˜ã€‚\nGQAé‡Œï¼Œ Q è¿˜æ˜¯æŒ‰åŸæ¥MHA/MQAçš„åšæ³•ä¸å˜ï¼Œä½†æ˜¯ ç”¨å¤šä»½ Kå’ŒVï¼Œä¸è¿‡æ•°é‡å°äº Q çš„ headsã€‚ç›¸å½“äºæŠŠ Q çš„å¤šä¸ªå¤´ç»™åˆ†äº†groupï¼ŒåŒä¸€ä¸ªgroupå†…çš„ Q å…±äº«åŒä¸€å¥— KVï¼Œä¸åŒgroupçš„ Q æ‰€ç”¨çš„ KV ä¸åŒã€‚\nGQAçš„é€Ÿåº¦ç›¸æ¯”MHAæœ‰æ˜æ˜¾æå‡ï¼Œè€Œæ•ˆæœä¸Šæ¯”MQAä¹Ÿå¥½ä¸€äº›ï¼Œèƒ½åšåˆ°å’ŒMHAåŸºæœ¬æ²¡å·®è·ã€‚Llama2 70B ç”¨çš„å°±æ˜¯GQAã€‚\næœªå®Œå¾…ç»­\u0026hellip;\nTodo: ä»£ç ï¼ŒGQAé‡åŒ–è®¡ç®—\nReference:\nç†è§£Attention:ä»èµ·æºåˆ°MHA,MQAå’ŒGQA self-attention code ","date":"2024-05-05","externalUrl":null,"permalink":"/posts/llm/attention/","section":"Blogs","summary":"Self-Attention # å¯¹äºself-attentionï¼Œç”±","title":"Attention and KV Cache","type":"posts"},{"content":"","date":"2024-05-05","externalUrl":null,"permalink":"/tags/kvcache/","section":"Tags","summary":"","title":"KVCache","type":"tags"},{"content":" Me # class Me: def __init__(self): self.names = \u0026#34;Victor Yang\u0026#34; self.prefer_name = \u0026#34;Victor\u0026#34; self.profession = \u0026#34;Programmer\u0026#34; self.hometown = \u0026#34;Shandong, CN\u0026#34; self.current_location = \u0026#34;Beijing, CN\u0026#34; self.graduate = \u0026#34;University of Science and Technology of China(USTC), Software Engineering\u0026#34; self.undergraduate = \u0026#34;China University of Petroleum(UPC), Computer Science\u0026#34; self.interests = [\u0026#34;AI\u0026#34;, \u0026#34;Taoist\u0026#34;, \u0026#34;Traditional Chinese Medicine\u0026#34;, \u0026#34;History\u0026#34;, \u0026#34;Finance\u0026#34;, \u0026#34;Science Fiction\u0026#34;] self.hoppies = [\u0026#34;Reading\u0026#34;, \u0026#34;Yoga\u0026#34;, \u0026#34;Meditation\u0026#34;, \u0026#34;Swimming\u0026#34;] My Skills # Generative AI/AIGC: Transformer, LLM, VLM, Llama, Llava, MoE, Attention, vLLM, DeepSpeed, RAG, Langchain, Agent Parallel Computing: CUDA, GPU, Horovod, MPI Machine Learning: Deep Learning, PyTorch, Federated Learning Programming Language: Python, C++, GoLang Cloud Native: Knative, Ray, Docker Others: Linux ","date":"2024-05-01","externalUrl":null,"permalink":"/about/","section":"Welcome to My Blog","summary":"Me # class Me: def __init__(self): self.names = \u0026#34;Victor Yang\u0026#34; self.prefer_name = \u0026#34;Victor\u0026#34; self.profession = \u0026#34;Programmer\u0026#34; self.hometown = \u0026#34;Shandong, CN\u0026#34; self.","title":"About","type":"page"},{"content":"This is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing. ğŸš€\nYou can also use these content pages to define Hugo metadata like titles and descriptions that will be used for SEO and other purposes.\n","date":"0001-01-01","externalUrl":null,"permalink":"/tags/advanced/","section":"Tags","summary":"This is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing.","title":"Advanced","type":"tags"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/series/attention/","section":"Series","summary":"","title":"Attention","type":"series"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/tags/_index.zh-cn/","section":"Tags","summary":"","title":"æ ‡ç­¾","type":"tags"},{"content":"æ¬¢è¿æ¥åˆ°æˆ‘çš„åšå®¢!\n","date":"0001-01-01","externalUrl":null,"permalink":"/_index.zh-cn/","section":"Welcome to My Blog","summary":"æ¬¢è¿æ¥åˆ°æˆ‘çš„åšå®¢!","title":"æ¬¢è¿æ¥åˆ°æˆ‘çš„åšå®¢","type":"page"},{"content":"è¿™æ˜¯é«˜çº§æ ‡è®°ã€‚ç±»ä¼¼å…¶ä»– Blowfish ä¸­çš„å…¶ä»–åˆ—è¡¨é¡µé¢ï¼Œä½ å¯ä»¥åœ¨åˆ†ç±»åˆ—è¡¨é¡µæ·»åŠ è‡ªå®šä¹‰å†…å®¹ï¼Œè¿™éƒ¨åˆ†å†…å®¹ä¼šæ˜¾ç¤ºåœ¨é¡¶éƒ¨ã€‚ğŸš€\nä½ ä¹Ÿå¯ä»¥ç”¨è¿™äº›å†…å®¹æ¥å®šä¹‰ Hugo çš„å…ƒæ•°æ®ï¼Œæ¯”å¦‚æ ‡é¢˜å’Œæè¿°ã€‚è¿™äº›å†…å®¹å¯ä»¥è¢«ç”¨æ¥å¢å¼º SEO æˆ–å…¶ä»–ç›®çš„ã€‚\n","date":"0001-01-01","externalUrl":null,"permalink":"/tags/advanced/_index.zh-cn/","section":"Tags","summary":"è¿™æ˜¯é«˜çº§æ ‡è®°ã€‚ç±»ä¼¼å…¶ä»– Blowfish ä¸­çš„å…¶ä»–åˆ—è¡¨é¡µé¢","title":"é«˜çº§","type":"tags"}]