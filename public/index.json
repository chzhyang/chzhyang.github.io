
[{"content":"","date":"2024-05-28","externalUrl":null,"permalink":"/posts/llm/","section":"Blogs","summary":"","title":"LLM","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/posts/cuda/","section":"Blogs","summary":"","title":"CUDA Parallel Programming","type":"posts"},{"content":"","date":"2024-05-28","externalUrl":null,"permalink":"/posts/algorithm/","section":"Blogs","summary":"","title":"Algoritm Journey","type":"posts"},{"content":"","date":"2024-06-01","externalUrl":null,"permalink":"/posts/finance/","section":"Blogs","summary":"","title":"Finance","type":"posts"},{"content":"","date":"2024-06-01","externalUrl":null,"permalink":"/posts/","section":"Blogs","summary":"","title":"Blogs","type":"posts"},{"content":"","date":"2024-06-01","externalUrl":null,"permalink":"/tags/finance/","section":"Tags","summary":"","title":"Finance","type":"tags"},{"content":"","date":"2024-06-01","externalUrl":null,"permalink":"/tags/quantitative-trading/","section":"Tags","summary":"","title":"Quantitative Trading","type":"tags"},{"content":"","date":"2024-06-01","externalUrl":null,"permalink":"/series/quantitative-trading/","section":"Series","summary":"","title":"Quantitative Trading","type":"series"},{"content":"","date":"2024-06-01","externalUrl":null,"permalink":"/posts/finance/quant/","section":"Blogs","summary":"","title":"Quantitative Trading","type":"posts"},{"content":"","date":"2024-06-01","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"2024-06-01","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"Hi, welcome to my blog :)\n","date":"2024-06-01","externalUrl":null,"permalink":"/","section":"Welcome to My Blog","summary":"Hi, welcome to my blog :)","title":"Welcome to My Blog","type":"page"},{"content":"","date":"2024-05-28","externalUrl":null,"permalink":"/tags/ai-quantization/","section":"Tags","summary":"","title":"AI Quantization","type":"tags"},{"content":"","date":"2024-05-28","externalUrl":null,"permalink":"/series/ai-quantization/","section":"Series","summary":"","title":"AI Quantization","type":"series"},{"content":"","date":"2024-05-28","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"","date":"2024-05-28","externalUrl":null,"permalink":"/tags/nlp/","section":"Tags","summary":"","title":"NLP","type":"tags"},{"content":" PTQ # PTQ(Post-Training Quantization)，即后训练量化，主要目标是无需对 LLM 架构进行修改或重新训练的前提下，通过量化减少 LLM 的存储和计算复杂度。\n主要优势是简单高效，但会在量化过程中引入一定程度的精度损失。\nPTQ 分为 权重量化 和 全量化。\n权重量化 # LUT-GEMM, 仅对权重进行量化以及使用BCQ格式在LLM中优化矩阵乘法，通过提高计算效率来增强延迟降低和性能 LLM.int8, 采用混合精度分解的量化方法。先做了一个矩阵分解，对绝大部分权重和激活用8bit量化（vector-wise）。对离群特征的几个维度保留16bit，对其做高精度的矩阵乘法 ZeroQuant, 对权重做group-wise，对激活值做token-wise。用逐层知识蒸馏缓解精度损失（原网络做老师），量化后的网络做学生。和W8A8的普通方法做比较，在BERT和GPT3-style模型上精度更好，还能把权重量化到4bit，但加速效果糟糕 GPTQ, 对某个 block 内的所有参数逐个量化，每个参数量化后，需要适当调整这个 block 内其他未量化的参数，以弥补量化造成的精度损失。 GPTQ 需要准备校准数据集 AWQ, 发现对于LLM的性能，权重并不是同等重要的，通过保留1%的显著权重可以大大减少量化误差。在此基础上，AWQ采用了激活感知方法，考虑与较大激活幅度对应的权重通道的重要性，这在处理重要特征时起着关键作用。该方法采用逐通道缩放技术来确定最佳缩放因子，从而在量化所有权重的同时最小化量化误差 权重和激活全量化 # LLM中激活往往由于异常值的存在而变得更加复杂\nSmoothQuant，观察到不同的token在它们的通道上展示出类似的变化，引入了逐通道缩放变换，有效地平滑了幅度，解决了量化激活的挑战。 LLM.int8() # LLM.int8()发现当 LLMs 的模型参数量超过 6.7B 的时候，激活中会成片的出现大幅的离群点(outliers)，朴素且高效的量化方法（W8A8、ZeroQuant等）会导致量化误差增大，精度下降。但是离群特征（Emergent Features）的分布是有规律的，通常分布在 Transformer 层的少数几个维度。针对这个问题，LLM.int8() 采用了混合精度分解计算的方式（离群点和其对应的权重使用 FP16 计算，其他量化成 INT8 后计算）。虽然能确保精度损失较小，但由于需要运行时进行异常值检测、scattering 和 gathering，导致它比 FP16 推理慢。\n步骤：\n从输入的隐含状态中，按列提取异常值 (离群特征，即大于某个阈值的值)。 对离群特征进行 FP16 矩阵运算，对非离群特征进行量化，做 INT8 矩阵运算； 反量化非离群值的矩阵乘结果，并与离群值矩阵乘结果相加，获得最终的 FP16 结果 虽然 LLM.in8() 带来的性能下降微乎其微，但是这种分离计算的方式拖慢了推理速度。对于 BLOOM-176B，相比于 FP16，LLM.int8() 慢了大约 15% 到 23%；对于更小的模型（3B 和 11B），速度差距更为明显，LLM.int8() 慢三倍以上\nLLM.int8() 的实现主要在 bitsandbytes 库，transformers 库已经集成了 bitsandbytes 这个量化库，其优点是不需要量化校准数据集，任何模型只要含有 torch.nn.Linear 模块，就可以对其进行开箱即用的量化。\n8-bit:\nfrom transformers import AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained( \u0026#39;decapoda-research/llama-7b-hf\u0026#39;, device_map=\u0026#39;auto\u0026#39;, load_in_8bit=True, max_memory={ i: f\u0026#39;{int(torch.cuda.mem_get_info(i)[0]/1024**3)-2}GB\u0026#39; for i in range(torch.cuda.device_count()) } ) 4-bit:\nfrom transformers import BitsAndBytesConfig nf4_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=\u0026#34;nf4\u0026#34;, bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16 ) model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config) GPTQ # GPTQ (Generalized Post-Training Quantization)，是一种训练后量化 (PTQ) 方法，采用 INT4/FP16 (W4A16) 的混合量化方案，其中模型权重被量化为 int4，激活值保留在 FP16，是一种仅权重量化方法。通过最小化权重的均方误差（基于近似二阶信息）将所有权重压缩到 INT4。推理时，动态地将权重反量化为 FP16。\nGPTQ 将权重分组（如：128列为一组）为多个子矩阵（block）。对某个 block 内的所有参数逐个量化，每个参数量化后，需要适当调整这个 block 内其他未量化的参数，以弥补量化造成的精度损失。因此，GPTQ 量化需要准备校准数据集。\nGPTQ 把量化问题视作优化问题，逐层寻找最优的量化权重，使用 Cholesky 分解 Hessian 矩阵的逆，在给定的step中对连续列的块进行量化，并在step结束时更新剩余的权重。\n优势：\nint4 量化能够节省接近4倍的内存 主要针对 GPU 推理和性能，对 GPU 进行了优化 不需要对模型进行重训练 缺陷：\n对 GPU 要求较高 量化预训练模型带来量化误差 量化和反量化的步骤：\n缩放：将输入张量x除以缩放因子scale。这一步是为了将x的值范围调整到预期的量化范围 四舍五入：将缩放后的结果四舍五入到最近的整数。这一步将x的值离散化，即将其转换为整数 限制范围：使用torch.clamp函数将四舍五入后的结果限制在0和maxq之间，确保量化后的值不会超出预期的量化范围 反量化：将量化后的张量减去零点zero，然后乘以缩放因子scale。这一步是为了将量化后的值恢复到原始的值范围 一般来说，GPTQ推荐使用 8-bit 量化及 groupsize = 128。\n使用 AutoGPTQ 量化模型 # AutoGPTQ 是基于 GPTQ 算法、有用户友好型接口的 LLM 量化 toolkit，AutoGPTQ 代码库已被集成到 Transformers 中，可以使用 GPTQ 算法在 8 bit、4 bit、3 bit、2 bit 精度下量化和运行模型\n安装 GPTQ\ngit clone https://github.com/AutoGPTQ/AutoGPTQ \u0026amp;\u0026amp; cd AutoGPTQ pip install -e . 构建 GPTQ 量化模型需要使用训练数据进行校准。以单卡 GPU 进行量化为例：\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig from transformers import AutoTokenizer import logging logging.basicConfig( format=\u0026#34;%(asctime)s %(levelname)s [%(name)s] %(message)s\u0026#34;, level=logging.INFO, datefmt=\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34; ) model_path = \u0026#34;model_path\u0026#34; quant_path = \u0026#34;quantized_model_path\u0026#34; quantize_config = BaseQuantizeConfig( bits=8, # INT4 or INT8 group_size=128, # 量化 group damp_percent=0.01, desc_act=False, # set to False can significantly speed up inference but the perplexity may slightly bad static_groups=False, sym=True, true_sequential=True, model_name_or_path=None, model_file_base_name=\u0026#34;model\u0026#34; ) max_len = 8192 tokenizer = AutoTokenizer.from_pretrained(model_path) model = AutoGPTQForCausalLM.from_pretrained(model_path, quantize_config) # 使用训练数据进行校准 # 样本的数据类型应该为 List[Dict]，其中字典的键有且仅有 input_ids 和 attention_mask data = [{input_ids:\u0026#34;\u0026#34;, attention_mask: \u0026#34;\u0026#34;}] model.quantize(data, cache_examples_on_gpu=False) # 保存模型, 不支持模型分片 model.save_quantized(quant_path, use_safetensors=True) tokenizer.save_pretrained(quant_path) 如果使用多个 GPU，需要配置 使用 max_memory 而不是 device_map：\nmodel = AutoGPTQForCausalLM.from_pretrained( model_path, quantize_config, max_memory={i:\u0026#34;20GB\u0026#34; for i in range(4)} # 每个 GPU 的内存配置 ) 在 Transformers 中加载 GPTQ 模型 # Transformers, optimum, peft 已支持 AutoGPTQ，可以直接在 Transformers 中使用量化后的模型。以 Qwen1.5-7B-Chat-GPTQ-Int8 为例：\nfrom transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; # the device to load the model onto model = AutoModelForCausalLM.from_pretrained( \u0026#34;Qwen/Qwen1.5-7B-Chat-GPTQ-Int8\u0026#34;, # the quantized model device_map=\u0026#34;auto\u0026#34; ) tokenizer = AutoTokenizer.from_pretrained(\u0026#34;Qwen/Qwen1.5-7B-Chat-GPTQ-Int8\u0026#34;) prompt = \u0026#34;What is AI?\u0026#34; messages = [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: prompt} ] text = tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True ) model_inputs = tokenizer([text], return_tensors=\u0026#34;pt\u0026#34;).to(device) generated_ids = model.generate( model_inputs.input_ids, max_new_tokens=512 ) generated_ids = [ output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids) ] response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] 在 vLLM 中加载 GPTQ 量化模型 # python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen1.5-7B-Chat-GPTQ-Int8\nSmoothQuant # LLM 量化的挑战之一是激活值比权重更难量化，因为权重数据分布一般比较均匀，而激活的异常值多且大让激活值量化变得更艰难，但是异常值只存在少数通道。单一 token 方差很大（异常值会存在于每一个 token 中），单一 channel 方差会小很多。\nSmoothQuant 是一种同时确保准确率且推理高效的训练后量化 (PTQ) 方法，可实现 8bit 权重量化（W8A16）、8bit 全量化（W8A8），它引入平滑因子 s 来平滑激活的异常值，通过数学等效变换将量化难度从激活转移到权重上。\nSmoothQuant 对激活进行平滑，按通道（列）除以 smoothing factor，同时为了保持 liner layer 数学上的等价性，以相反的方式对权重进行对应调整。\nSmoothQuant 证明自己可以无损地量化（8bit）所有超过100B参数的开源LLM。通过集成到PyTorch和FasterTransformer中，与 FP16 相比，获得高达1.56倍的推理加速，并将内存占用减半，并且模型越大，加速效果越明显。\n与其他量化方法相比，SmoothQuant 可以保持较高的精度，同时具有更低的延迟。\n目前，SmoothQuant 已经被集成到 TensorRT-LLM(NVIDIA) 和 Neural-Compressor(Intel) 中。\nAWQ # AWQ(Activation-aware Weight Quantization), 即激活感知权重量化，是一种针对 LLM 的、硬件友好的、低比特权重量化方法，同时支持 CPU、GPU。\nAWQ 源于一个观察，即权重对于LLM的性能并不同等重要：存在约（0.1%-1%）的显著权重（salient weight）对大模型性能影响很大，跳过这1%的显著权重（不量化），可大大减少量化误差。\nAWQ 通过观察激活分布而非权重分布来寻找保护显著权重的最佳每通道缩放比例（per-channel），在量化过程中会对特殊权重进行特殊处理以减轻量化过程中的精度损失，在和GPTQ量化保持类似推理速度的同时可以具备更好的精度。\n除了官方支持 llm-awq以外，AutoAWQ、vLLM、HuggingFace TGI、LMDeploy、TensorRT-LLM、FastChat 等都支持 AWQ\nAutoAWQ # 基于 AWQ 的 量化工具包，与 FP16 相比，AutoAWQ 使用4bit量化将模型速度提高了 3 倍，并将对内存需求降低了 3 倍。Transformers 已经集成了 AutoAWQ\n使用autoawq量化模型\nfrom awq import AutoAWQForCausalLM from transformers import AutoTokenizer model_path = \u0026#34;facebook/opt-125m\u0026#34; quant_path = \u0026#34;opt-125m-awq\u0026#34; quant_config = { \u0026#34;zero_point\u0026#34;: True, \u0026#34;q_group_size\u0026#34;: 128, \u0026#34;w_bit\u0026#34;: 4, \u0026#34;version\u0026#34;:\u0026#34;GEMM\u0026#34;} # Load model model = AutoAWQForCausalLM.from_pretrained(model_path) tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True) # Quantize model.quantize(tokenizer, quant_config=quant_config) 加载 awq 模型\nfrom transformers import AutoTokenizer, AutoModelForCausalLM quant_path=\u0026#34;opt-125m-awq\u0026#34; tokenizer = AutoTokenizer.from_pretrained(quant_path) model = AutoModelForCausalLM.from_pretrained(quant_path).to(0) text = \u0026#34;What is AI?\u0026#34; inputs = tokenizer(text, return_tensors=\u0026#34;pt\u0026#34;).to(0) out = model.generate(**inputs, max_new_tokens=5) print(tokenizer.decode(out[0], skip_special_tokens=True)) GGUF # GGUF(GPT-Generated Unified Format)，以前称为 GGML(General Matrix Multiply Library)，GGUF格式较新，可以保留模型版本等其他自定义信息。这两种格式也是PTQ形式的量化算法。允许用户使用 CPU 来运行 LLM，它专注于优化矩阵乘，以提高量化后的计算效率，适用于在资源受限的设备。\n加载 GGUF 模型 # pip install ctransformers[cuda]\nfrom ctransformers import AutoModelForCausalLM from transformers import AutoTokenizer, pipeline # Use `gpu_layers` to specify how many layers will be offloaded to the GPU. model = AutoModelForCausalLM.from_pretrained( \u0026#34;TheBloke/zephyr-7B-beta-GGUF\u0026#34;, model_file=\u0026#34;zephyr-7b-beta.Q4_K_M.gguf\u0026#34;, model_type=\u0026#34;mistral\u0026#34;, gpu_layers=50, hf=True ) tokenizer = AutoTokenizer.from_pretrained( \u0026#34;HuggingFaceH4/zephyr-7b-beta\u0026#34;, use_fast=True ) # Create a pipeline pipe = pipeline(model=model, tokenizer=tokenizer, task=\u0026#39;text-generation\u0026#39;) # Inference outputs = pipe(prompt, max_new_tokens=256) print(outputs[0][\u0026#34;generated_text\u0026#34;]) 总结 # Method Type Need Dataset Scale 粒度 DateType Hardware LLM.int8() PTQ No per-channel 8bit,4bit GPU GPTQ PTQ Yes per-group 8bit,4bit GPU SmothQuant PTQ No per-tensor, per-token 8bit GPU AWQ PTQ No per-channel 4bit GPU,CPU Reference\nGPTQ AutoGPTQ smoothquant AWQ Which Quantization Method is Right for You? (GPTQ vs. GGUF vs. AWQ) 大模型量化技术原理-LLM.int8,GPTQ 大模型量化技术原理-SmoothQuant 量化技术解析 ","date":"2024-05-28","externalUrl":null,"permalink":"/posts/llm/quantize_ptq/","section":"Blogs","summary":"PTQ # PTQ(Post-Training Quantization)，即后训","title":"PTQ for LLM","type":"posts"},{"content":"","date":"2024-05-28","externalUrl":null,"permalink":"/tags/transformer/","section":"Tags","summary":"","title":"Transformer","type":"tags"},{"content":" Download Llama3 Model Weight # Llama3\nDownload Llama3 weights from https://llama.meta.com/llama-downloads/\nInstall requirements # pip install -r requirements.txt Tokenizer # Use tiktoken as the tokenizer\nfrom pathlib import Path import tiktoken from tiktoken.load import load_tiktoken_bpe import torch import json import matplotlib.pyplot as plt tokenizer_path = \u0026#34;/home/sdp/models/Meta-Llama-3-8B/tokenizer.model\u0026#34; special_tokens = [ \u0026#34;\u0026lt;|begin_of_text|\u0026gt;\u0026#34;, \u0026#34;\u0026lt;|end_of_text|\u0026gt;\u0026#34;, \u0026#34;\u0026lt;|reserved_special_token_0|\u0026gt;\u0026#34;, \u0026#34;\u0026lt;|reserved_special_token_1|\u0026gt;\u0026#34;, \u0026#34;\u0026lt;|reserved_special_token_2|\u0026gt;\u0026#34;, \u0026#34;\u0026lt;|reserved_special_token_3|\u0026gt;\u0026#34;, \u0026#34;\u0026lt;|start_header_id|\u0026gt;\u0026#34;, \u0026#34;\u0026lt;|end_header_id|\u0026gt;\u0026#34;, \u0026#34;\u0026lt;|reserved_special_token_4|\u0026gt;\u0026#34;, \u0026#34;\u0026lt;|eot_id|\u0026gt;\u0026#34;, # end of turn ] + [f\u0026#34;\u0026lt;|reserved_special_token_{i}|\u0026gt;\u0026#34; for i in range(5, 256 - 5)] mergeable_ranks = load_tiktoken_bpe(tokenizer_path) tokenizer = tiktoken.Encoding( name=Path(tokenizer_path).name, pat_str=r\u0026#34;(?i:\u0026#39;s|\u0026#39;t|\u0026#39;re|\u0026#39;ve|\u0026#39;m|\u0026#39;ll|\u0026#39;d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\u0026#34;, mergeable_ranks=mergeable_ranks, special_tokens={token: len(mergeable_ranks) + i for i, token in enumerate(special_tokens)}, ) tokenizer.decode(tokenizer.encode(\u0026#34;Im AI!\u0026#34;)) Load model weights and model config # 模型权重：\nmodel_path=\u0026#34;/home/sdp/models/Meta-Llama-3-8B/consolidated.00.pth\u0026#34; model = torch.load(model_path) print(json.dumps(list(model.keys())[:20], indent=4)) \u0026#34;tok_embeddings.weight\u0026#34;, \u0026#34;layers.0.attention.wq.weight\u0026#34;, \u0026#34;layers.0.attention.wk.weight\u0026#34;, \u0026#34;layers.0.attention.wv.weight\u0026#34;, \u0026#34;layers.0.attention.wo.weight\u0026#34;, \u0026#34;layers.0.feed_forward.w1.weight\u0026#34;, \u0026#34;layers.0.feed_forward.w3.weight\u0026#34;, \u0026#34;layers.0.feed_forward.w2.weight\u0026#34;, \u0026#34;layers.0.attention_norm.weight\u0026#34;, \u0026#34;layers.0.ffn_norm.weight\u0026#34;, \u0026#34;layers.1.attention.wq.weight\u0026#34;, \u0026#34;layers.1.attention.wk.weight\u0026#34;, \u0026#34;layers.1.attention.wv.weight\u0026#34;, \u0026#34;layers.1.attention.wo.weight\u0026#34;, \u0026#34;layers.1.feed_forward.w1.weight\u0026#34;, \u0026#34;layers.1.feed_forward.w3.weight\u0026#34;, \u0026#34;layers.1.feed_forward.w2.weight\u0026#34;, \u0026#34;layers.1.attention_norm.weight\u0026#34;, \u0026#34;layers.1.ffn_norm.weight\u0026#34;, \u0026#34;layers.2.attention.wq.weight\u0026#34; 模型的配置信息：\n32 个 DecoderLayer 每个 AttentionLayer(GQA) 有 32 个 Query Head, 8 个 KV Group 每 4 个 Head 共享一个 KV， 每个 Q head 的 size 是 dim/32=128 每个 KV group 的 size 是 dim/4=1024 分词表大小为128256 with open(\u0026#34;/home/sdp/models/Meta-Llama-3-8B/params.json\u0026#34;, \u0026#34;r\u0026#34;) as f: config = json.load(f) config {\u0026#39;dim\u0026#39;: 4096, \u0026#39;n_layers\u0026#39;: 32, \u0026#39;n_heads\u0026#39;: 32, \u0026#39;n_kv_heads\u0026#39;: 8, \u0026#39;vocab_size\u0026#39;: 128256, \u0026#39;multiple_of\u0026#39;: 1024, \u0026#39;ffn_dim_multiplier\u0026#39;: 1.3, \u0026#39;norm_eps\u0026#39;: 1e-05, \u0026#39;rope_theta\u0026#39;: 500000.0} 将config信息存入变量\ndim = config[\u0026#34;dim\u0026#34;] n_layers = config[\u0026#34;n_layers\u0026#34;] n_heads = config[\u0026#34;n_heads\u0026#34;] n_kv_heads = config[\u0026#34;n_kv_heads\u0026#34;] vocab_size = config[\u0026#34;vocab_size\u0026#34;] multiple_of = config[\u0026#34;multiple_of\u0026#34;] ffn_dim_multiplier = config[\u0026#34;ffn_dim_multiplier\u0026#34;] norm_eps = config[\u0026#34;norm_eps\u0026#34;] rope_theta = torch.tensor(config[\u0026#34;rope_theta\u0026#34;]) Prepare prompt tokens # prompt = \u0026#34;the answer to the ultimate question of life, the universe, and everything is \u0026#34; tokens = [128000] + tokenizer.encode(prompt) print(len(tokens)) # len=17 print(tokens) tokens = torch.tensor(tokens) prompt_split_as_tokens = [tokenizer.decode([token.item()]) for token in tokens] print(prompt_split_as_tokens) input 的 tokens 长度是17,即 seq_len = 17， 则此时可以推算出的 model 中部分参数维度：\nseq_len 17 embed_dim 4096 embed_output (seq_len, embed_dim) (17, 4096) attention q_head_num 32 attention q_head_size 128 attention kv_group_num 4 attention kv_head_size 1024 attention.wq.weight (q_head_num, seq_len, q_head_size) (32, 17, 128) attention.wk.weight (kv_group_num, seq_len, kv_head_size) (4, 17, 1024) attention.wv.weight (kv_group_num, seq_len, kv_head_size) (4, 17, 1024) attention.wo.weight feed_forward.w1.weight feed_forward.w3.weight feed_forward.w2.weight attention_norm.weight ffn_norm.weight Embedding layer # 将input tokens转换为embedding，(17,1) -\u0026gt; (17,4096)\nembedding_layer = torch.nn.Embedding(vocab_size, dim) embedding_layer.weight.data.copy_(model[\u0026#34;tok_embeddings.weight\u0026#34;]) token_embeddings_unnormalized = embedding_layer(tokens).to(torch.bfloat16) token_embeddings_unnormalized.shape # torch.Size([17, 4096]) RMS Normalization # 用RMS对embedding进行归一化，这里使用torch的rsqrt求均值方差, 归一化后的Tensor形状不变\n[Todo: CUDA LayerNorm Kernel] 更好的方法是编写专用的 RMSNorm 算子(kernel)\ndef rms_norm(tensor, norm_weights): return (tensor * torch.rsqrt(tensor.pow(2).mean(-1, keepdim=True) + norm_eps)) * norm_weights token_embeddings = rms_norm(token_embeddings_unnormalized, model[\u0026#34;layers.0.attention_norm.weight\u0026#34;]) token_embeddings.shape # torch.Size([17, 4096]) 接下来实现 Model 的主体 DecoderLayer\nDecoderLayer # 一共 32 个 DecoderLayer， 每个 Layer 含有 1 个 AttentionLayer 和 1 个 FFNLayer\n为了方便识别，假设 embedding 层的输出 token_embeddings_unnormalized 为 x\n每一层DecoderLayer的工作流程：\nx -\u0026gt; rmsnorm -\u0026gt; AttentionLayer -\u0026gt; attention output attention output += x attention output -\u0026gt; rmsnorm -\u0026gt; FFNLayer -\u0026gt; ffn output output = ffn output + attention output AttentionLayer # 输入为norm后的 x， 维度为(17, 4096), 查看 attention weight 的维度\nprint( model[\u0026#34;layers.0.attention.wq.weight\u0026#34;].shape, model[\u0026#34;layers.0.attention.wk.weight\u0026#34;].shape, model[\u0026#34;layers.0.attention.wv.weight\u0026#34;].shape, model[\u0026#34;layers.0.attention.wo.weight\u0026#34;].shape ) # torch.Size([4096, 4096]) # torch.Size([1024, 4096]) # torch.Size([1024, 4096]) # torch.Size([4096, 4096]) attention 要计算 \\(softmax(QK^T)V\\)， 其中 Q,K,V 都用 attention input（17, 4096） 与 相应的 weight(wq, wk, wv) 计算出来的, 则根据矩阵乘的原则可以推算出 QKV 各自的维度：\nQ (17, 128) K (17, 1024) V (17, 1024) Query # q_layer0 = model[\u0026#34;layers.0.attention.wq.weight\u0026#34;] head_dim = q_layer0.shape[0] // n_heads q_layer0 = q_layer0.view(n_heads, head_dim, dim) print(q_layer0.shape) q_layer0_head0 = q_layer0[0] print(q_layer0_head0.shape) q_per_token = torch.matmul(token_embeddings, q_layer0_head0.T) print(q_per_token.shape) # torch.Size([32, 128, 4096]) # torch.Size([128, 4096]) # torch.Size([17, 128]) Q和V都要经过 RoPE 进行旋转位置编码， 因为注意力机制中对每个token没有序列位置的概念，第一个词和最后一个词在Q、K、V矩阵看来都是一样的，因此需要在Query中嵌入维度为[1x128]的位置编码。位置编码有多种方法，Llama模型采用的是旋转位置编码 RoPE\n# 让 q 两两成对，共64对 q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2) print(q_per_token_split_into_pairs.shape) # 句子中在index位置的一对查询向量，旋转角度为index*(rope_theta) zero_to_one_split_into_64_parts = torch.tensor(range(64))/64 print(zero_to_one_split_into_64_parts) freqs = 1.0 / (rope_theta ** zero_to_one_split_into_64_parts) print(freqs) # 构建freq_cis矩阵，存储句子中每个位置的、对查询向量每个值的旋转角度 freqs_for_each_token = torch.outer(torch.arange(17), freqs) freqs_cis = torch.polar(torch.ones_like(freqs_for_each_token), freqs_for_each_token) # 将每对查询向量转换为复数，之后进行与旋转角度进行点积操作 q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs) print(q_per_token_as_complex_numbers.shape) q_per_token_as_complex_numbers_rotated = q_per_token_as_complex_numbers * freqs_cis print(q_per_token_as_complex_numbers_rotated.shape) # 把旋转后的查询向量转换回实数形式, 恢复原始维度 q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers_rotated) print(q_per_token_split_into_pairs_rotated.shape) q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape) print(q_per_token_rotated.shape) torch.Size([17, 64, 2]) tensor([0.0000, 0.0156, 0.0312, 0.0469, 0.0625, 0.0781, 0.0938, 0.1094, 0.1250, 0.1406, 0.1562, 0.1719, 0.1875, 0.2031, 0.2188, 0.2344, 0.2500, 0.2656, 0.2812, 0.2969, 0.3125, 0.3281, 0.3438, 0.3594, 0.3750, 0.3906, 0.4062, 0.4219, 0.4375, 0.4531, 0.4688, 0.4844, 0.5000, 0.5156, 0.5312, 0.5469, 0.5625, 0.5781, 0.5938, 0.6094, 0.6250, 0.6406, 0.6562, 0.6719, 0.6875, 0.7031, 0.7188, 0.7344, 0.7500, 0.7656, 0.7812, 0.7969, 0.8125, 0.8281, 0.8438, 0.8594, 0.8750, 0.8906, 0.9062, 0.9219, 0.9375, 0.9531, 0.9688, 0.9844]) tensor([1.0000e+00, 8.1462e-01, 6.6360e-01, 5.4058e-01, 4.4037e-01, 3.5873e-01, 2.9223e-01, 2.3805e-01, 1.9392e-01, 1.5797e-01, 1.2869e-01, 1.0483e-01, 8.5397e-02, 6.9566e-02, 5.6670e-02, 4.6164e-02, 3.7606e-02, 3.0635e-02, 2.4955e-02, 2.0329e-02, 1.6560e-02, 1.3490e-02, 1.0990e-02, 8.9523e-03, 7.2927e-03, 5.9407e-03, 4.8394e-03, 3.9423e-03, 3.2114e-03, 2.6161e-03, 2.1311e-03, 1.7360e-03, 1.4142e-03, 1.1520e-03, 9.3847e-04, 7.6450e-04, 6.2277e-04, 5.0732e-04, 4.1327e-04, 3.3666e-04, 2.7425e-04, 2.2341e-04, 1.8199e-04, 1.4825e-04, 1.2077e-04, 9.8381e-05, 8.0143e-05, 6.5286e-05, 5.3183e-05, 4.3324e-05, 3.5292e-05, 2.8750e-05, 2.3420e-05, 1.9078e-05, 1.5542e-05, 1.2660e-05, 1.0313e-05, 8.4015e-06, 6.8440e-06, 5.5752e-06, 4.5417e-06, 3.6997e-06, 3.0139e-06, 2.4551e-06]) torch.Size([17, 64]) torch.Size([17, 64]) torch.Size([17, 64, 2]) torch.Size([17, 128]) Key # 经过 RoPE 后 的 Q 维度不变，下一步计算 K，计算方法与 Q 类似，也需要 RoPE， 但维度不同，因为 Q 有 32 个 head， 而 K 和 V 有 8 个 group head, 每个 K和V 被 4 个 Q 共享。\n用于计算 K 的 权重维度是(1024, 4096), 最终得到的 K 的维度 是 (17,128)\n# 将 k 的权重 分成 n_kv_heads=8 组 k_layer0 = model[\u0026#34;layers.0.attention.wk.weight\u0026#34;] k_layer0 = k_layer0.view(n_kv_heads, k_layer0.shape[0] // n_kv_heads, dim) print(k_layer0.shape) # 每组 k 权重 的维度是 （128, 4096） k_layer0_head0 = k_layer0[0] print(k_layer0_head0.shape) # 矩阵乘，计算 k， （17, 4096）* (4096, 128）得到 k 的维度 (17, 128) k_per_token = torch.matmul(token_embeddings, k_layer0_head0.T) print(k_per_token.shape) torch.Size([8, 128, 4096]) torch.Size([128, 4096]) torch.Size([17, 128]) 对 K 进行旋转位置编码，编码后的 K 维度不变\nk_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2) print(k_per_token_split_into_pairs.shape) k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs) print(k_per_token_as_complex_numbers.shape) k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis) print(k_per_token_split_into_pairs_rotated.shape) k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape) print(k_per_token_rotated.shape) torch.Size([17, 64, 2]) torch.Size([17, 64]) torch.Size([17, 64, 2]) torch.Size([17, 128]) QK^T # Q 和 K 的维度 都是 torch.Size([17, 128]), 通过矩阵乘得到 \\(QK^T/sqrt(head_dim)\\) 矩阵， 矩阵中的每个值都代表了对应位置 token 的 Q 和 K 的相关程度， 这就是 self-attention 的过程\nqk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(head_dim)**0.5 qk_per_token.shape Mask # 为了只保留每个 token 与他前面的[0\u0026hellip;token]的注意力，将token位置之后的 QK 结果屏蔽，方法也很简单，创建一个上三角为负无穷、下三角和对角线为0的 mask 矩阵，然后与 \\(QK^T/sqrt(head_dim)\\) 相加即可\nmask = torch.full((len(tokens), len(tokens)), float(\u0026#34;-inf\u0026#34;), device=tokens.device) mask = torch.triu(mask, diagonal=1) qk_per_token_after_masking = qk_per_token + mask print(mask) Softmax # 未完待续\u0026hellip;\nFFNLayer # Reference:\nMeta-llama3 llama3-from-scratch ","date":"2024-05-28","externalUrl":null,"permalink":"/posts/llm/llama3/","section":"Blogs","summary":"Download Llama3 Model Weight # Llama3 Download Llama3 weights from https://llama.meta.com/llama-downloads/ Install requirements # pip install -r requirements.txt Tokenizer #","title":"Implement Llama3 in Python and Quantitative Analysis","type":"posts"},{"content":"","date":"2024-05-28","externalUrl":null,"permalink":"/tags/llama/","section":"Tags","summary":"","title":"Llama","type":"tags"},{"content":"","date":"2024-05-28","externalUrl":null,"permalink":"/series/llm-architecture/","section":"Series","summary":"","title":"LLM Architecture","type":"series"},{"content":"","date":"2024-05-28","externalUrl":null,"permalink":"/tags/dynamic-programming/","section":"Tags","summary":"","title":"Dynamic Programming","type":"tags"},{"content":" 二维动态规划的基本思路 # 基本思路与一维 DP 类似：\n首先完成递归解法，2维递归的时间复杂度\\(O(n^3)))，空间复杂度\\(O((log_n)^2)\\)，时间复杂度高主要原因是存在重复计算 递归时，要确定 base case，即判断递归到达底部边界，及其返回值 二维递归的特点是f(i,j)的结果可能依赖于前面的多种情况，比如f(i-1,j-1), f(i-1,j), f(i, j-1)等 递归转换为带有2维 DP 缓存表的、递归版的、自顶向底的 DP 顶指的是最终目标答案，底指的是base case 递归版 DP 基本只是在递归解法的基础上增加了 DP 表，其他逻辑照抄 以空间换时间，dp[i][j]存f(i,j)的结果，递归时，如果 dp[i][j] 已存在则直接返回结果，否则继续递归，并将结果存入dp[i][j]，这样就减少了重复计算 时间复杂度\\(O(n)))，空间复杂度\\(O(n)\\) 将递归版的DP转化为严格位置依赖的、迭代版的、自底向顶的DP dp[i] 依赖于若干个 dp[\u0026lt;i] 的值，最终结果是 dp[n] 时间复杂度\\(O(n)))，空间复杂度\\(O(n)\\) 继续优化DP，空间压缩 用有限个变量的滚动更新代替dp数组，比如 cur = last + lastlast 时间复杂度\\(O(n)))，空间复杂度\\(O(1)\\) leetcode 64.最小路径和[中等] # 64.最小路径和\n给定一个包含非负整数的 m x n 网格 grid ，请找出一条从左上角到右下角的路径，使得路径上的数字总和为最小。\n说明：每次只能向下或者向右移动一步。\nclass Solution { public: int minPathSum(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid) { // return f1(grid, grid.size()-1, grid[0].size()-1); // return f2(grid); // return f3(grid); return f4(grid); } //递归：只能想下或向右移动, f(i,j) = max(f(i-1,j), f(i,j-1))+grid[i,j] int f1(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid, int i, int j){ if(i==0 \u0026amp;\u0026amp; j==0){ return grid[0][0]; } int up=INT_MAX; int left=INT_MAX; if(i-1\u0026gt;=0){ up = f1(grid,i-1,j); } if(j-1\u0026gt;=0){ left = f1(grid,i,j-1); } return min(up, left)+grid[i][j]; } //DP int f2(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid){ vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; dp(grid.size(), vector\u0026lt;int\u0026gt;(grid[0].size(), -1));//存放(i,j)位置的最短路径 dp[0][0] = grid[0][0]; return f2_dp1(grid, grid.size()-1, grid[0].size()-1, dp); } //dp_1，递归版、自顶向底的DP int f2_dp1(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid, int i, int j, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; dp){ if(dp[i][j] != -1) return dp[i][j]; int up=INT_MAX; int left=INT_MAX; if(i-1\u0026gt;=0){ up = f2_dp1(grid,i-1,j,dp); } if(j-1\u0026gt;=0){ left = f2_dp1(grid,i,j-1,dp); } dp[i][j]= min(up, left)+grid[i][j]; return dp[i][j]; } // 优化DP， 递归转迭代, 严格位置依赖的动态规划 // 从左往右，从上往下，一行一行的计算dp[i][j] int f3(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid){ vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; dp(grid.size(), vector\u0026lt;int\u0026gt;(grid[0].size(), -1));//存放(i,j)位置的最短路径 dp[0][0] = grid[0][0]; for(int i=1; i\u0026lt;grid.size(); i++){ dp[i][0] = dp[i-1][0]+grid[i][0]; } for(int j=1; j\u0026lt;grid[0].size(); j++){ dp[0][j] = dp[0][j-1]+grid[0][j]; } for(int i=1; i\u0026lt;grid.size(); i++){ for(int j=1; j\u0026lt;grid[0].size(); j++){ dp[i][j] = min(dp[i-1][j], dp[i][j-1]) + grid[i][j]; } } return dp[grid.size()-1][grid[0].size()-1]; } // 继续优化DP，减小dp数组，二位dp缓存表 先转成2个一维数组，再优化就是一个以为数组，grid每一行复用 int f4(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid){ vector\u0026lt;int\u0026gt; dp(grid[0].size()); dp[0] = grid[0][0]; for(int j=1; j\u0026lt;grid[0].size(); j++){ dp[j] = dp[j-1]+grid[0][j]; } for(int i=1; i\u0026lt;grid.size(); i++){ // i = 1，dp表变成想象中二维表的第1行的数据 // ... // i = n-1，dp表变成想象中二维表的第n-1行的数据 //先更新一下这一行左侧第一个值，相当与原来的dp[i][j=0] dp[0] += grid[i][0]; for(int j=1; j\u0026lt;grid[0].size(); j++){ // dp[j]代表了原来的up=dp[i][j-1]的值，dp[j-1]原来的left=dp[i-1][j]的值 dp[j] = min(dp[j], dp[j-1]) + grid[i][j]; } } return dp[grid[0].size()-1]; } }; leetcode 1147. 单词搜索[中等] # 1147. 单词搜索\n给定一个 m x n 二维字符网格 board 和一个字符串单词 word 。如果 word 存在于网格中，返回 true ；否则，返回 false 。\n单词必须按照字母顺序，通过相邻的单元格内的字母构成，其中“相邻”单元格是那些水平相邻或垂直相邻的单元格。同一个单元格内的字母不允许被重复使用。\nclass Solution { public: bool exist(vector\u0026lt;vector\u0026lt;char\u0026gt;\u0026gt;\u0026amp; board, string word) { // 带路径的递归 // 双循环遍历每一个位置，从该位置作起点，用递归法搜索相邻位置，看是够能够完全匹配当w // 递归时，退出条件是:1.匹配到完整的w，返回true；2.无法匹配，返回false。如果当前字符匹配，则继续通过相邻位置去匹配下一个字符。走过的位置用mask掩盖下，防止后续搜索中重复过去的位置 int is = board.size(), js = board[0].size(); for(int r = 0; r \u0026lt; is; r++){ for(int c = 0; c \u0026lt; js; c++){ if(f1(board, word, 0, r, c)){ return true; } } } return false; } // k代表word需要匹配的位置 bool f1(vector\u0026lt;vector\u0026lt;char\u0026gt;\u0026gt;\u0026amp; board, string\u0026amp; word, int k, int i, int j){ if(k == word.size()){ return true; } if(i \u0026lt; 0 || j \u0026lt; 0 || i \u0026gt;= board.size() || j \u0026gt;= board[0].size() || board[i][j] != word[k]){ return false; } // 加入mask，但要保留字符用于递归结束的回填 auto tmp = board[i][j]; board[i][j] = \u0026#39;0\u0026#39;;// 这时如果下面的递归中再来到这个位置，一定有b[i][j] != w[k] bool result = f1(board, word, k + 1, i - 1, j); result = result || f1(board, word, k + 1, i + 1, j); result = result || f1(board, word, k + 1, i, j - 1); result = result || f1(board, word, k + 1, i, j + 1); board[i][j] = tmp; return result; } }; leetcode 1143. 最长公共子序列[中等] # 1143. 最长公共子序列\n给定两个字符串 text1 和 text2，返回这两个字符串的最长 公共子序列 的长度。如果不存在 公共子序列 ，返回 0 。\n一个字符串的 子序列 是指这样一个新的字符串：它是由原字符串在不改变字符的相对顺序的情况下删除某些字符（也可以不删除任何字符）后组成的新字符串。\n例如，\u0026ldquo;ace\u0026rdquo; 是 \u0026ldquo;abcde\u0026rdquo; 的子序列，但 \u0026ldquo;aec\u0026rdquo; 不是 \u0026ldquo;abcde\u0026rdquo; 的子序列。 两个字符串的 公共子序列 是这两个字符串所共同拥有的子序列。\n示例 1：\n输入：text1 = \u0026ldquo;abcde\u0026rdquo;, text2 = \u0026ldquo;ace\u0026rdquo; 输出：3\n解释：最长公共子序列是 \u0026ldquo;ace\u0026rdquo; ，它的长度为 3 。 示例 2：\n输入：text1 = \u0026ldquo;abc\u0026rdquo;, text2 = \u0026ldquo;abc\u0026rdquo; 输出：3 解释：最长公共子序列是 \u0026ldquo;abc\u0026rdquo; ，它的长度为 3 。 示例 3：\n输入：text1 = \u0026ldquo;abc\u0026rdquo;, text2 = \u0026ldquo;def\u0026rdquo; 输出：0 解释：两个字符串没有公共子序列，返回 0 。\nclass Solution { public: int longestCommonSubsequence(string text1, string text2) { return f3(text1,text2); } // 递归法(超时) // a[i],b[j]的结果依赖于三种情况 int f1(string text1, string text2){ return f1_help(text1,text2,text1.size(),text2.size()); } int f1_help(string a, string b, int len1, int len2){ if(len1==0 || len2==0) return 0; int ans; if(a[len1-1]==b[len2-1]){ ans = 1+f1_help(a,b,len1-1,len2-1); }else{ ans = max(f1_help(a,b,len1-1,len2), f1_help(a,b,len1,len2-1));//还有一种可能f1_help(a,b,i-1,j-1)，必然小于这两种可能，所以直接舍弃 } return ans; } //DP，递归直接转dp表，自顶向底的DP int f2(string text1, string text2){ int n=text1.size(); int m=text2.size(); vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; dp(n+1, vector\u0026lt;int\u0026gt;(m+1,-1)); return f2_help(text1,text2,n,m,dp);//[0...n] [0...m] } int f2_help(string a, string b, int len1, int len2, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; \u0026amp;dp){ //二维dp表的第一行和第一列为0 if(len1==0 || len2==0){ return 0; } if(dp[len1][len2]!=-1) { return dp[len1][len2]; } int ans; if(a[len1-1]==b[len2-1]){ ans = 1+f2_help(a,b,len1-1,len2-1,dp); }else{ ans = max(f2_help(a,b,len1-1,len2,dp), f2_help(a,b,len1,len2-1,dp));//还有一种可能f1_help(a,b,i-1,j-1)，必然小于这两种可能，所以直接舍弃 } dp[len1][len2]=ans; return ans; } //DP-2:严格位置依赖的DP（递归转迭代，自底向顶的DP） int f3(string text1, string text2){ int n=text1.size(); int m=text2.size(); vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; dp(n+1, vector\u0026lt;int\u0026gt;(m+1,0)); //二维dp表的第一行和第一列为0，但这两个循环可以省略 // for(int i=0; i\u0026lt;=m;i++){ // dp[0][m]=0; // } // for(int i=0; i\u0026lt;=n;i++){ // dp[n][0]=0; // } //开始更新二维dp表，从上向下，从左到右，按行更新，循环的i、j是len1、len2 //dp[i][j]依赖三个位置，左上，上，左 for(int i=1;i\u0026lt;=n;i++){ for(int j=1;j\u0026lt;=m;j++){ int leftup=dp[i-1][j-1]; int left=dp[i][j-1]; int up=dp[i-1][j]; if(text1[i-1]==text2[j-1]){ dp[i][j] = leftup+1; }else{ dp[i][j] = max(left, up); } } } return dp[n][m]; } //DP-3：对DP2的二维dp表压缩为数组（一行），位置依赖中的up和left很容易，但是leftup需要把前一个的备份一下 // Todo }; leetcode 516. 最长回文子序列[中等] # 516. 最长回文子序列\n给你一个字符串 s ，找出其中最长的回文子序列，并返回该序列的长度。\n子序列定义为：不改变剩余字符顺序的情况下，删除某些字符或者不删除任何字符形成的一个序列。\n示例 1：\n输入：s = \u0026ldquo;bbbab\u0026rdquo; 输出：4 解释：一个可能的最长回文子序列为 \u0026ldquo;bbbb\u0026rdquo; 。 示例 2：\n输入：s = \u0026ldquo;cbbd\u0026rdquo; 输出：2 解释：一个可能的最长回文子序列为 \u0026ldquo;bb\u0026rdquo; 。\nclass Solution { public: int longestPalindromeSubseq(string s) { return f3(s); } // 暴力：搞一个逆转的string，两个比较 // 递归(超时)：left,right两个指针，left-\u0026gt;...\u0026lt;-right // s[left...right]的最长回文子序列，依赖于四种情况 // 1. s[left]==s[right]时，结果为 2 + s[left+1...right-1]的最长回文子序列 // 2, s[left]!=s[right]时，有三种情况,取最大值: // s[left+1...right], s[left...right-1], s[left+1...right-1]，其中欧冠你最后一种可以省略 int f1(string s){ if(s.size()==0) return 0; int l = 0; int r = s.size()-1; return f1_help(s, l, r); } int f1_help(string s, int l, int r){ if(l==r) return 1;//如 a if(l+1==r) return s[l]==s[r] ? 2: 1;//如 aa 或 ab if(s[l]==s[r]){ return 2 + f1_help(s, l+1, r-1); }else{ return max(f1_help(s, l+1,r),f1_help(s, l,r-1)); } } //DP(内存超限): 二维dp表，自顶向底的DP（递归版DP） int f2(string s){ if(s.size()==0) return 0; int l = 0; int r = s.size()-1; vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; dp(s.size(), vector\u0026lt;int\u0026gt;(s.size())); return f2_help(s, l, r, dp); } int f2_help(string s, int l, int r, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; \u0026amp;dp){ if(l==r) return 1;//如 a if(l+1==r) return s[l]==s[r] ? 2: 1;//如 aa 或 ab if(dp[l][r]!=0) return dp[l][r]; int ans; if(s[l]==s[r]){ ans = 2 + f2_help(s, l+1, r-1, dp); }else{ ans = max(f2_help(s, l+1,r,dp),f2_help(s, l,r-1,dp)); } dp[l][r] = ans; return ans; } //DP: 严格位置依赖的DP，迭代版 // dp表的对角线上的值dp[i][i]=1,该值的右侧dp[i][i+1]=s[i]==s[i+1] ? 2: 1 // dp表对角线左下的元素都是l\u0026lt;r，无意义 // dp表中其他的值依赖于三个位置：左侧、下侧、左下侧 // 注意：最终结果是表的右上角，dp表要从下往上，从左往右去更新 int f3(string s){ if(s.size()==0) return 0; int n = s.size(); vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; dp(n, vector\u0026lt;int\u0026gt;(n)); for(int i=n-1; i\u0026gt;=0; i--){ dp[i][i] = 1; if(i+1\u0026lt;n){ dp[i][i+1]=s[i]==s[i+1] ? 2: 1; } for(int j=i+2; j\u0026lt;n; j++){ if(s[i]==s[j]){ dp[i][j] = 2 + dp[i+1][j-1];//左下侧 }else{ dp[i][j] = max(dp[i+1][j], dp[i][j-1]);//左和右侧 } } } return dp[0][n-1]; } //DP继续优化，可以压缩空间，用数组代替而二维表，注意加一个变量缓存迁移个位置的左下侧结果 }; Reference:\n左程云算法\n","date":"2024-05-28","externalUrl":null,"permalink":"/posts/algorithm/leetcode/dp_dim2/","section":"Blogs","summary":"二维动态规划的基本思路 # 基本思路与一维 DP","title":"Dynamic Programming: Dim-2","type":"posts"},{"content":"","date":"2024-05-28","externalUrl":null,"permalink":"/tags/leetcode/","section":"Tags","summary":"","title":"Leetcode","type":"tags"},{"content":"","date":"2024-05-28","externalUrl":null,"permalink":"/posts/algorithm/leetcode/","section":"Blogs","summary":"","title":"LeetCode","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/tags/cuda/","section":"Tags","summary":"","title":"CUDA","type":"tags"},{"content":"CUDA 程序获得高性能的必要（但不充分）条件有：\n数据传输比例较小 核函数的算术强度较高（计算访存比） 核函数中定义的线程数目较多 在编写与优化 CUDA 程序时，要想方设法（设计算法）做到：\n减少主机与设备之间的数据传输 提高核函数的算术强度（计算访存比） 增大核函数的并行规模 ","date":"2024-05-24","externalUrl":null,"permalink":"/posts/cuda/cuda_optimize/","section":"Blogs","summary":"CUDA 程序获得高性能的必要（但不充分）条件有","title":"CUDA Optimization","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/series/cuda-parallel-programming/","section":"Series","summary":"","title":"CUDA Parallel Programming","type":"series"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/attention/","section":"Tags","summary":"","title":"Attention","type":"tags"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/series/attention-and-optimization/","section":"Series","summary":"","title":"Attention and Optimization","type":"series"},{"content":"[WIP]\nFlashAttention V2:\n在V1的基础上减少了非矩阵乘法运算的FLOPs。 通过并行化和任务分配优化提高了计算速度和GPU利用率，性能提升了2-3倍。 Flash-Decoding借鉴了FlashAttention的优点，将并行化维度扩展到keys/values序列长度，提高了推理速度。 Flash-Decoding几乎不用额外存储大量数据到全局内存中，减少了内存开销。 Flash-Decoding++通过异步softmax和统一最大值、flat GEMM优化和双缓冲、启发式数据流和硬件资源适应等方法进一步提高了LLM推理的性能。 ","date":"2024-05-23","externalUrl":null,"permalink":"/posts/llm/flash_attention_2/","section":"Blogs","summary":"[WIP] FlashAttention V2: 在V1的基础上减少了非矩阵乘法运算","title":"Flash Attention V2","type":"posts"},{"content":" Global Memory # 全局内存的访问模式，有合并（coalesced）与非合并（uncoalesced）之分。\n合并访问指的是一个线程束对全局内存的一次访问请求（读或者写）导致最少数量的数据传输，否则称访问是非合并的。\n合并度（degree of coalescing）等于线程束请求的字节数除以由该请求导致的所有数据传输处理的字节数。如果所有数据传输中处理的数据都是线程束所需要的，那么合并度就是 100%，即对应合并访问。也可以将合并度理解为一种资源利用率。利用率越高，核函数中与全局内存访问有关的部分的性能就更好\n顺序的合并访问:\nvoid __global__ add(float *x, float *y, float *z) { int n = threadIdx.x + blockIdx.x * blockDim.x; z[n] = x[n] + y[n]; } add\u0026lt;\u0026lt;\u0026lt;128, 32\u0026gt;\u0026gt;\u0026gt;(x, y, z); 第一个线程块中的线程束将访问数组 x 中第 0-31 个元素，对应 128 字节的连续内存，而且首地址一定是 256 字节的整数倍。这样的访问只需要 4 次数据传输即可完成，所以是合并访问，合并度为 100%\n乱序的合并访问:\nvoid __global__ add_permuted(float *x, float *y, float *z) { int tid_permuted = threadIdx.x ^ 0x1;//是将 0-31 的整数做某种置换（交换两个相邻的数） int n = tid_permuted + blockIdx.x * blockDim.x; z[n] = x[n] + y[n]; } add_permuted\u0026lt;\u0026lt;\u0026lt;128, 32\u0026gt;\u0026gt;\u0026gt;(x, y, z); 第一个线程块中的线程束将依然访问数组 x 中第 0-31 个元素，只不过线程号与数组元素指标不完全一致而已，合并度也为 100%\n不对齐的非合并访问:\nvoid __global__ add_offset(float *x, float *y, float *z) { int n = threadIdx.x + blockIdx.x * blockDim.x + 1; z[n] = x[n] + y[n]; } add_offset\u0026lt;\u0026lt;\u0026lt;128, 32\u0026gt;\u0026gt;\u0026gt;(x, y, z); 第一个线程块中的线程束将访问数组 x 中第 1-32 个元素。假如数组 x 的首地址为 256字节，该线程束将访问设备内存的 260-387 字节。这将触发 5 次数据传输，对应的内存地址分别是256-287 字节、288-319 字节、320-351 字节、352-383 字节和 384-415 字节，合并度为 4/5 = 80%\n跨越式的非合并访问\nvoid __global__ add_stride(float *x, float *y, float *z) { int n = blockIdx.x + threadIdx.x * gridDim.x; z[n] = x[n] + y[n]; } add_stride\u0026lt;\u0026lt;\u0026lt;128, 32\u0026gt;\u0026gt;\u0026gt;(x, y, z); 第一个线程块中的线程束将访问数组 x 中指标为 0、128、256、384 等的元素。每一对数据都不在一个连续的 32 字节的内存片段，故该线程束的访问将触发 32 次数据传输，合并度为 4/32 = 12.5%\nCUDA Kernel - Matrix Transpose # 对于多维数组，x 维度的线程指标 threadIdx.x 是最内层的（变化最快），所以相邻的 threadIdx.x 对应相邻的线程，即对threadIdx.x相邻的数据的访问是连续的。\nCUDA中，顺序读的性能理论上高于非顺序读，但是实际性能一致。因为从帕斯卡架构开始，如果编译器能够判断一个全局内存变量在整个核函数的范围都只可读（如这里的矩阵 A），则会自动用函数 __ldg() 读取全局内存，从而对数据的读取进行缓存，缓解非合并访问带来的影响。\n但是写操作没有这种自动配置，所以 顺序写的实际性能高于非顺序写。所以，CUDA中访问全局内存时，要注意优先做到顺序写。\n__global__ void transpose1(const real *A, real *B, const int N) { const int nx = blockIdx.x * blockDim.x + threadIdx.x; const int ny = blockIdx.y * blockDim.y + threadIdx.y; if (nx \u0026lt; N \u0026amp;\u0026amp; ny \u0026lt; N) { B[nx * N + ny] = A[ny * N + nx]; //顺序读，非顺序写 } } __global__ void transpose2(const real *A, real *B, const int N) { const int nx = blockIdx.x * blockDim.x + threadIdx.x; const int ny = blockIdx.y * blockDim.y + threadIdx.y; if (nx \u0026lt; N \u0026amp;\u0026amp; ny \u0026lt; N) { B[ny * N + nx] = A[nx * N + ny];//顺序写，非顺序读 } } __global__ void transpose3(const real *A, real *B, const int N) { const int nx = blockIdx.x * blockDim.x + threadIdx.x; const int ny = blockIdx.y * blockDim.y + threadIdx.y; if (nx \u0026lt; N \u0026amp;\u0026amp; ny \u0026lt; N) { B[ny * N + nx] = __ldg(\u0026amp;A[nx * N + ny]);//顺序写，自动化 } } 并行配置：\nint N=1024; int TILE_DIM = 32;//32 = float32的大小，一个thread访问一个float32数据 int grid_x = (N+TILE_DIM-1)/TILE_DIM); dim3 grid(grid_x, grid_x); dim3 block(32,32)//block最多1024个threads int M = sizeof(real) * N2; real *d_A, *d_B; cudaMallocManaged(\u0026amp;d_A, M); cudaMallocManaged(\u0026amp;d_B, M); 在Tesla T4上测试结果：\ntranspose with coalesced read: Time = 0.193677 +- 0.000643045 ms. transpose with coalesced write: Time = 0.129763 +- 0.00107203 ms. transpose with coalesced write and __ldg read: Time = 0.130074 +- 0.00122755 ms. Shared memory # 上面的矩阵转置例子中，对全局内存的读和写这两个操作，总有一个是合并的，另—个是非合并的。利用共享内存可以改善全局内存的访问模式，使得对全局内存的读和写都是合并的。\n全局内存的访问速度是所有内存中最低的，应该尽量减少对它的使用。所有设备内存中，寄存器是最高效的，但在需要线程合作的问题中，用仅对单个线程可见的寄存器是不够的, 需要使用对整个线程块可见的共享内存：\n在核函数中，要将一个变量定义为共享内存变量，就要在定义语句中加上一个限定符 __shared__。一般情况下，共享内存的数组长度等于线程块大小 线程块的处理逻辑完成后，在利用共享内存进行线程块之间的合作（通信）之前，都要进行同步 __syncthreads()，以确保共享内存变量中的数据对线程块内的所有线程来说都准备就绪 因为共享内存变量的生命周期仅仅在核函数内，所以必须在核函数结束之前将共享内存中的某些结果保存到全局内存 动态共享内存和静态共享内存：\n静态的限定符 __shared__， 需要指定内存大小，如 __shared__ real s_y[128] 动态的限定符 __extern__, 定义变量时不用指定内存大小，如 extern __shared__ real s_y[]， 但是需要在调用 kernel 时，加入共享内存的参数，即共享内存的数组长度等于线程块大小，如 kernel\u0026lt;\u0026lt;\u0026lt;grid_size, block_size, sizeof(real) * block_size\u0026gt;\u0026gt;\u0026gt;(), 这个效果与静态共享内存是一样的，但可以防止使用静态共享内存时指定内存长度时出错 使用动态共享内存的核函数和使用静态共享内存的核函数在执行时间上几乎没有差别。但使用动态共享内存容易可提高程序的可维护性 上面的矩阵转置例子使用共享内存的思路是用一个 block 处理 BLOCK_SIZE * BLOCK_SIZE 的矩阵块\nconst int block_size = 32; __global__ void matrix_trans(int* in, int* out){ __shared__ int buf[block_size]; int i = blockIdx.x * blockDim.x + threadIdx.x; int j = blockIdx.y * blockDim.y + threadIdx.y; int n = in.size(); if(i\u0026lt;n \u0026amp;\u0026amp; j\u0026lt;n){ buf[threadIdx.y][threadIdx.x] = in[i*n+j]; } __syncthreads(); i = blockIdx.y * blockDim.y + threadIdx.x; j = blockIdx.x * blockDim.x + threadIdx.y; if(i\u0026lt;n\u0026amp;\u0026amp;j\u0026lt;n\u0026gt;){ out[j*n+i] = buf[threadIdx.x][threadIdx.y]; } } CUDA Kernel - Array Reduce # 一个有 N（\\(10^8\\)） 个元素的数组 x，假如我们需要计算该数组中所有元素的和，即 sum = x[0] + x[1] + \u0026hellip; + x[N - 1]。\n先调用 kernel 将数组 x 归约到 grid_size 大小，即每个线程块完成 block_size 大小的归约，结果写到数组 y (y 的长度为 grid_size) 然后在 host 上 完成最后一步的归约，即 y[0\u0026hellip;grid_size-1] -\u0026gt; result Kernel如下：\n// 全局内存 void __global__ reduce_global(real *d_x, real *d_y) { const int tid = threadIdx.x; real *x = d_x + blockDim.x * blockIdx.x; for (int offset = blockDim.x \u0026gt;\u0026gt; 1; offset \u0026gt; 0; offset \u0026gt;\u0026gt;= 1) { if (tid \u0026lt; offset) { x[tid] += x[tid + offset]; } __syncthreads(); } if (tid == 0) { d_y[blockIdx.x] = x[0]; } } // 静态共享内存 void __global__ reduce_shared(real *d_x, real *d_y){ const int tid = threadIdx.x; const int bid = blockIdx.x; const int n = bid * blockDim.x + tid; __shared__ real s_y[128]; s_y[tid] = (n\u0026lt;N) ? d_x[n] : 0.0; __syncthreads(); for(int offset=blockDim.x \u0026gt;\u0026gt; 1; offset \u0026gt;0; offset \u0026gt;\u0026gt;= 1){ if(tid\u0026lt;offset\u0026gt;){ s_y[tid]=s_y[tid+offset]; } __syncthreads(); } if (tid == 0) { d_y[blockIdx.x] = x[0]; } } //动态共享内存 void __global__ reduce_dynamic(real *d_x, real *d_y) { const int tid = threadIdx.x; const int bid = blockIdx.x; const int n = bid * blockDim.x + tid; extern __shared__ real s_y[]; s_y[tid] = (n \u0026lt; N) ? d_x[n] : 0.0; __syncthreads(); for (int offset = blockDim.x \u0026gt;\u0026gt; 1; offset \u0026gt; 0; offset \u0026gt;\u0026gt;= 1) { if (tid \u0026lt; offset) { s_y[tid] += s_y[tid + offset]; } __syncthreads(); } if (tid == 0) { d_y[bid] = s_y[0]; } } Host上完成最后一步：\nreal result = 0.0; for (int n = 0; n \u0026lt; grid_size; ++n) { result += h_y[n]; } Telsa T4上的性能对比，详细参数见 code：\nUsing global memory only: Time sum = 123633392.000000. Using static shared memory: Time sum = 123633392.000000. Using dynamic shared memory: Time sum = 123633392.000000. Bank Conflict in Shared Memory # 有一个内存 bank 的概念值得注意。为了获得高的内存带宽，共享内存在物理上被分为 32 个（刚好等于一个线程束中的线程数目，即内建变量 warpSize 的值）同样宽度的、能被同时访问的内存 bank。\n将 32 个 bank 从 0 到 31 编号。在每一个 bank 中，又可以对其中的内存地址从 0 开始编号。所有 bank 中编号为 0 的内存称为第一层内存；将所有 bank 中编号为 1 的内存称为第二层内存。\n对于 bank 宽度为 4 字节的架构(开普勒)，共享内存数组是按如下方式线性地映射到内存 bank 的：共享内存数组中连续的 128 字节的内容分摊到 32 个 bank 的某一层中，每个 bank 负责 4 字节的内容。例如：对一个长度为 128 的单精度浮点数变量的共享内存数组来说，每个 bank 分摊 4 个在地址上相差 128 字节的数据：\n第 0-31 个数组元素依次对应到 32 个 bank 的第一层 第 32-63 个数组元素依次对应到 32 个 bank 的第二层 第 64-95 个数组元素依次对应到 32 个 bank 的第三层 第 96-127 个数组元素依次对应到 32 个 bank 的第四层 只要同一线程束内的多个线程不同时访问同一个 bank 中不同层的数据，该线程束对共享内存的访问就只需要一次内存事务（memory transaction）。当同一线程束内的多个线程试图访问同一个 bank 中不同层的数据时，就会发生 bank 冲突。在一个线程束内对同一个 bank 中的 n 层数据同时访问将导致 n 次内存事务，称为发生了 n 路 bank 冲突。最坏的情况是线程束内的 32 个线程同时访问同一个 bank 中 32 个不同层的地址，这将导致 32 路 bank 冲突。这种 n 很大的 bank 冲突是要尽量避免的。\n如何消除冲突？ 可以用改变共享内存数组大小的方式来消除或减轻共享内存的 bank 冲突。详细方法见下面的矩阵转置例子transpose2。\nCUDA Kernel - Matrix Transpose(Shared Memory) # 核心思想：\n用一个线程块处理一块(tile)的矩阵，比如设置 tile 的边长 TILE_DIM = 32 将一个 tile 的矩阵从全局内存数组 A 中读入线程块的共享内存（二维数组 S[TILE_DIM][TILE_DIM], 线程也是按照二维的方式去执行） 线程块迭代归约整个 tile（） Kernel:\n// N 为矩阵的边长 __global__ void transpose1(const real *A, real *B, const int N) { __shared__ real S[TILE_DIM][TILE_DIM]; int bx = blockIdx.x * TILE_DIM; int by = blockIdx.y * TILE_DIM; int nx1 = bx + threadIdx.x; int ny1 = by + threadIdx.y; if (nx1 \u0026lt; N \u0026amp;\u0026amp; ny1 \u0026lt; N) { S[threadIdx.y][threadIdx.x] = A[ny1 * N + nx1]; } __syncthreads(); int nx2 = bx + threadIdx.y; int ny2 = by + threadIdx.x; if (nx2 \u0026lt; N \u0026amp;\u0026amp; ny2 \u0026lt; N) { B[nx2 * N + ny2] = S[threadIdx.x][threadIdx.y]; } } // 消除bank conflict __global__ void transpose2(const real *A, real *B, const int N) { __shared__ real S[TILE_DIM][TILE_DIM + 1]; int bx = blockIdx.x * TILE_DIM; int by = blockIdx.y * TILE_DIM; int nx1 = bx + threadIdx.x; int ny1 = by + threadIdx.y; if (nx1 \u0026lt; N \u0026amp;\u0026amp; ny1 \u0026lt; N) { S[threadIdx.y][threadIdx.x] = A[ny1 * N + nx1]; } __syncthreads(); int nx2 = bx + threadIdx.y; int ny2 = by + threadIdx.x; if (nx2 \u0026lt; N \u0026amp;\u0026amp; ny2 \u0026lt; N) { B[nx2 * N + ny2] = S[threadIdx.x][threadIdx.y]; } } const int N = 1024; const int TILE_DIM = 32;// 线程块 要处理的二维数据的维度为（TILE_DIM，TILE_DIM） const dim3 block_size(TILE_DIM,TILE_DIM); const int grid_size_x = (N + TILE_DIM-1)/N; const dim3 grid_size(grid_size_x,grid_size_x); const int M = sizeof(real) * (N*N); real *d_A = (real *) malloc(M);//输入数组X real *d_B = (real *) malloc(M);//结果数组Y，需要做最后一轮归约 // init A for (int i = 0; i \u0026lt; N*N; ++i) { h_A[i] = i; } cudaMallocManaged(\u0026amp;d_A, M); cudaMallocManaged(\u0026amp;d_B, M); transpose1\u0026lt;\u0026lt;\u0026lt;grid_size, block_size\u0026gt;\u0026gt;\u0026gt;(d_A, d_B, N); cudaFree(d_A); cudaFree(d_B); ","date":"2024-05-23","externalUrl":null,"permalink":"/posts/cuda/cuda_memory/","section":"Blogs","summary":"Global Memory # 全局内存的访问模式，有合并（coa","title":"CUDA Memory and Optimization","type":"posts"},{"content":"","date":"2024-05-21","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"2024-05-21","externalUrl":null,"permalink":"/tags/pytorch/","section":"Tags","summary":"","title":"Pytorch","type":"tags"},{"content":"","date":"2024-05-21","externalUrl":null,"permalink":"/series/pytorch/","section":"Series","summary":"","title":"PyTorch","type":"series"},{"content":"","date":"2024-05-21","externalUrl":null,"permalink":"/posts/pytorch/","section":"Blogs","summary":"","title":"PyTorch","type":"posts"},{"content":" PyTorch 代码结构 # PyTroch 主要由C10、ATen、torch三大部分组成：\ntorch/ 下包含 import 和使用的 Python 模块 torch/csrc/ 包含了 PyTorch 前端的 C++ 代码及C++前端代码。具体而言，它包含了 Python 和 C++ 之间转换的binding代码， autograd 引擎和 JIT 编译器等。 c10(Caffe Tensor Library), 包含 PyTorch 的核心抽象，存放最基础的Tensor库代码，包括 Tensor 和 Storage 数据结构的实际实现，可以运行在服务端和移动端。 最具代表性的class是 TensorImpl ，实现了Tensor的最基础框架。继承者和使用者有： Variable的Variable::Impl SparseTensorImpl detail::make_tensor(storage_impl, CUDATensorId(), false) Tensor(c10::intrusive_ptr\u0026lt;TensorImpl, UndefinedTensorImpl\u0026gt; tensor_impl) c10::make_intrusive\u0026lt;at::TensorImpl, at::UndefinedTensorImpl\u0026gt; ATen(A Tensor library for C++11)，包含声明和定义 Tensor 运算相关逻辑的代码，是实现张量运算的 C++ 库，kernel代码大多在这里 包含 C++ 实现的native算子和 C 实现的legacy算子(TH, THC, THNN, THCUNN) . aten/src/ATen/gen.py 用来动态生成一些ATen相关的代码 PyTroch 的编译过程 # 入口 setup.py； 提前检查依赖项； 使用 cmake 生成 Makefile Make: 产生中间源文件 Make: 编译三方库 Make: 生成静态库、动态库、可执行文件 Make: Copy文件到合适路径 setuptools, build_py setuptools, build_ext setuptools, install_lib PyTorch 工作流和计算图 # PyTorch 1.0 整体工作流：\n使用 imperative / eager 的范式，每一行代码都构建一个图作为完整计算图的一部分。即使完整的计算图还没有完成构建，也可以独立执行这些作为组件的小计算图，这种动态计算图被称为define-by-run Eager 模式适合块做原型、实验、debug，Script 模式(torch.jit)适合做优化与部署 动态图 # 假设PyTorch的autograd系统是一个 graph，那么每个 Function 实例就是 graph 中的节点，各个 Function 实例之间通过 Edge 连接。Edge 是个 struct，(Function, input_nr) 组合可以代表一个 edge\nstruct Edge { ... std::shared_ptr\u0026lt;Function\u0026gt; function; uint32_t input_nr; }; Function 的成员变量 next_edges_ 就是一组 Edge 实例，代表当前Function实例的返回值要输出到哪个Function\nFunction 的 input, ouput 都是 Variable实例，因此，当一个 graph 被执行时，Variable 实例就在这些 edge 之间来流动，传输信息\nFunction 的成员变量 sequence number，随着Function实例的不断构建而单调增长\nJIT # Code/AST -\u0026gt; Parsing-\u0026gt; Checking -\u0026gt; Optimization -\u0026gt; Translation -\u0026gt; Execution\nJIT 主要会输入代码或 Python 的抽象句法树（AST），其中 AST 会用树结构表征 Python 源代码的句法结构。 Parsing可能是解析句法结构和计算图，然后语法检测接连着代码优化过程，最后只要编译并执行就可以 优化计算图，如展开循环、指令转换等 执行，与 Python 解释器可以执行代码一样，PyTorch JIT 过程中也有一个解释器执行中间表征指令 PyTorch 从 Python 代码到 kernel # PyTorch 从 Python 代码到 kernel 的中间过程十分复杂, 在进入内核之前，所有代码都是自动生成的\n假设调用 torch.add()，流程如下：\nPython 域转换到 C++ 域（Python 参数解析） 处理 VariableType dispatch 处理 DeviceType/布局 dispatch 执行kernel(native kernel 或 TH kernel) ATen 动态生成的代码 # Type继承体系，包含头文件和源文件 Type继承体系是联系 Tensor op 与 legacy 的 TH 或 native kernel 的纽带 Type继承体系维护了2/3级分发机制 Declarations.yaml，会被Torch模块动态生成代码调用 生成 Tensor 类 生成Type家族注册初始化的代码 生成 legacy 的 TH/THC 的kernel声明 生成 native kernel 的声明 PyTroch Tensor # #在python中定义了Parameter类 class Parameter(torch.Tensor) #在python中定义了torch.Tensor类 class Tensor(torch._C._TensorBase) #在C++中定义了Variable类 struct TORCH_API Variable : public at::Tensor //PyObject* Py_InitModule(char *name, PyMethodDef *methods) //创建torch._C Py_InitModule(\u0026#34;torch._C\u0026#34;, methods.data()） //创建 torch._C._TensorBase PyModule_AddObject(module, \u0026#34;_TensorBase\u0026#34;, (PyObject *)\u0026amp;THPVariableType); Tensor运算 Dispatch 机制中的 Type 继承体系 # Type类派生出了TypeExtendedInterface，TypeExtendedInterface又派生了TypeDefault。TypeDefault又派生了CUDATypeDefault、CPUTypeDefault、VariableType（实现了autograd）、UndefinedType等。其中，根据 density 和 scaler type 的不同：\nCUDATypeDefault派生了：\nCUDAIntType CUDAShortType SparseCUDACharType CUDADoubleType CUDAByteType CUDACharType SparseCUDAByteType CUDAFloatType SparseCUDALongType CUDALongType CUDAHalfType SparseCUDAShortType SparseCUDADoubleType SparseCUDAIntType SparseCUDAFloatType CPUTypeDefault派生了：\nSparseCPUShortType CPUFloatType CPUHalfType CPUDoubleType CPUByteType SparseCPUFloatType SparseCPUIntType SparseCPUDoubleType CPUCharType SparseCPUByteType CPUIntType CPULongType SparseCPULongType SparseCPUCharType CPUShortType Type继承体系的作用\nPyTorch Kernel 组成 # Error checking, TORCH CHECK Output allocation Dtype dispatch Parallelization Data access 未完待续\u0026hellip;\n","date":"2024-05-21","externalUrl":null,"permalink":"/posts/pytorch/pytorch/","section":"Blogs","summary":"PyTorch 代码结构 # PyTroch 主要由C10、ATen、t","title":"PyTorch Architecture","type":"posts"},{"content":"","date":"2024-05-20","externalUrl":null,"permalink":"/tags/paged-attention/","section":"Tags","summary":"","title":"Paged Attention","type":"tags"},{"content":" vLLM # vLLM是吞吐性能卓越的大模型推理框架，PagedAttention是vLLM最大的创新点： Efficient Memory Management for Large Language Model Serving with PagedAttention\nvLLM中的attention计算，在推理的prefill阶段, 使用第三方库xformers的优化实现，decoding阶段使用 CUDA kernel 实现(csrc/attention/attention_kernels.cu，大约800多行)。\nAttention计算时使用页式管理 KV Cache 来提高内存利用率，进而提高吞吐量。\nPaged Attention(PA) # vLLM中有两个版本的 PA，其中：\nV1 源于 FasterTransformers 的 MHA，适用于 len(seq) \u0026lt; 8192 或 num_seqs * num_heads \u0026gt; 512 的情况。 V2 参考了 Flash Decoding方式，对 sequence 的维度进行切分来增加并行粒度 Paged Attention V1 # Block table in PA\n一个 req 中包含多个 seq 时，可以共享blocks\nPaged Attention V1 CUDA Kernel(vLLM) # csrc/attention/attention_kernels.cu\nsingle_query attention 函数\nDispatch逻辑：\nCALL_KERNEL_LAUNCHER_BLOCK_SIZE 根据存储的kv blocksize进行派发，分别是 8， 16， 32 LAUNCH_ATTENTION_KERNEL 根据注意力头大小HEADSIZE静态派发 并行任务的划分：\ndim3 grid(num_heads, num_seqs， 1) dim3 block(NUM_THREADS), 线程数是128，每个 block 负责完成 output 矩阵一行（head_size个元素）结果的 attention 计算 block 的线程划分为若干个 Warp, 每个 Warp 的32个线程划分为 blk_size 个 thread group Kernel 输入参数\nout[num_seqs, num_heads, head_size] q[num_seqs, num_heads, head_size] k_cache[num_blocks, num_kv_heads, head_size/x, block_size, x] # x表示一个向量化的大小，如float16 -\u0026gt; 16 / sizeof(float16) = 8 v_cache[num_blocks, num_kv_heads, head_size, block_size] head_mapping[num_heads] # 使用MQA, GQA时的kv_head block_tables[num_seqs, max_num_blocks_per_seq] # 维护各个Q对应KVCache的哪些block context_lens[num_seqs] # 用于变长 num_head： Q 的 head 数 num_kv_heads：K, V 的 head 数，MHA 的 num_kv_heads = num_head，GQA、MQA 的 num_kv_heads \u0026lt; num_head blk_size # block_size，每个page block存储的元素数量，每个page存(blk_size, num_head，head_size)个K、V的元素\nKernel 的常量定义：\nTHREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1) 通过WARPSIZE / BLOCKSIZE 得到一个thread_group大小。注意这里的BLOCKSIZE不是cuda blocksize，而是一个kv block的大小(默认值16) NUM_TOKENS_PER_THREAD_GROUP = (BLOCK_SIZE + WARP_SIZE - 1) / - WARP_SIZE 表示每个thread_group处理多少个token NUM_WARPS 表示一个threadblock有多少个warp VEC_SIZE 表示向量化大小，保证每个thread_group一次性获取16bytes，MAX(16 / (THREAD_GROUP_SIZE * sizeof(scalar_t)), 1) NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE 表示每个thread要负责多少个数据计算 NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE 表示每个thread负责的数据经过向量化后，一共有多少个vec V_VEC_SIZE = MIN(16 / sizeof(scalar_t), BLOCK_SIZE) 每个thread一次性读取16bytes NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE。对于v_cache[head_size, block_size]，表示一行需要几个V_VEC NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW 表示一个warp可以处理多少行 NUM_ROWS_PER_THREAD 表示每个thread需要负责多少行 Kernel 代码逻辑：\n（1）循环从显存读取\\(Q\\)到 shared memory：\n迭代读取，每 CUDA block 负责读取\\(Q\\)的一行（head_size 个元素）存入 shared memory。其中，block 的每个 Warp 负责读取 16blk_size 字节的 Q，即每个 thread group 会读取16字节的 Q，16blk_size 字节的 Q 对应 sequence 的一个 head。\nconst int thread_group_idx = thread_idx / THREAD_GROUP_SIZE; const int thread_group_offset = thread_idx % THREAD_GROUP_SIZE; // Load the query to registers. // Each thread in a thread group has a different part of the query. // For example, if the the thread group size is 4, then the first thread in // the group has 0, 4, 8, ... th vectors of the query, and the second thread // has 1, 5, 9, ... th vectors of the query, and so on. NOTE(woosuk): Because // q is split from a qkv tensor, it may not be contiguous. const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE; __shared__ Q_vec q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD]; #pragma unroll for (int i = thread_group_idx; i \u0026lt; NUM_VECS_PER_THREAD; i += NUM_THREAD_GROUPS) { const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE; q_vecs[thread_group_offset][i] = *reinterpret_cast\u0026lt;const Q_vec*\u0026gt;(q_ptr + vec_idx * VEC_SIZE); } __syncthreads(); （2）循环从显存读取\\(K\\)到 register，并计算QK：\n每个 seq 包含 cxt_length * num_kv_heads * head_size 个元素 每个 CUDA block 负责计算一个 seq 的一个 head 的 \\(QK^T\\)， 只需要读取 ctx_length * head_size 个 K 的元素 因为页式内存管理，K 在 ctx_length 维度的存储不连续，以 blk_size 个 token 为粒度分布在不同的内存地址，所以需要根据 Q 的 head_idx 和 seq_idx 访问 block_table 找到 K 的 physical_block_num K Cache的布局为 [num_blocks, num_kv_heads, head_size/x, block_size, x]， 目的是优化写入 shared memory。Q和K的同一行元素被读入寄存器并进行点乘运算后，结果要写入shared memory。如果一个 Warp 中所有线程都计算 Q、K 同一行数据，会导致写入 shared memory 的同一个位置，这将造成 warp 内不同线程顺序地写入。所以 warp 的线程最好计算 Q和K 的不同行数据。在设计 K 布局时，将 block_size 放在比 head_size 更低的维度。由于warp size大于block_size，我们需要将head_size拆分为head_size/x和x两个维度，借x到最低维度，以确保每个线程读入的数据量和计算量都足够大。最后，每个线程组派一个线程去写入shared memory，这样一个warp有blk_size个线程并行写入shared memory，从而增加了shared memory的访问带宽。这种设计策略是为了实现高效的并行计算和内存访问，以提高整体的计算性能。 读取 K 需要一个循环，循环中每个CUDA block中的所有 warp 依次访问num_blocks 个 page block。每次迭代： 每个 warp 负责访问连续的 blk_size 个 KCache 的行数据（blk_size * head_size个元素）。每个 thread group 负责访问 KCache 的一行，将head_size 个元素读入寄存器 寄存器中的Q和K元素进行点乘，结果写入shared memory。一个 CUDA block 的 shared memory 存储了一行 QK^T 的结果，共 ctx_length 个元素 CUDA block 对 shared memory 中元素进行 max，sum 方式 reduction，然后计算得到 softmax 的结果 代码步骤：\ngroup是由block大小决定的，当block\u0026gt;32时，每个warp实现了一个group,否则在一个warp中实现多个group\n每个warp负责计算一个block KCache，而每个block key shape为 [block_size, num_head, head_size]\n每个thread_group取一个key，即num_head个元素，计算QK dot\n只有thread_group的第一个thread负责将QK结果写入shared memory\nhead_idx标记GPU BLOCKs，也即每个GPU Blocks计算一个head\nnum_heads标记使用的GPU BLOCKs总数，也即head num\nseq_idx标记的是第二维GPU BLOCKs， 也即seq的位置\n分配red_smem[2*NUM_WARPS]为reduce所用，保留的是warp内的局部最大值。后面计算了qvec的dot结果保存为qk，先在group内reduce计算得到局部最大值，然后在每个warp内reduce计算得到全局最大值为qk_max。\n// 每个warp负责 blocksize * headsize个元素 // block_idx是block cache中的序号（逻辑序号） for (int block_idx = warp_idx; block_idx \u0026lt; num_blocks; block_idx += NUM_WARPS) { // TODO(Zhengzekang) // 定位物理块 const int physical_block_number = block_table[block_idx]; // ... K_vec k_vecs[NUM_VECS_PER_THREAD]; // 遍历每个thread_group处理多少个token for (int i = 0; i \u0026lt; NUM_TOKENS_PER_THREAD_GROUP; i++) { const int physical_block_offset = (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE; const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset; K_vec k_vecs[NUM_VECS_PER_THREAD]; // 遍历每个thread需要处理多少个VEC for (int j = 0; j \u0026lt; NUM_VECS_PER_THREAD; j++) { // vectorized取到key k_vecs[j] = xxxx; } // 计算QKdot，里面包含了一个thread_groupsize的WarpReduceSum， float qk = scale * Qk_dot\u0026lt;scalar_t, THREAD_GROUP_SIZE\u0026gt;::dot(q_vecs, k_vecs); // 只有thread_group的第一个thread负责将QK结果写入shared memory // 并且维护一个qk_max，用于后续softmax if (thread_group_offset == 0) { // Store the partial reductions to shared memory. // NOTE(woosuk): It is required to zero out the masked logits. const bool mask = token_idx \u0026gt;= context_len; logits[token_idx] = mask ? 0.f : qk; // Update the max value. qk_max = mask ? qk_max : fmaxf(qk_max, qk); } } } 此时各个thread_group已经完成了自己的qk_dot操作，并且都维护了qk_max。下面就需要和其他thread_group做warp shuffle操作，得到一个warp内的qk max值。\n由于每个thread_group里的thread内维护的qk_max是一样的，所以warp shuffle只需到 thread_group_size即可停止。并由lane_id = 0的线程将warp里的qk_max存储到smem，最后再做一次warpreduce，得到一个block里的qkmax值，通过shfl_sync广播操作，让每个线程都拿到max\n#pragma unroll for (int mask = WARP_SIZE / 2; mask \u0026gt;= THREAD_GROUP_SIZE; mask /= 2) { qk_max = fmaxf(qk_max, __shfl_xor_sync(uint32_t(-1), qk_max, mask)); } if (lane == 0) { red_smem[warp_idx] = qk_max; } __syncthreads(); // TODO(woosuk): Refactor this part. // Get the max qk value for the sequence. qk_max = lane \u0026lt; NUM_WARPS ? red_smem[lane] : -FLT_MAX; #pragma unroll for (int mask = NUM_WARPS / 2; mask \u0026gt;= 1; mask /= 2) { qk_max = fmaxf(qk_max, __shfl_xor_sync(uint32_t(-1), qk_max, mask)); } // Broadcast the max qk value to all threads. qk_max = __shfl_sync(uint32_t(-1), qk_max, 0); 接下来就是常规的softmax\n执行exp(x-qk_max)并得到每个warp上的exp_sum，规约得全局（所有warp）的exp_sum,计算每个节点上的softmax\n// Get the sum of the exp values. float exp_sum = 0.f; for (int i = thread_idx; i \u0026lt; context_len; i += NUM_THREADS) { float val = __expf(logits[i] - qk_max); logits[i] = val; exp_sum += val; } exp_sum = block_sum\u0026lt;NUM_WARPS\u0026gt;(\u0026amp;red_smem[NUM_WARPS], exp_sum); // Compute softmax. const float inv_sum = __fdividef(1.f, exp_sum + 1e-6f); for (int i = thread_idx; i \u0026lt; context_len; i += NUM_THREADS) { logits[i] *= inv_sum; } __syncthreads(); （3）从显存读取\\(V\\)到 register, 计算 softmax(QK^T)V\n和KCache一样，CUDA block 依次访问 num_blk 个 VCahce block 到寄存器，每个 warp 负责 1 个 VCache block，。不过这里不需要以 thread group 为单位访问16字节，而是每个 thread 读取16字节的元素到寄存器，然后与shared memory的 softmax(QK^T)中间结果 对应位置16字节的数据进行点乘，得到一个 float 结果，写到 output 的对应位置中。\n为了读写连续，将V_cache转置，shape为：[num_blocks, num_kv_heads, head_size, block_size]\n注意这里使用了fp32模式以防止累加过程中的精度损失\n// 每个线程一次性读16bytes数据 constexpr int V_VEC_SIZE = MIN(16 / sizeof(scalar_t), BLOCK_SIZE); using V_vec = typename Vec\u0026lt;scalar_t, V_VEC_SIZE\u0026gt;::Type; using L_vec = typename Vec\u0026lt;scalar_t, V_VEC_SIZE\u0026gt;::Type; using Float_L_vec = typename FloatVec\u0026lt;L_vec\u0026gt;::Type; // 每一行有多少个V_VEC，假设BLOCK_SIZE=8，那么NUM_V_VECS_PER_ROW=1 constexpr int NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE; // 一个WARP一次处理多少行，按照上面假设，这里是32 constexpr int NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW; // 每个thread需要负责多少行，假设headsize=128，那么每个thread要处理4行 constexpr int NUM_ROWS_PER_THREAD = (HEAD_SIZE + NUM_ROWS_PER_ITER - 1) / NUM_ROWS_PER_ITER; // 提前分配accumulate buffer，用float累加 float accs[NUM_ROWS_PER_THREAD]; #pragma unroll for (int i = 0; i \u0026lt; NUM_ROWS_PER_THREAD; i++) { accs[i] = 0.f; } for (int block_idx = warp_idx; block_idx \u0026lt; num_blocks; block_idx += NUM_WARPS) { // ... #pragma unroll for (int i = 0; i \u0026lt; NUM_ROWS_PER_THREAD; i++) { const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER; if (row_idx \u0026lt; HEAD_SIZE) { const int offset = row_idx * BLOCK_SIZE + physical_block_offset; V_vec v_vec = *reinterpret_cast\u0026lt;const V_vec*\u0026gt;(v_ptr + offset); accs[i] += dot(logits_vec, v_vec); } } } （4）更新最终的结果\n将一个block分成上半部分warp和下半部分warp。上半部分warp(warp_id \u0026gt; mid)将自己累加的结果写到shared memory。下半部分warp将之前上半部分warp存到shared_memory 的结果取出，进行累加。这样重复，当warp_idx==0时，将所有结果写回到每一行中。\n// Perform reduction across warps. float* out_smem = reinterpret_cast\u0026lt;float*\u0026gt;(shared_mem); #pragma unroll for (int i = NUM_WARPS; i \u0026gt; 1; i /= 2) { int mid = i / 2; // Upper warps write to shared memory. if (warp_idx \u0026gt;= mid \u0026amp;\u0026amp; warp_idx \u0026lt; i) { float* dst = \u0026amp;out_smem[(warp_idx - mid) * HEAD_SIZE]; #pragma unroll for (int i = 0; i \u0026lt; NUM_ROWS_PER_THREAD; i++) { const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER; if (row_idx \u0026lt; HEAD_SIZE \u0026amp;\u0026amp; lane % NUM_V_VECS_PER_ROW == 0) { dst[row_idx] = accs[i]; } } } } __syncthreads(); // Lower warps update the output. if (warp_idx \u0026lt; mid) { const float* src = \u0026amp;out_smem[warp_idx * HEAD_SIZE]; #pragma unroll for (int i = 0; i \u0026lt; NUM_ROWS_PER_THREAD; i++) { const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER; if (row_idx \u0026lt; HEAD_SIZE \u0026amp;\u0026amp; lane % NUM_V_VECS_PER_ROW == 0) { accs[i] += src[row_idx]; } } } __syncthreads(); // Write the final output. if (warp_idx == 0) { scalar_t* out_ptr = out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE + head_idx * max_num_partitions * HEAD_SIZE + partition_idx * HEAD_SIZE; #pragma unroll for (int i = 0; i \u0026lt; NUM_ROWS_PER_THREAD; i++) { const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER; if (row_idx \u0026lt; HEAD_SIZE \u0026amp;\u0026amp; lane % NUM_V_VECS_PER_ROW == 0) { from_float(*(out_ptr + row_idx), accs[i]); } } } 为什么 VCache 的 layout 是 [num_blocks, num_kv_heads, head_size, block_size]，和 KCache layout 不一样？ 因为 V 要去做点乘的对象在shared memory，只需要读，不涉及并行写。\nPA V1 和 Flash Attention 的区别 # 并行任务的划分方式不同\nFlashAttention 用了两层循环，每次写一个 Tile 的 output tensor，而 PA 只有一层循环，每次写一行 output tensor。因为每次迭代都有整行的 QK^T 中间结果，不需要online softmax PA V1 设计的 KCache layout 充分利用了 shared memory 写带宽 PA V1 的缺陷 # 不足：\n不适合 seq 很长的情况，因为没有沿着 ctx_length 或者 batch 维度做切分 和MHA相比，MQA和GAQ没有减少对KV Cache的读写次数。读K、V Cache时候只是做了一个head_idx的转换，会重复从显存读相同的head 未完待续\u0026hellip;\nReference:\nvllm Efficient Memory Management for Large Language Model Serving with PagedAttention PageAttention代码走读 vLLM kernel ","date":"2024-05-20","externalUrl":null,"permalink":"/posts/llm/paged_attention_v1/","section":"Blogs","summary":"vLLM # vLLM是吞吐性能卓越的大模型推理框","title":"Paged Attention V1(vLLM)","type":"posts"},{"content":"","date":"2024-05-20","externalUrl":null,"permalink":"/tags/vllm/","section":"Tags","summary":"","title":"VLLM","type":"tags"},{"content":"","date":"2024-05-20","externalUrl":null,"permalink":"/series/vllm/","section":"Series","summary":"","title":"VLLM","type":"series"},{"content":"","date":"2024-05-20","externalUrl":null,"permalink":"/tags/conv/","section":"Tags","summary":"","title":"Conv","type":"tags"},{"content":" CUDA Conv # 在 PyTorch 上实现Conv很简单\nimport torch from torch.nn.functional import conv2d device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) import time width = 1000 height = 1000 img =torch.randn([width,height]) img = img.to(device) kernel = torch.tensor([[-1.0, 0.0, 1.0], [-1.0, 0.0, 1.0], [-1.0, 0.0, 1.0]]) img = torch.reshape(img, (1, 1, width, height)) kernel = torch.reshape(kernel, (1, 1, 3, 3)) kernel = kernel.to(device) start = time.perf_counter() output = F.conv2d(img, kernel, stride=1).to(device) end = time.perf_counter() print(f\u0026#39;total_cost: {end-start} ms\u0026#39;) print(f\u0026#39;output_size: {output.shape}\u0026#39;) print(f\u0026#39;output_tensor: {output}\u0026#39;) 用CUDA实现Conv，Steps:\nCheck Get thread info Memory allocation Copy data to device Call conv kernel Copy data to host Free memory //file: conv.cu #include\u0026lt;stdint.h\u0026gt; #include\u0026lt;cuda.h\u0026gt; #include \u0026#34;cuda_runtime.h\u0026#34; #include \u0026#34;device_launch_parameters.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;math.h\u0026gt; const int NUM_REPEATS = 10; #define CHECK(call) \\ do \\ { \\ const cudaError_t error_code = call; \\ if (error_code != cudaSuccess) \\ { \\ printf(\u0026#34;CUDA Error:\\n\u0026#34;); \\ printf(\u0026#34; File: %s\\n\u0026#34;, __FILE__); \\ printf(\u0026#34; Line: %d\\n\u0026#34;, __LINE__); \\ printf(\u0026#34; Error code: %d\\n\u0026#34;, error_code); \\ printf(\u0026#34; Error text: %s\\n\u0026#34;, \\ cudaGetErrorString(error_code)); \\ exit(1); \\ } \\ } while (0) static void HandleError(cudaError_t err, const char* file, int line) { if (err != cudaSuccess) { printf(\u0026#34;%s in %s at line %d\\n\u0026#34;, cudaGetErrorString(err), file, line); exit(EXIT_FAILURE); } } #define HANDLE_ERROR(err) (HandleError(err, __FILE__, __LINE__)) int getThreadNum() { cudaDeviceProp prop; int count; CHECK(cudaGetDeviceCount(\u0026amp;count)); printf(\u0026#34;GPU num: %d\\n\u0026#34;, count); CHECK(cudaGetDeviceProperties(\u0026amp;prop, 0)); printf(\u0026#34;Max thread num per block: %d\\n\u0026#34;, prop.maxThreadsPerBlock); printf(\u0026#34;Max grid dim: %d, %d, %d)\\n\u0026#34;, prop.maxGridSize[0], prop.maxGridSize[1], prop.maxGridSize[2]); return prop.maxThreadsPerBlock; } __global__ void conv(float* img, float* kernel, float* result, int width, int height, int kernelSize) { int ti = threadIdx.x; int bi = blockIdx.x; int id = (bi * blockDim.x + ti); if (id \u0026gt;= width * height) { return; } int row = id / width; int col = id % width; for (int i = 0; i \u0026lt; kernelSize; ++i) { for (int j = 0; j \u0026lt; kernelSize; ++j) { float imgValue = 0; int curRow = row - kernelSize / 2 + i; int curCol = col - kernelSize / 2 + j; if (curRow \u0026lt; 0 || curCol \u0026lt; 0 || curRow \u0026gt;= height || curCol \u0026gt;= width) { } else { imgValue = img[curRow * width + curCol]; } result[id] += kernel[i * kernelSize + j] * imgValue; } } } int main() { int width = 1000; int height = 1000; float* img = new float[width * height]; for (int row = 0; row \u0026lt; height; ++row) { for (int col = 0; col \u0026lt; width; ++col) { img[col + row * width] = (col + row) % 256; } } int kernelSize = 3; float* kernel = new float[kernelSize * kernelSize]; for (int i = 0; i \u0026lt; kernelSize * kernelSize; ++i) { kernel[i] = i % kernelSize - 1; } float* imgGpu; float* kernelGpu; float* resultGpu; CHECK(cudaMalloc((void**)\u0026amp;imgGpu, width * height * sizeof(float))); CHECK(cudaMalloc((void**)\u0026amp;kernelGpu, kernelSize * kernelSize * sizeof(float))); CHECK(cudaMalloc((void**)\u0026amp;resultGpu, width * height * sizeof(float))); CHECK(cudaMemcpy(imgGpu, img, width * height * sizeof(float), cudaMemcpyHostToDevice)); CHECK(cudaMemcpy(kernelGpu, kernel, kernelSize * kernelSize * sizeof(float), cudaMemcpyHostToDevice)); int threadNum = getThreadNum(); int blockNum = (width * height - 0.5) / threadNum + 1; float t_sum = 0; float t2_sum = 0; for (int repeat = 0; repeat \u0026lt;= NUM_REPEATS; ++repeat) { cudaEvent_t start, stop; CHECK(cudaEventCreate(\u0026amp;start)); CHECK(cudaEventCreate(\u0026amp;stop)); CHECK(cudaEventRecord(start)); cudaEventQuery(start); conv \u0026lt;\u0026lt; \u0026lt;blockNum, threadNum \u0026gt;\u0026gt; \u0026gt; (imgGpu, kernelGpu, resultGpu, width, height, kernelSize); CHECK(cudaEventRecord(stop)); CHECK(cudaEventSynchronize(stop)); float elapsed_time; CHECK(cudaEventElapsedTime(\u0026amp;elapsed_time, start, stop)); printf(\u0026#34;Time = %g ms.\\n\u0026#34;, elapsed_time); if (repeat \u0026gt; 0) { t_sum += elapsed_time; t2_sum += elapsed_time * elapsed_time; } CHECK(cudaEventDestroy(start)); CHECK(cudaEventDestroy(stop)); } const float t_ave = t_sum / NUM_REPEATS; const float t_err = sqrt(t2_sum / NUM_REPEATS - t_ave * t_ave); printf(\u0026#34;Time = %g +- %g ms.\\n\u0026#34;, t_ave, t_err); float* result = new float[width * height]; CHECK(cudaMemcpy(result, resultGpu, width * height * sizeof(float), cudaMemcpyDeviceToHost)); // visualization printf(\u0026#34;img\\n\u0026#34;); for (int row = 0; row \u0026lt; 10; ++row) { for (int col = 0; col \u0026lt; 10; ++col) { printf(\u0026#34;%2.0f \u0026#34;, img[col + row * width]); } printf(\u0026#34;\\n\u0026#34;); } printf(\u0026#34;kernel\\n\u0026#34;); for (int row = 0; row \u0026lt; kernelSize; ++row) { for (int col = 0; col \u0026lt; kernelSize; ++col) { printf(\u0026#34;%2.0f \u0026#34;, kernel[col + row * kernelSize]); } printf(\u0026#34;\\n\u0026#34;); } printf(\u0026#34;result\\n\u0026#34;); for (int row = 0; row \u0026lt; 10; ++row) { for (int col = 0; col \u0026lt; 10; ++col) { printf(\u0026#34;%2.0f \u0026#34;, result[col + row * width]); } printf(\u0026#34;\\n\u0026#34;); } return 0; } ","date":"2024-05-20","externalUrl":null,"permalink":"/posts/cuda/cuda_conv/","section":"Blogs","summary":"CUDA Conv # 在 PyTorch 上实现Conv很简单 import torch from torch.nn.functional import","title":"CUDA Conv","type":"posts"},{"content":"","date":"2024-05-18","externalUrl":null,"permalink":"/tags/gpu/","section":"Tags","summary":"","title":"GPU","type":"tags"},{"content":" CPU 与 GPU 的不同 # CPU，4个 ALU，主要负责逻辑计算，1个控制单元 Control，1个 DRAM，1个 Cache GPU，绿色小方块看作 ALU，红色框看作一个 SM，SM 中的多个 ALU share 一个Control 和 Cache，SM 可以看作一个多核 CPU，但是 ALU 更多，control 更少，也就是算力提升，控制力减弱 所以，CPU 适合控制逻辑复杂的任务，GPU 适合逻辑简单、数据量大、计算量大的任务。\nGPU, CUDA, AI Framework 的关系 # Reference:\nNVIDIA CUDA Docs cuda编程学习 一张图了解GPU、CUDA、CUDA toolkit和pytorch的关系 GPU 内存概念浅析 GPU 内部结构 # 每一个 SM 有自己的 Wrap scheduler 、寄存器（Register）、指令缓存、L1缓存、共享内存。\nA100 中每个 SM 包括 4 个 SM partition（SMP），里边绿色的就是 Streaming Processor（SP），也叫 CUDA cores，它们是实际执行计算的基本单元。\n所有的 SM 共享 L2 缓存。整个 GPU 内存结构如下图所示\nGPU 内存结构 # 按照存储功能进行细分，GPU 内存可以分为：局部内存（local memory）、全局内存（global memory）、常量内存（constant memory）、共享内存（shared memory）、寄存器（register）、L1/L2 缓存等。\n其中全局内存、局部内存、常量内存都是片下内存(off-chip)，储存在 HBM 上。所以 HBM 的大部分作为全局内存。\non-chip：L1/L2 cache：多级缓存，在 GPU 芯片内部\noff-chip：GPU DRAM/HBM, global memory\nL2 缓存可以被所有 SM 访问，速度比全局内存快。Flash attention 的思路就是尽可能地利用 L2 缓存，减少 HBM 的数据读写时间\nL1 缓存用于存储 SM 内的数据，被 SM 内的 CUDA cores 共享，但是跨 SM 之间的 L1 不能相互访问\n局部内存 (local memory) 是线程独享的内存资源，线程之间不可以相互访问。局部内存属于off-chip，所以访问速度跟全局内存一样。它主要是用来应对寄存器不足时的场景，即在线程申请的变量超过可用的寄存器大小时，nvcc 会自动将一部数据放置到片下内存里。\n寄存器（register）是线程能独立访问的资源，它是片上（on chip）存储，用来存储一些线程的暂存数据。寄存器的速度是访问中最快的，但是它的容量较小，只有几百甚至几十 KB，而且要被许多线程均分\n共享内存（shared memory） 是一种在线程块内能访问的内存，是片上（on chip）存储，访问速度较快。共享内存主要是缓存一些需要反复读写的数据。共享内存与 L1 缓存的位置、速度极其类似，区别在于共享内存的控制与生命周期管理与 L1 不同：共享内存受用户控制，L1 受系统控制。共享内存更利于线程块之间数据交互。\n常量内存（constant memory）是片下（off chip）存储，但是通过特殊的常量内存缓存（constant cache）进行缓存读取，它是只读内存。常量内存主要是解决一个 warp scheduler 内多个线程访问相同数据时速度太慢的问题。假设所有线程都需要访问一个 constant_A 的常量，在存储介质上 constant_A 的数据只保存了一份，而内存的物理读取方式决定了多个线程不能在同一时刻读取到该变量，所以会出现先后访问的问题，这样使得并行计算的线程出现了运算时差。常量内存正是解决这样的问题而设置的，它有对应的 cache 位置产生多个副本，让线程访问时不存在冲突，从而保证并行度。\n内存类型 物理位置 访问权限 可见范围 生命周期 全局内存 在芯片外 可读可写 所有线程和主机端 由主机分配与释放 常量内存 在芯片外 仅可读 所有线程和主机端 由主机分配与释放 纹理和表面内存 在芯片外 一般仅可读 所有线程和主机端 由主机分配与释放 寄存器内存 在芯片内 可读可写 单个线程 所在线程 局部内存 在芯片外 可读可写 单个线程 所在线程 共享内存 在芯片内 可读可写 单个线程块 所在线程块 “全局内存”（global memory）的含义是核函数中的所有线程都能够访问其中的数 据. 一般用 cudaMalloc 函数为全局内存变量分配设备内存，用 cudaMemcpy(h_z, d_z, M, cudaMemcpyDeviceToHost) 或 cudaMemcpyDeviceToDevice 拷贝内存。在处理逻辑上的两维或三维问题时，可以用 cudaMallocPitch 和 cudaMalloc3D 函数分配内存，用 cudaMemcpy2D 和 cudaMemcpy3D 复制数据，释放时依然用 cudaFree 函数。\n静态全局内存变量： 所占内存数量是在编译期间就确定的。而且，这样的静态全局内存变量必须在所有主机与设备函数外部定义，所以是一种“全局的静态全局内存变量”。在核函数中，可直接对静态全局内存变量进行访问，并不需要将它们以参数的形式传给核 函数。静态全局内存变量由以下方式在任何函数外部定义：\n__device__ T x; // 单个变量 __device__ T y[N]; // 固定长度的数组 不可在主机函数中直接访问静态全局内存变量，但可以用 cudaMemcpyToSymbol 函 数和 cudaMemcpyFromSymbol 函数在静态全局内存与主机内存之间传输数据。\n常量内存（constant memory）是有常量缓存的全局内存，数量有限，一共仅有 64 KB。它的可见范围和生命周期与全局内存一样。常量内存仅可读、不可写。访问速度比全局内存高，但得到高访问速度的前提是一个线程束中的线程（一个线程块中相邻的 32 个线程）要读取相同的常量内存数据。常量内存的方法是在核函数外面用 constant 定义变量，用 cudaMemcpyToSymbol 将数据从主机端复制到设备的常量内存后 供核函数使用\n在核函数中定义的不加任何限定符的变量一般来说就存放于寄存器（register）中。核函数中定义的不加任何限定符的数组有可能存放于寄存器中，但也有可能存放于局部内存中。如 gridDim、blockDim、blockIdx、threadIdx 及 warpSize 都保存在特殊的寄存器中。这里的 n 就是一个寄存器变量:\nconst int n = blockDim.x * blockIdx.x + threadIdx.x; 寄存器变量仅仅被一个线程可见。\n局部内存： 寄存器中放不下的变量，以及索引值不能在编译时就确定的数组，都有可能放在局部内存中。\n共享内存和寄存器类似，存在于芯片上，具有仅次于寄存器的读写速度，数量也有限。共享内存对整个线程块可见，主要作用是减少对全局内存的访问。\nSM 的构成 # 一个GPU是由多个SM(Streaming Multiprocessor)构成的。一个SM包含如下资源：\n一定数量的寄存器 一定数量的共享内存 常量内存的缓存 纹理和表面内存的缓存 L1缓存 两个（计算能力6.0）或4个（其他计算能力）线程束调度器（warp scheduler）用于不同线程的上下文之间迅速地切换，以及为准备就绪的线程束发出执行指令 执行核心，包括： 若干整型数运算的核心（INT32） 若干单精度浮点数运算的核心（FP32） 若干双精度浮点数运算的核心（FP64） 若干单精度浮点数超越函数（transcendental functions）的特殊函数单元（special function units, SFUs） 若干混合精度的张量核心（tensor cores，由伏特架构引入，适用于机器学习中的低精度矩阵计算） SM的占有率： 在并行规模足够大（即核函数执行配置中定义的总线程数足够多）的前提下分几种情况来分析SM的理论占有率：\n(1) 寄存器和共享内存使用量很小的情况。此时，SM的占有率完全由执行配置中的线程块大小决定。关于线程块大小，读者也许注意到我们之前一直用128。这是因为，SM中线程的执行是以线程束为单位的，所以最好将线程块大小取为线程束大小（32个线程）的整数倍。例如，假设将线程块大小定义为100，那么一个线程块中将有3个完整的线程束（一共96个线程）和一个不完整的线程束（只有4个线程）。在执行核函数中的指令时，不完整的线程束花的时间和完整的线程束花费的时间一样，这就无形中浪费了计算资源。所以，建议将线程块大小取为32的整数倍。在该前提下，任何不小于$N_t/N_b$而且能整除$N_t$的线程块大小都能得到100%的占有率；线程块大小不小于64时其他架构能获得100%的占有率。根据我们列出的数据，线程块大小不小于128时开普勒架构能获得100%的占有率；线程块大小不小于64时其他架构能获得100%的占有率。作者近几年都用一块开普勒架构的Tesla K40开发程序，所以习惯了在一般情况下用128的线程块大小。\n(2) 有限寄存器数量对占有率的约束情况。我们只针对第三节中列出的几个计算能力进行分析，读者可以类似地分析其他未列出的计算能力。对于第三节中列出的所有计算能力，一个SM最多能使用的寄存器个数为64K（64 x 1024）。除图灵架构外，如果我们希望在一个SM中驻留最多的线程（2048个），核函数中的每个线程最多只能用32个寄存器。当每个线程所用寄存器个数大于64时，SM的占有率将小于50%；当每个线程所用寄存器个数大于128时，SM的占有率将小于25%。对于图灵架构，同样的占有率允许使用更多的寄存器。\n(3) 有限的共享内存对占有率的约束清理。因为共享内存的数量随着计算能力的上升没有显著的变化规律，所以我们这里仅对计算能力3.5进行分析，对其他计算能力可以类似地分析。如果线程块大小为128，那么每个SM要激活16个线程块才能有2048个线程，达到100%的占有率。此时，一个线程块最多能使用3KB的共享内存。在不改变线程块大小的情况下，要达到50%的占有率，一个线程块最多能使用6KB的共享内存；要达到25%的占有率，一个线程块最多能使用12KB的共享内存。如果一个线程块使用了超过48KB的共享内存，会直接导致核函数无法允许。对其他线程块大小可进行类似的分析。\n在 CUDA 工具箱中，有一个 CUDA_Occupancy_Calculator.xls，可用来计算各种情况下的 SM 占有率。\n## 重要的计算能力技术指标 SM 寄存器数上限、单个线程块寄存器数上限、单个线程寄存器数上限、SM 共享内存上限、单个线程块共享内存上限、SM 线程块上限、 SM 线程数上限 代码中查询技术指标： ```c++ #include \u0026#34;error.cuh\u0026#34; //CHECK #include \u0026lt;cstdio\u0026gt; int main(int argc, char *argv[]) { int device_id = 0; if (argc \u0026gt; 1) device_id = atoi(argv[1]); CHECK(cudaSetDevice(device_id)); cudaDeviceProp prop; CHECK(cudaGetDeviceProperties(\u0026amp;prop, device_id)); printf(\u0026#34;Device id: %d\\n\u0026#34;, device_id); printf(\u0026#34;Device name: %s\\n\u0026#34;, prop.name); printf(\u0026#34;Compute capability: %d.%d\\n\u0026#34;, prop.major, prop.minor); printf(\u0026#34;Amount of global memory: %g GB\\n\u0026#34;, prop.totalGlobalMem / (1024.0 * 1024 * 1024)); printf(\u0026#34;Amount of constant memory: %g KB\\n\u0026#34;, prop.totalConstMem / 1024.0); printf(\u0026#34;Maximum grid size: %d %d %d\\n\u0026#34;, prop.maxGridSize[0], prop.maxGridSize[1], prop.maxGridSize[2]); printf(\u0026#34;Maximum block size: %d %d %d\\n\u0026#34;, prop.maxThreadsDim[0], prop.maxThreadsDim[1], prop.maxThreadsDim[2]); printf(\u0026#34;Number of SMs: %d\\n\u0026#34;, prop.multiProcessorCount); printf(\u0026#34;Maximum amount of shared memory per block: %g KB\\n\u0026#34;, prop.sharedMemPerBlock / 1024.0); printf(\u0026#34;Maximum amount of shared memory per SM: %g KB\\n\u0026#34;, prop.sharedMemPerMultiprocessor / 1024.0); printf(\u0026#34;Maximum number of registers per block: %d K\\n\u0026#34;, prop.regsPerBlock / 1024); printf(\u0026#34;Maximum number of registers per SM: %d K\\n\u0026#34;, prop.regsPerMultiprocessor / 1024); printf(\u0026#34;Maximum number of threads per block: %d\\n\u0026#34;, prop.maxThreadsPerBlock); printf(\u0026#34;Maximum number of threads per SM: %d\\n\u0026#34;, prop.maxThreadsPerMultiProcessor); } ## Tensor Core CUDA core 和 Tensor core 的区别： - Tensor core 是在 Volta 以及之后的架构中才有的, 相比于CUDA core，它可以提供更高效的运算。 - 每个 GPU clock，CUDA core 可以进行一次单精度乘加运算，即：in fp32: x += y * z。 - 每个 GPU clock，Tensor core 可以完成 4 × 4 的混合精度矩阵乘加 (matrix multiply-accumulate, MMA)：D=A * B + C，其中 A、B、C、D 都是 4 × 4 矩阵。A 和 B是 FP16 矩阵，而累加矩阵 C 和 D 可以是 FP16 或 FP32 矩阵（FP16/FP16 或 FP16/FP32 两种模式。所以每个 GPU clock，Tensor core 可以执行 64 个浮点 FMA 混合精度运算（4 × 4 × 4）。 - Turing 架构中新增了 INT8/INT32, INT4/INT32, INT1/INT32 等模式 ![Tensor Core](https://img2024.cnblogs.com/blog/798398/202402/798398-20240219182403297-314074416.png) V100 中，一个 SM 中有 8 个 Tensor core，每个 GPU clock 共可以执行 1024 个浮点运算（64 × 8 × 2，乘以 2 因为乘加是两个浮点运算） Reference: - [TENSOR CORE DL PERFORMANCE GUIDE](https://link.zhihu.com/?target=https%3A//developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9926-tensor-core-performance-the-ultimate-guide.pdf) - [GPU内存概念浅析](https://www.cnblogs.com/ArsenalfanInECNU/p/18021724) ","date":"2024-05-18","externalUrl":null,"permalink":"/posts/cuda/gpu/","section":"Blogs","summary":"CPU 与 GPU 的不同 # CPU，4个 ALU，主要负","title":"GPU 结构","type":"posts"},{"content":" Kernel Function # gridDim\ngridDim.x、gridDim.y、gridDim.z分别表示 grid 各个维度的大小\nblockDim\nblockDim.x、blockDim.y、blockDim.z分别表示 block 各个维度的大小\nblockIdx\nblockIdx.x、blockIdx.y、blockIdx.z分别表示当前 block 在 grid 中的坐标\nthreadIdx\nthreadIdx.x、threadIdx.y、threadIdx.z分别表示当前 thread 在 block 的坐标\ngrid 里总的线程个数 N = gridDim.x * gridDim.y * gridDim.z * blockDim.x * blockDim.y * blockDim.z\n通过 blockIdx.x、blockIdx.y、blockIdx.z、threadIdx.x、threadIdx.y、threadIdx.z 可以定位一个线程的坐标。\n主流架构一个block三个唯独的设置最多为(1024， 1024， 64)，同时总线程数最多只能有 1024 个。\n将所有的线程排成一个序列，序列号为 0 , 1 , 2 , … , N ，如何找到当前 thread 的序列号 ?\n先找到该thread所在的 block的序号 blockId = blockIdx.x + blockIdx.ygridDim.x + blockIdx.zgridDim.x*gridDim.y 然后找到当前 thread 在 block 中的序号 threadId = threadIdx.x + threadIdx.yblockDim.x + threadIdx.zblockDim.x*blockDim.y 计算一个 block 中一共有多少个 thread， M = blockDim.xblockDim.yblockDim.z 求得当前的线程的序列号 idx = threadId + M*blockId Device function # kernel 可以调用不带执行配置的自定义函数，这样的自定义函数称为设备函数（devicefunction）。它是在设备中执行，并在设备中被调用的。与之相比，核函数是在设备中执行，但在主机端被调用的。\ndouble __device__ add1_device(const double x, const double y) { return (x + y); } void __global__ add1(const double *x, const double *y, double *z, const int N) { const int n = blockDim.x * blockIdx.x + threadIdx.x; if (n \u0026lt; N) { z[n] = add1_device(x[n], y[n]); } } CUDA 常用的函数 # 同步函数 __syncthreads: 只能用在核函数中 __syncthreads(),该函数可保证一个线程块中的所有线程（或者说所有线程束）在执行该语句后面的语句之前都完全执行了该语句前面的语句。然而，该函数只是针对同一个线程块中的线程的，不同线程块中线程的执行次序依然是不确定的。\nCUDA Event Record # 在 C++ 中，有多种可以对一段代码进行计时的方法，如使用 GCC 的 clock 函数和与头文件 对应的时间库、GCC 中的 gettimeofday 函数。\nCUDA 提供了一种基于 CUDA 事件（CUDA event）的计时方式，可用来给一段 CUDA 代码（可能包含了主机代码和设备代码）计时。下面的例子涵盖了计时的基本流程：\n//creat CHECK(cudaEventCreate(\u0026amp;start)); CHECK(cudaEventCreate(\u0026amp;stop)); //record CHECK(cudaEventRecord(start)); cudaEvent_t start, stop; cudaEventQuery(start) //+需要计时的代码块 CHECK(cudaEventRecord(stop)); CHECK(cudaEventSynchronize(stop)); float elapsed_time; //compute CHECK(cudaEventElapsedTime(\u0026amp;elapsed_time, start, stop)); printf(\u0026#34;Time = %g ms.\\n\u0026#34;, elapsed_time); //clean CHECK(cudaEventDestroy(start)); CHECK(cudaEventDestroy(stop)); Steps of CUDA Program # Config GPU(device), cudaSetDevice() Allocate device memory, cudaMalloc(), cudaMallocManaged() Allocate CPU(Host) memory Copy data from host to device, cudaMemcpy() Run kernel on device Copy result from device to host, cudaMemcpy() Print result on host Release host and device memory, cudaFree(), free() CPU is always called host, the GPU is called device\nC Example:\n// file: sample.cu #include\u0026lt;stdint.h\u0026gt; #include\u0026lt;cuda.h\u0026gt; #include \u0026#34;cuda_runtime.h\u0026#34; #include \u0026#34;device_launch_parameters.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; // This is a sample CUDA kernel, called on host and execte on device __global__ void add(float* a) { a[threadIdx.x] = 1; } int main(int argc, char** argv) { // 1. Set device cudaSetDevice(0); // 2. Allocate device memory float* dx; cudaMalloc((void**)\u0026amp;dx, 16 * sizeof(float)); // 3. Allocate host memory float hx[16] = { 0 }; // 4. Copy data from host to device cudaMemcpy(dx, hx, 16 * sizeof(float), cudaMemcpyHostToDevice); // 5. Run kernel on device add \u0026lt;\u0026lt; \u0026lt;1, 16 \u0026gt;\u0026gt; \u0026gt; (dx); // 6. Copy result from device to host cudaMemcpy(hx, dx, 16 * sizeof(float), cudaMemcpyDeviceToHost); // 7. Print result on host for (int i = 0; i \u0026lt; 16; i++) { printf(\u0026#34;%f \\n\u0026#34;, hx[i]); } 8. Release host and device memory cudaFree(dx); free(hx); //cudaDeviceReset(); return 0; } 使用nvcc编译,然后运行\nnvcc sample.cu - o sample ./sample CUDA 提供了统一内存: gpu和cpu可访问的单一内存空间. 调用cudaMallocManaged()，它返回一个指针，从host code或device code都可以访问。要释放数据，只需将指针传递给cudaFree()。\nCUDA Kernel and Parrallel Computing # 前置知识: 理解 GPU 结构, Grid, Block, Thread 这几个逻辑概念之间的关系\nCUDA kernel 的编程模型\n[Todo] Dim and size detail\n调用kernel: add \u0026lt;\u0026lt; \u0026lt;blockNumber, threadNumber \u0026gt;\u0026gt; \u0026gt; (dx);\n编写kernel：\n用关键字描述符 __global__ 声明kernel: __global__ void add(){} 调用 kernel 时的参数 \u0026lt;\u0026lt;\u0026lt;blockNumber per grid, threadNumber per block\u0026gt;\u0026gt;\u0026gt; 决定了共有 TotalThreadNum = blockNumber * threadNumber 个线程可以并行执行任务 kernel 内的每一次迭代，意味着 TotalThreadNum 个线程并行执行了一次循环体中的任务（即每个线程完成对一份数据的处理），也就是每次迭代能处理 TotalThreadNum 份数据，TotalThreadNum 也等价于跨步(stride)的大小 kernel 中 threadIdx.x 代表 the index of the thread within the block， blockDim.x 代表 the size of block（number of threads in block（假设 这里的 grid 和 block 的 dim 只有一维） kernel 内 threadIdx.x 和 blockIdx.x 的组合对应线程的唯一标识 以add_3这个 kernel 为例，可以用 index = blockIdx.x * blockDim.x + threadIdx.x 获得当前线程的要处理的数据的数组下标（见下图），\n__global__ void add_3(int n, float *x, float *y) { int index = blockIdx.x * blockDim.x + threadIdx.x; // stride为grid的线程总数:blockDim.x*gridDim.x int stride = blockDim.x * gridDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] = x[i] + y[i]; } Kernel examples(下面的 C++ Example)\n1个block,1个线程: add_1() 1个block,多个线程: add_2() 多个block,多个线程: add_3() 多个block，多个线程也称为网格跨步循环，其中每次循环的跨步(stride)为 grid 的线程总数: stride = blockDim.x * gridDim.x\nC++ Example:\n// file: add.cu #include \u0026lt;iostream\u0026gt; #include \u0026lt;math.h\u0026gt; // Kernel function to add the elements of two arrays // single thread __global__ void add_1(int n, float *x, float *y) { for (int i = 0; i \u0026lt; n; i++) y[i] = x[i] + y[i]; } // single block, multi threads __global__ void add_2(int n, float *x, float *y) { int index = threadIdx.x; int stride = blockDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] = x[i] + y[i]; } // multi block, multi threads // 网格跨步(stride)循环 __global__ void add_3(int n, float *x, float *y) { int index = blockIdx.x * blockDim.x + threadIdx.x; // stride为grid的线程总数:blockDim.x*gridDim.x int stride = blockDim.x * gridDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] = x[i] + y[i]; } int main(void) { int N = 1\u0026lt;\u0026lt;20; float *x, *y; // Allocate Unified Memory – accessible from CPU or GPU cudaMallocManaged(\u0026amp;x, N*sizeof(float)); cudaMallocManaged(\u0026amp;y, N*sizeof(float)); // initialize x and y arrays on the host for (int i = 0; i \u0026lt; N; i++) { x[i] = 1.0f; y[i] = 2.0f; } // Run kernel on 1M elements on the GPU // Just run with single thread :) add_1\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Run with 1 block and multi threads add_2\u0026lt;\u0026lt;\u0026lt;1, 256\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Run with multi block and multi threads int blockSize = 256;//并行线程数量 int numBlocks = (N + blockSize - 1) / blockSize;//线程块数量 add_3\u0026lt;\u0026lt;\u0026lt;numBlocks, blockSize\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Wait for GPU to finish before accessing on host cudaDeviceSynchronize(); // Check for errors (all values should be 3.0f) float maxError = 0.0f; for (int i = 0; i \u0026lt; N; i++) maxError = fmax(maxError, fabs(y[i]-3.0f)); std::cout \u0026lt;\u0026lt; \u0026#34;Max error: \u0026#34; \u0026lt;\u0026lt; maxError \u0026lt;\u0026lt; std::endl; // Free memory cudaFree(x); cudaFree(y); return 0; } CUDA Code Profiling # nvprof是CUDA工具包附带的命令行GPU分析器\nReference:\nNVIDIA CUDA Docs CUDA C++ Programming Guide CUDA 矩阵乘法终极优化指南 ","date":"2024-05-10","externalUrl":null,"permalink":"/posts/cuda/cuda_basic/","section":"Blogs","summary":"Kernel Function # gridDim gridDim.x、gridDi","title":"CUDA Programming","type":"posts"},{"content":" 动态规划(DP)的基本思路 # DP的前提是递归，任何DP问题一定对应一个存在重复调用的递归问题，DP的作用就是解决递归中的重复调用/计算问题。\n基本思路：\n首先完成递归解法，一维递归的时间复杂度\\(O(2^n)))，空间复杂度\\(O(log_n)\\)，时间复杂度高主要原因是存在重复计算 递归时，要确定 base case，即判断递归到达底部边界，及其返回值 1维递归的特点是 f(i) 的结果可能依赖于前面的 1～2 种结果，比如f(i-1), f(i-2,j)等 递归转换为带有DP缓存表的、递归版的、自顶向底的DP 顶指的是最终目标答案，底指的是base case 递归版 DP 基本只是在递归解法的基础上增加了 DP 表，其他逻辑照抄 以空间换时间，dp[i] 存 f(i) 的结果，递归时，如果 dp[i] 已存在则直接返回结果，否则继续递归，并将结果存入dp[i]，这样就减少了重复计算 时间复杂度\\(O(n)))，空间复杂度\\(O(n)\\) 将递归版的DP转化为严格位置依赖的、迭代版的、自底向顶的DP dp[i] 依赖于若干个 dp[\u0026lt;i] 的值，最终结果是 dp[n] 时间复杂度\\(O(n)))，空间复杂度\\(O(n)\\) 继续优化DP，空间压缩 用有限个变量的滚动更新代替dp数组，比如 cur = last + lastlast 时间复杂度\\(O(n)))，空间复杂度\\(O(1)\\) leetcode 64.最小路径和[中等] # 64.最小路径和\n给定一个包含非负整数的 m x n 网格 grid ，请找出一条从左上角到右下角的路径，使得路径上的数字总和为最小。\n说明：每次只能向下或者向右移动一步。\nclass Solution { public: int minPathSum(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid) { // return f1(grid, grid.size()-1, grid[0].size()-1); // return f2(grid); // return f3(grid); return f4(grid); } //递归：只能想下或向右移动, f(i,j) = max(f(i-1,j), f(i,j-1))+grid[i,j] int f1(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid, int i, int j){ if(i==0 \u0026amp;\u0026amp; j==0){ return grid[0][0]; } int up=INT_MAX; int left=INT_MAX; if(i-1\u0026gt;=0){ up = f1(grid,i-1,j); } if(j-1\u0026gt;=0){ left = f1(grid,i,j-1); } return min(up, left)+grid[i][j]; } //DP int f2(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid){ vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; dp(grid.size(), vector\u0026lt;int\u0026gt;(grid[0].size(), -1));//存放(i,j)位置的最短路径 dp[0][0] = grid[0][0]; return f2_dp1(grid, grid.size()-1, grid[0].size()-1, dp); } //dp_1，递归版、自顶向底的DP int f2_dp1(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid, int i, int j, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; dp){ if(dp[i][j] != -1) return dp[i][j]; int up=INT_MAX; int left=INT_MAX; if(i-1\u0026gt;=0){ up = f2_dp1(grid,i-1,j,dp); } if(j-1\u0026gt;=0){ left = f2_dp1(grid,i,j-1,dp); } dp[i][j]= min(up, left)+grid[i][j]; return dp[i][j]; } // 优化DP， 递归转迭代, 严格位置依赖的动态规划 // 从左往右，从上往下，一行一行的计算dp[i][j] int f3(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid){ vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; dp(grid.size(), vector\u0026lt;int\u0026gt;(grid[0].size(), -1));//存放(i,j)位置的最短路径 dp[0][0] = grid[0][0]; for(int i=1; i\u0026lt;grid.size(); i++){ dp[i][0] = dp[i-1][0]+grid[i][0]; } for(int j=1; j\u0026lt;grid[0].size(); j++){ dp[0][j] = dp[0][j-1]+grid[0][j]; } for(int i=1; i\u0026lt;grid.size(); i++){ for(int j=1; j\u0026lt;grid[0].size(); j++){ dp[i][j] = min(dp[i-1][j], dp[i][j-1]) + grid[i][j]; } } return dp[grid.size()-1][grid[0].size()-1]; } // 继续优化DP，减小dp数组，二位dp缓存表 先转成2个一维数组，再优化就是一个以为数组，grid每一行复用 int f4(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid){ vector\u0026lt;int\u0026gt; dp(grid[0].size()); dp[0] = grid[0][0]; for(int j=1; j\u0026lt;grid[0].size(); j++){ dp[j] = dp[j-1]+grid[0][j]; } for(int i=1; i\u0026lt;grid.size(); i++){ // i = 1，dp表变成想象中二维表的第1行的数据 // ... // i = n-1，dp表变成想象中二维表的第n-1行的数据 //先更新一下这一行左侧第一个值，相当与原来的dp[i][j=0] dp[0] += grid[i][0]; for(int j=1; j\u0026lt;grid[0].size(); j++){ // dp[j]代表了原来的up=dp[i][j-1]的值，dp[j-1]原来的left=dp[i-1][j]的值 dp[j] = min(dp[j], dp[j-1]) + grid[i][j]; } } return dp[grid[0].size()-1]; } }; leetcode 1147. 单词搜索[中等] # 1147. 单词搜索\n给定一个 m x n 二维字符网格 board 和一个字符串单词 word 。如果 word 存在于网格中，返回 true ；否则，返回 false 。\n单词必须按照字母顺序，通过相邻的单元格内的字母构成，其中“相邻”单元格是那些水平相邻或垂直相邻的单元格。同一个单元格内的字母不允许被重复使用。\nclass Solution { public: bool exist(vector\u0026lt;vector\u0026lt;char\u0026gt;\u0026gt;\u0026amp; board, string word) { // 带路径的递归 // 双循环遍历每一个位置，从该位置作起点，用递归法搜索相邻位置，看是够能够完全匹配当w // 递归时，退出条件是:1.匹配到完整的w，返回true；2.无法匹配，返回false。如果当前字符匹配，则继续通过相邻位置去匹配下一个字符。走过的位置用mask掩盖下，防止后续搜索中重复过去的位置 int is = board.size(), js = board[0].size(); for(int r = 0; r \u0026lt; is; r++){ for(int c = 0; c \u0026lt; js; c++){ if(f1(board, word, 0, r, c)){ return true; } } } return false; } // k代表word需要匹配的位置 bool f1(vector\u0026lt;vector\u0026lt;char\u0026gt;\u0026gt;\u0026amp; board, string\u0026amp; word, int k, int i, int j){ if(k == word.size()){ return true; } if(i \u0026lt; 0 || j \u0026lt; 0 || i \u0026gt;= board.size() || j \u0026gt;= board[0].size() || board[i][j] != word[k]){ return false; } // 加入mask，但要保留字符用于递归结束的回填 auto tmp = board[i][j]; board[i][j] = \u0026#39;0\u0026#39;;// 这时如果下面的递归中再来到这个位置，一定有b[i][j] != w[k] bool result = f1(board, word, k + 1, i - 1, j); result = result || f1(board, word, k + 1, i + 1, j); result = result || f1(board, word, k + 1, i, j - 1); result = result || f1(board, word, k + 1, i, j + 1); board[i][j] = tmp; return result; } }; leetcode 1143. 最长公共子序列[中等] # 1143. 最长公共子序列\n给定两个字符串 text1 和 text2，返回这两个字符串的最长 公共子序列 的长度。如果不存在 公共子序列 ，返回 0 。\n一个字符串的 子序列 是指这样一个新的字符串：它是由原字符串在不改变字符的相对顺序的情况下删除某些字符（也可以不删除任何字符）后组成的新字符串。\n例如，\u0026ldquo;ace\u0026rdquo; 是 \u0026ldquo;abcde\u0026rdquo; 的子序列，但 \u0026ldquo;aec\u0026rdquo; 不是 \u0026ldquo;abcde\u0026rdquo; 的子序列。 两个字符串的 公共子序列 是这两个字符串所共同拥有的子序列。\n示例 1：\n输入：text1 = \u0026ldquo;abcde\u0026rdquo;, text2 = \u0026ldquo;ace\u0026rdquo; 输出：3\n解释：最长公共子序列是 \u0026ldquo;ace\u0026rdquo; ，它的长度为 3 。 示例 2：\n输入：text1 = \u0026ldquo;abc\u0026rdquo;, text2 = \u0026ldquo;abc\u0026rdquo; 输出：3 解释：最长公共子序列是 \u0026ldquo;abc\u0026rdquo; ，它的长度为 3 。 示例 3：\n输入：text1 = \u0026ldquo;abc\u0026rdquo;, text2 = \u0026ldquo;def\u0026rdquo; 输出：0 解释：两个字符串没有公共子序列，返回 0 。\nclass Solution { public: int longestCommonSubsequence(string text1, string text2) { return f3(text1,text2); } // 递归法(超时) // a[i],b[j]的结果依赖于三种情况 int f1(string text1, string text2){ return f1_help(text1,text2,text1.size(),text2.size()); } int f1_help(string a, string b, int len1, int len2){ if(len1==0 || len2==0) return 0; int ans; if(a[len1-1]==b[len2-1]){ ans = 1+f1_help(a,b,len1-1,len2-1); }else{ ans = max(f1_help(a,b,len1-1,len2), f1_help(a,b,len1,len2-1));//还有一种可能f1_help(a,b,i-1,j-1)，必然小于这两种可能，所以直接舍弃 } return ans; } //DP，递归直接转dp表，自顶向底的DP int f2(string text1, string text2){ int n=text1.size(); int m=text2.size(); vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; dp(n+1, vector\u0026lt;int\u0026gt;(m+1,-1)); return f2_help(text1,text2,n,m,dp);//[0...n] [0...m] } int f2_help(string a, string b, int len1, int len2, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; \u0026amp;dp){ //二维dp表的第一行和第一列为0 if(len1==0 || len2==0){ return 0; } if(dp[len1][len2]!=-1) { return dp[len1][len2]; } int ans; if(a[len1-1]==b[len2-1]){ ans = 1+f2_help(a,b,len1-1,len2-1,dp); }else{ ans = max(f2_help(a,b,len1-1,len2,dp), f2_help(a,b,len1,len2-1,dp));//还有一种可能f1_help(a,b,i-1,j-1)，必然小于这两种可能，所以直接舍弃 } dp[len1][len2]=ans; return ans; } //DP-2:严格位置依赖的DP（递归转迭代，自底向顶的DP） int f3(string text1, string text2){ int n=text1.size(); int m=text2.size(); vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; dp(n+1, vector\u0026lt;int\u0026gt;(m+1,0)); //二维dp表的第一行和第一列为0，但这两个循环可以省略 // for(int i=0; i\u0026lt;=m;i++){ // dp[0][m]=0; // } // for(int i=0; i\u0026lt;=n;i++){ // dp[n][0]=0; // } //开始更新二维dp表，从上向下，从左到右，按行更新，循环的i、j是len1、len2 //dp[i][j]依赖三个位置，左上，上，左 for(int i=1;i\u0026lt;=n;i++){ for(int j=1;j\u0026lt;=m;j++){ int leftup=dp[i-1][j-1]; int left=dp[i][j-1]; int up=dp[i-1][j]; if(text1[i-1]==text2[j-1]){ dp[i][j] = leftup+1; }else{ dp[i][j] = max(left, up); } } } return dp[n][m]; } //DP-3：对DP2的二维dp表压缩为数组（一行），位置依赖中的up和left很容易，但是leftup需要把前一个的备份一下 // Todo }; leetcode 516. 最长回文子序列[中等] # 516. 最长回文子序列\n给你一个字符串 s ，找出其中最长的回文子序列，并返回该序列的长度。\n子序列定义为：不改变剩余字符顺序的情况下，删除某些字符或者不删除任何字符形成的一个序列。\n示例 1：\n输入：s = \u0026ldquo;bbbab\u0026rdquo; 输出：4 解释：一个可能的最长回文子序列为 \u0026ldquo;bbbb\u0026rdquo; 。 示例 2：\n输入：s = \u0026ldquo;cbbd\u0026rdquo; 输出：2 解释：一个可能的最长回文子序列为 \u0026ldquo;bb\u0026rdquo; 。\nclass Solution { public: int longestPalindromeSubseq(string s) { return f3(s); } // 暴力：搞一个逆转的string，两个比较 // 递归(超时)：left,right两个指针，left-\u0026gt;...\u0026lt;-right // s[left...right]的最长回文子序列，依赖于四种情况 // 1. s[left]==s[right]时，结果为 2 + s[left+1...right-1]的最长回文子序列 // 2, s[left]!=s[right]时，有三种情况,取最大值: // s[left+1...right], s[left...right-1], s[left+1...right-1]，其中欧冠你最后一种可以省略 int f1(string s){ if(s.size()==0) return 0; int l = 0; int r = s.size()-1; return f1_help(s, l, r); } int f1_help(string s, int l, int r){ if(l==r) return 1;//如 a if(l+1==r) return s[l]==s[r] ? 2: 1;//如 aa 或 ab if(s[l]==s[r]){ return 2 + f1_help(s, l+1, r-1); }else{ return max(f1_help(s, l+1,r),f1_help(s, l,r-1)); } } //DP(内存超限): 二维dp表，自顶向底的DP（递归版DP） int f2(string s){ if(s.size()==0) return 0; int l = 0; int r = s.size()-1; vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; dp(s.size(), vector\u0026lt;int\u0026gt;(s.size())); return f2_help(s, l, r, dp); } int f2_help(string s, int l, int r, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; \u0026amp;dp){ if(l==r) return 1;//如 a if(l+1==r) return s[l]==s[r] ? 2: 1;//如 aa 或 ab if(dp[l][r]!=0) return dp[l][r]; int ans; if(s[l]==s[r]){ ans = 2 + f2_help(s, l+1, r-1, dp); }else{ ans = max(f2_help(s, l+1,r,dp),f2_help(s, l,r-1,dp)); } dp[l][r] = ans; return ans; } //DP: 严格位置依赖的DP，迭代版 // dp表的对角线上的值dp[i][i]=1,该值的右侧dp[i][i+1]=s[i]==s[i+1] ? 2: 1 // dp表对角线左下的元素都是l\u0026lt;r，无意义 // dp表中其他的值依赖于三个位置：左侧、下侧、左下侧 // 注意：最终结果是表的右上角，dp表要从下往上，从左往右去更新 int f3(string s){ if(s.size()==0) return 0; int n = s.size(); vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; dp(n, vector\u0026lt;int\u0026gt;(n)); for(int i=n-1; i\u0026gt;=0; i--){ dp[i][i] = 1; if(i+1\u0026lt;n){ dp[i][i+1]=s[i]==s[i+1] ? 2: 1; } for(int j=i+2; j\u0026lt;n; j++){ if(s[i]==s[j]){ dp[i][j] = 2 + dp[i+1][j-1];//左下侧 }else{ dp[i][j] = max(dp[i+1][j], dp[i][j-1]);//左和右侧 } } } return dp[0][n-1]; } //DP继续优化，可以压缩空间，用数组代替而二维表，注意加一个变量缓存迁移个位置的左下侧结果 }; Reference:\n左程云算法\n","date":"2024-05-10","externalUrl":null,"permalink":"/posts/algorithm/leetcode/dp_dim1/","section":"Blogs","summary":"动态规划(DP)的基本思路 # DP的前提是","title":"Dynamic Programming: Dim-1","type":"posts"},{"content":"","date":"2024-05-05","externalUrl":null,"permalink":"/tags/flash-attention/","section":"Tags","summary":"","title":"Flash Attention","type":"tags"},{"content":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n与标准 attention 相比，Flash Attention 有以下三点特点：\n运算速度更快 (Fast) 更节省显存 (Memory-Efficient) 计算结果相同 (Exact) FlashAttention 目的不是节约 FLOPs，而是减少对HBM的访问。它没有改变原有的计算公式，整体计算复杂度并未降低。\n背景 # GPU中存储单元主要有 HBM 和 SRAM：HBM 容量大但是访问速度慢，SRAM容量小却有着较高的访问速度。例如：A100 GPU有40-80GB的HBM，带宽为1.5-2.0TB/s；每108个流式多核处理器各有192KB的片上SRAM，带宽估计约为 19TB/s。可以看出，片上的SRAM比HBM快一个数量级，但尺寸要小许多数量级。\n当输入序列（sequence length）较长时，Transformer的计算过程缓慢且耗费内存，这是因为 self-attention 的 time 和 memory complexity 会随着 sequence length 的增加成二次增长。\n标准Attention的计算过程： $$ S=Q K^T \\in \\mathbb{R}^{N \\times N} $$ $$ P=\\operatorname{softmax}(S) \\in \\mathbb{R}^{N \\times N} $$ $$ O=P V \\in \\mathbb{R}^{N \\times N} $$\n标准Attention的中间结果 𝑆, 𝑃 通常需要通过高带宽内存（HBM）进行存取，两者所需内存空间复杂度为\\(O(Nd+N^2)\\), 对 HBM 的重复读写是主要瓶颈。要解决这个问题，需要做两件事：\n在不访问整个输入的情况下计算 softmax 不为反向传播存储大的中间 attention 矩阵(\\(N^2\\)) FlashAttention V1 # FlashAttention 提出了两种方法来解决上述问题：tiling 和 recomputation。\ntiling - 注意力计算被重新构造，将输入分割成块，并通过在输入块上进行多次传递来递增地执行softmax操作。 recomputation - 存储来自前向的 softmax 归一化因子，以便在反向中快速重新计算芯片上的 attention，这比从HBM读取中间矩阵的标准注意力方法更快。可以把它看作基于 tiling 的特殊的 gradient checkpointing 正常的softmax计算：\n$$m(x):=\\max _i x i$$ $$f(x):=\\left[e^{x_1-m(x)} \\ldots e^{x_B-m(x)}\\right]$$ $$\\ell(x):=\\sum_i f(x)_i$$ $$\\operatorname{softmax}(x):=\\frac{f(x)}{\\ell(x)}$$\nsoftmax 伪代码: softmax 函数需要三个循环，第一个循环计算数组的最大值，第二个循环计算 softmax 的分母，第三个循环计算 softmax 输出。\n分块的 softmax 计算(假设分2块并行计算)：\n$$m(x)=m\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)=\\max \\left(m\\left(x^{(1)}\\right), m\\left(x^{(2)}\\right)\\right) $$ $$f(x)=\\left[e^{m\\left(x^{(1)}\\right)-m(x)} f\\left(x^{(1)}\\right) \\quad e^{m\\left(x^{(2)}\\right)-m(x)} f\\left(x^{(2)}\\right)\\right] $$ $$\\ell(x)=\\ell\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)=e^{m\\left(x^{(1)}\\right)-m(x)} \\ell\\left(x^{(1)}\\right)+e^{m\\left(x^{(2)}\\right)-m(x)} \\ell\\left(x^{(2)}\\right) $$ $$\\operatorname{softmax}(x)=\\frac{f(x)}{\\ell(x)}$$\n复杂度分析：\n读取Q，K，写入\\(S=QK^T\\)，内存访问复杂度\\(O(Nd+N^2)\\) 读取S，写入\\(P=softmax(S)\\)，内存访问复杂度\\(O(N^2)\\) 读取V和P，写入\\(O=PV\\)，内存访问复杂度\\(O(Nd+N^2)\\) 综上，self-attention的 HBM 访问复杂度\\(O(Nd+N^2)\\)\n分块的 softmax 伪代码: 在第一个循环中同时对最大值\\(m\\)以及 softmax 的分母\\(d\\)进行更新，从而减少了一个循环。通过 tiling 的方式，softmax 的循环数从三个减到了两个，从而可以降低内存消耗。\nflashattention 伪代码： 中间变量：\\(O_i\\)(最终乘积)、\\(l_i\\)（softmax的分母，即累加和）、\\(m_i\\)（遍历到当前块为止的最大值），再也不用保存全部的S和P了。\n由于重新计算导致 FLOPs 增加，但是由于大量减少HBM访问，FlashAttention 运行速度更快\nFlashAttention的 FLOPs 为 \\(𝑂(𝑁^2𝑑)\\)，除了 input 和 output，额外需要的内存为 \\(𝑂(𝑁)\\), 对HBM访问的次数为 \\(𝑂(𝑁^2𝑑^2𝑀^{−1})\\), 其中 M 为SRAM的大小，当 \\(M=O(Nd)\\)时，对HBM访问的次数为\\(O(Nd)\\)， 远远小于标准 Attention 的 \\(O(Nd+N^2)\\)\nPyTorch 2.0已将 FlashAttention 集成到官方库中，可以直接调用 torch.nn.functional.scaled_dot_product_attention\n总结 # FlashAttention V1:\n通过切块技术减少了内存访问次数，提高了计算速度和内存利用率。 内存访问复杂度为 \\(𝑂(𝑁^2𝑑^2𝑀^{−1})\\), 比标准 Attention 的 \\(O(Nd+N^2)\\)更高效 Reference：\n一些改cuda加速的思路：FlashAttention、PagedAttention、LightSeq、ByteTransformer FlashAttention: 更快训练更长上下文的GPT 手撕Flash Attention ","date":"2024-05-05","externalUrl":null,"permalink":"/posts/llm/flash_attention/","section":"Blogs","summary":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness 与标准 attention 相比，Flash","title":"Flash Attention","type":"posts"},{"content":" Self-Attention # 对于self-attention，由于 Q, K, V 都来自输入 X ，在计算 \\(QT^T\\) 时，模型很容易关注到自身的位置上，也就是 \\(QT^T\\) 对角线上的激活值会明显比较大, 这会削弱模型关注其他高价值位置的能力，也就限制模型的理解和表达能力。MHA 对这个问题有一定的缓解作用。\nMHA # MHA(multi-head attention)\nQKV 经过线性变换后，将他们分别在 hidden states 维度上切分成 heads 份。\nMHA 相比单头的情况，相当于只是把 QKV 切成多份并行计算了，对于实际需要缓存的大小没有影响\nKV Cache # Decoding阶段，下一个step的输入其实包含了上一个step的内容，而且只在最后面多了一个token的数据，下一个step的计算应该也包含了上一个step的计算。\nKV Cache 的目的：空间换时间，用缓存把需要重复利用的中间计算结果存下来，减少重复计算。而 K 和 V 就是要缓存的对象。\nQ K V\n对于输入长度为 \\(seq\\) ，层数为 \\(L\\) ，hidden size为 \\(d\\) 的模型:\n当 batch size=1 时 需要缓存的参数量为: 2Ls*d ，其中 2 表示 K + V 需要的空间为（使用半精度浮点数 float16）：22Lsd Bytes ，其中第一个 2 表示 float16 占用 2 Bytes 当 batch size=B 时 需要缓存的参数量为: 2LsdB 需要的空间为（使用半精度浮点数 float16）：22Lsd*B Bytes MHA 相比单头的情况，相当于只是把 QKV 切成多份并行计算了，对于实际需要缓存的大小没有影响 GQA、MQA [Todo] 以Llama2 7B为例，L=32, d=4096，此时每个 token 需要的 cache 空间为 524,288 Bytes(512 KB)，当 s=1024, batch size=1 时，需要 512 MB\n主流显卡配置：\nNV A100(Ampere Arch)，HBM2e 40/80GB，L2 Cache 40MB, CUDA Cores - 16896/14592 NV H100(Hopper Arch), HBM2e/HBM3 80GB, L2 Cache 50MB, CUDA Cores 6912 NV V100(Volta Arch), HBM2 16/32GB, L2 Cache 6MB, CUDA Cores 5120 H100也只有50M的L2 Cache，只能支持Llama2 7B共100个tokens的seq，超出L2 Cache的部分只能走到显存中去了，但是 HBM 的 bandwidth 比 L2 Cache 小多了，A100 memory bandwidth 如下图所示：\nMQA # 《Fast Transformer Decoding: One Write-Head is All You Need》\nQ 经过线性变换后，MQA 只对 Q 进行 多个 head 的切分，每个 head 的 Q 的维度变为 \\(Q_s*(d/heads)\\)， K和V并不切分，而是线性变换时直接把hidden state维度降低为 d/heads， 然后 heads 个 Q 分别于 同一份 K，V 继续宁 attention 计算，最后将结果 concat 起来。\n比如在Llama2 7B中用了32个头，MQA后，1024个 tokens 需要 KVCache 就变成MHA 的 1/32，即 512MB/32=16MB，基本可以全部放入A100的L2 Cache\n由于共享了多个头的参数，限制了模型的表达能力，MQA虽然能好地支持推理加速，但是在效果上略略比MHA差一点\nGQA # 《GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》\nGQA（Grouped-Query Attention）提出了一个折中的办法，既能减少 MQA 的损失，又比 MHA 需要更少的缓存。\nGQA里， Q 还是按原来MHA/MQA的做法不变，但是 用多份 K和V，不过数量小于 Q 的 heads。相当于把 Q 的多个头给分了group，同一个group内的 Q 共享同一套 KV，不同group的 Q 所用的 KV 不同。\nGQA的速度相比MHA有明显提升，而效果上比MQA也好一些，能做到和MHA基本没差距。Llama2 70B 用的就是GQA。\n未完待续\u0026hellip;\nTodo: 代码，GQA量化计算\nReference:\n理解Attention:从起源到MHA,MQA和GQA self-attention code ","date":"2024-05-05","externalUrl":null,"permalink":"/posts/llm/attention/","section":"Blogs","summary":"Self-Attention # 对于self-attention，由","title":"Attention and KV Cache","type":"posts"},{"content":"","date":"2024-05-05","externalUrl":null,"permalink":"/tags/kvcache/","section":"Tags","summary":"","title":"KVCache","type":"tags"},{"content":" Me # class Me: def __init__(self): self.names = \u0026#34;Victor Yang\u0026#34; self.prefer_name = \u0026#34;Victor\u0026#34; self.profession = \u0026#34;Programmer\u0026#34; self.hometown = \u0026#34;Shandong, CN\u0026#34; self.current_location = \u0026#34;Beijing, CN\u0026#34; self.email = \u0026#34;chzhyangchn@gmail.com\u0026#34; self.graduate = \u0026#34;University of Science and Technology of China(USTC), Software Engineering\u0026#34; self.undergraduate = \u0026#34;China University of Petroleum(UPC), Computer Science\u0026#34; self.interests = [\u0026#34;AI\u0026#34;, \u0026#34;Taoist\u0026#34;, \u0026#34;Traditional Chinese Medicine\u0026#34;, \u0026#34;Finance\u0026#34;, \u0026#34;Quantitative Trading\u0026#34;, \u0026#34;History\u0026#34;, \u0026#34;Science Fiction\u0026#34;] self.hoppies = [\u0026#34;Reading\u0026#34;, \u0026#34;Yoga\u0026#34;, \u0026#34;Meditation\u0026#34;, \u0026#34;Swimming\u0026#34;] My Skill Set # Generative AI/AIGC: Transformer, LLM, VLM, Llama, Llava, Stable Diffusion, MoE, Inference Optimization, LLM Serving, Quantization, vLLM, DeepSpeed, RAG, Langchain, Agent Parallel Computing: CUDA, GPU, Horovod, MPI Machine Learning: AI Framework, PyTorch, Federated Learning Programming Language: Python, C++, GoLang Cloud Native: Knative, Ray, Docker Others: Linux ","date":"2024-05-01","externalUrl":null,"permalink":"/about/","section":"Welcome to My Blog","summary":"Me # class Me: def __init__(self): self.names = \u0026#34;Victor Yang\u0026#34; self.prefer_name = \u0026#34;Victor\u0026#34; self.profession = \u0026#34;Programmer\u0026#34; self.hometown = \u0026#34;Shandong, CN\u0026#34; self.","title":"About","type":"page"},{"content":" Data in Memory # Float 和 Double 类型的数据在内存中以二进制方式存储，由三部分组成：\n符号位 S（Sign）: 0 代表正数，1 代表负数 指数位 E（Exponent）: 存储科学计数法中的指数部分，指数位越多，可表示的数值范围越大。 尾数位 M（Mantissa）: 存储尾数（小数）部分，尾数位越多，可表示的数值精度越高。 INT 类型只包括符号位和指数位，没有尾数位。\n在计算机中，任何一个数都可以表示为 \\(1.xxx × 2^n\\) 的形式，其中 n 是指数位，xxx 是尾数位，如 Float 9.125 在计算机中分别按照整数和尾数的二进制进行存储:\n9 的二进制为 1001 0.125 的二进制为 0.001 9.125 表示为 1001.001，其二进制的科学计数法表示为 \\(1.001001 × 2^3\\) DL 中模型的权重和激活通常由单精度浮点数(FP32)表示，如下图所示，FP32 包含1位符号位，8位指数位和23位尾数位，可以表示 1.18e-38 和 3.4e38 之间的值\nDataTypes Comparation in AI # 类型 bits 符号位 指数位 尾数位 范围 精度 原理 其他 FP32 32 1 8 23 \\(-3.4 \\times 10^{38}\\) ~ \\(3.4 \\times 10^{38}\\) \\(10^{-6}\\) 大部分CPU/GPU/深度学习框架中默认使用FP32 FP16 16 1 5 10 -65504 ~ 65504 \\(10^{-3}\\) 预训练LLM保存时默认使用的格式 TF32 19 1 8 10 \\(-3.4 \\times 10^{38}\\) ~ \\(3.4 \\times 10^{38}\\) \\(10^{-3}\\) 数值范围与FP32相同，精度与FP16相同 TF32(TensorFloat)是NV在Ampere架构GPU上推出的用于 TensorCore的格式，在A100 TF32 TensorCore的运算速度是V100 FP32 CUDACore的8倍 BF16 16 1 8 7 \\(-3.39 \\times 10^{38}\\) ~ \\(3.39 \\times 10^{38}\\) \\(10^{-2}\\) 数值范围与FP32一致，远大于FP16，但精度略低于FP16 BF16(brain floating point 16)由Google Brain提出，适合大模型训练，目前只适配于Ampere架构的GPU（A100） Int32 32 1 31 0 \\(-2.15 \\times 10^{9}\\) ~ \\(2.15 \\times 10^{9}\\) 1 Int16 16 1 15 0 -32768 ~ 32767 1 Int8 8 1 7 0 -128 ~ 127 1 使用pytorch验证数据：\nimport torch print(torch.finfo(torch.float32)) # finfo(resolution=1e-06, min=-3.40282e+38, max=3.40282e+38, eps=1.19209e-07, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=float32) print(torch.finfo(torch.float16)) # finfo(resolution=0.001, min=-65504, max=65504, eps=0.000976562, smallest_normal=6.10352e-05, tiny=6.10352e-05, dtype=float16) print(torch.finfo(torch.bfloat16)) # finfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=bfloat16) print(torch.iinfo(torch.int32)) # iinfo(min=-2.14748e+09, max=2.14748e+09, dtype=int32) print(torch.iinfo(torch.int16)) # iinfo(min=-32768, max=32767, dtype=int16) print(torch.iinfo(torch.int8)) # iinfo(min=-128, max=127, dtype=int8) BF16 的优势：\n设计思想是在不改变内存占用的情况下，用 1 / 10 倍的精度换取了 \\(10^{34}\\) 倍的数值范围，只用 2 bytes 的内存，但数值范围与 4 bytes 的 FP32 相同 BF16 比 FP16 更适合深度学习。对于 DL，数值范围的作用远高于精度。因为在梯度下降时(\\(w = w - \\Delta w = w - grad * learningRate\\))，grad 和学习率通常较小，因此必须使用能够表达较大范围的数据类型。使用 FP16时往往会出现 underflow： 当数值小于 \\(− 6.55 × 10^4\\) 时会被截断为 0 ，导致梯度无法更新。 BF16 与 FP32 的相互转换更容易。BF16 基本上可以看作成一个截断版的 FP32, 两者之间的转换是非常直接，相比于 FP16，BF16的电路实现更简单，可有效降低电路面积。 DataType Use Case # 不用任务使用不同的数据类型 分类任务对数据类型比较不敏感，FP16 和 INT8 获得的精度差不多，一般可采用INT8 NLP 任务以FP16为主 目标检测对数据类型比较敏感，以FP16为主 训练和推理的不同 FP32 往往只是作为精度基线 (baseline)，比如要求使用 FP16 获得的精度达到 FP32 baseline 的 99% 以上。但 DL 训练，尤其是 LLM 训练，通常不会使用 FP32 训练往往使用 FP16, BF16 和 TF32，降低内存占用、训练时间和资源需求 CV 推理以INT8为主， NLP 推理以FP16为主 DataType Conversion # GPU中的数据类型转换，FP32 转换为 FP16\n强制把float转为unsigned long 尾数位：截取后23位尾数，右移13位，剩余10位 符号位：直接右移16位 指数位：截取指数的8位，先右移13位(左边多出3位不管了)，之前是0~255表示-127~128, 调整之后变成0~31表示-15~16，因此要减去 （127-15=112再左移10位） typedef unsigned short half; half nvFloat2Half(float m) { unsigned long m2 = *(unsigned long*)(\u0026amp;m); unsigned short t = ((m2 \u0026amp; 0x007fffff) \u0026gt;\u0026gt; 13) | ((m2 \u0026amp; 0x80000000) \u0026gt;\u0026gt; 16) | (((m2 \u0026amp; 0x7f800000) \u0026gt;\u0026gt; 13) - (112 \u0026lt;\u0026lt; 10)); if(m2 \u0026amp; 0x1000) t++;// 四舍五入(尾数被截掉部分的最高位为1, 则尾数剩余部分+1) half h = *(half*)(\u0026amp;t);// 强制转为half return h ; } FP16 转换为 FP32\nfloat nvHalf2Float(half n) { unsigned short frac = (n \u0026amp; 0x3ff) | 0x400; int exp = ((n \u0026amp; 0x7c00) \u0026gt;\u0026gt; 10) - 25; float m; if(frac == 0 \u0026amp;\u0026amp; exp == 0x1f) m = INFINITY; else if (frac || exp) m = frac * pow(2, exp); else m = 0; return (n \u0026amp; 0x8000) ? -m : m; } Quantization in LLM # LLMs的巨大模型规模和边缘设备的限制（主要是内存大小和带宽）给部署带来了显著挑战。\n模型量化是指以较低的推理精度损失将连续取值（通常为float32或者大量可能的离散值）的浮点型权重近似为有限多个离散值（通常为int8）的过程。\n通过以更少的位数表示浮点数据，可以有效降低 LLMs 对内存和带宽的需求，在一些低精度运算较快的处理器上可以增加推理速度。\n量化的对象：\n权重，最常规的量化对象 激活，activation 往往是占内存使用的大头，量化 activation 不仅可以减少内存占用，结合 weight 量化可以充分利用整数计算获得性能提升 KV Cache，有助于提高长序列生成的吞吐量 梯度 LLM中的量化示例：\nimport numpy as np np.random.seed(0) # 生成维度为（5,5）的FP16格式的矩阵m1 m1 = np.random.rand(5, 5).astype(np.float16) print(m1) # 求scale oldMax = np.max(m1) scale = 127/oldMax print(oldMax,scale) # 量化为m2 m2 = np.round(scale * m1).astype(np.int8) print(m2) # 反量化为m3 m3 = (m2/scale).astype(np.float16) print(m3) 现有 FP16 格式的权重矩阵m1:\n[[0.549 0.7153 0.6025 0.545 0.4236 ] [0.646 0.4375 0.8916 0.964 0.3835 ] [0.7915 0.529 0.568 0.926 0.07104] [0.08716 0.02022 0.8325 0.7783 0.87 ] [0.9785 0.7993 0.4614 0.781 0.1183 ]] 量化为 INT8 格式的步骤：\n旧范围： FP16 格式中的最大权重值 - FP16 格式中的最小权重值 = 0.9785–0.07104 新范围： INT8 包含从 -128 到 127 的数字。因此，范围 = 127-(-128) 缩放比例(Scale)： 新范围中的最大值 / 旧范围中的最大值 = 127 / 0.9785 = 129.7884231536926 量化值： Scale * 原始值, 四舍五入 m2:\n[[ 71 93 78 71 55] [ 84 57 116 125 50] [103 69 74 120 9] [ 11 3 108 101 113] [127 104 60 101 15]] 反量化： 量化值 / Scale m3:\n[[0.547 0.7163 0.601 0.547 0.4238 ] [0.647 0.4392 0.8936 0.963 0.3853 ] [0.7935 0.5317 0.5703 0.925 0.06934] [0.0848 0.02312 0.832 0.7783 0.8706 ] [0.9785 0.8013 0.4624 0.7783 0.1156 ]] 量化往往以 group 为单位，group 的划分对旧范围有影响\n由于量化时产生了四舍五入和误差，导致反量化回到 FP16 格式后与原始数据略有误差\nAsymmetry and symmetry quantization # 量化的形式：\n根据原始数据范围是否均匀，可以将量化方法分为线性量化和非线性量化。DL中的权重和激活值通常是不均匀的，因此理论上使用非线性量化导致的精度损失更小，但在实际推理中非线性量化的计算复杂度较高，通常使用线性量化。\n线性量化的原理。假设r表示量化前的浮点数，量化后的整数q可以表示为：\n$q=clip(round(r/s+z),q_{min},q_{max})$\n其中，$round()$和$clip()$ 分别表示取整和截断操作，$q_{min}$和$q_{max}$表示量化后的上下限，$s$是数据量画的间隔，$z$是表示数据偏移的偏置，当z=0时称为对称（Symmetric）量化，不为0时称为非对称（Asymmetric）量化\n对称量化可以避免量化算子在推理中计算z相关的部分，降低推理时的计算复杂度；非对称量化可以根据实际数据的分布确定最小值和最小值，可以更加充分的利用量化数据信息，使得量化导致的损失更低\n量化的粒度：\n根据量化参数sss和zzz的共享范围（即量化粒度），量化方法可以分为逐层量化（per-tensor）、逐通道（per-token \u0026amp; per-channel 或者 vector-wise quantization ）量化和逐组量化（per-group、Group-wise）。\nReference\nint8/fp16/bf16/tf32在AI芯片中什么作用？ fp16和fp32神经网络混合精度训练 大模型量化概述 ","date":"2024-04-28","externalUrl":null,"permalink":"/posts/llm/quantize_datatype/","section":"Blogs","summary":"Data in Memory # Float 和 Double 类型的数据在内存中以二进制","title":"DataType in AI","type":"posts"},{"content":" Introduction # LLMs的巨大模型规模和边缘设备的限制（主要是内存大小和带宽）给部署带来了显著挑战。\n模型量化是指以较低的推理精度损失将连续取值（通常为float32或者大量可能的离散值）的浮点型权重近似为有限多个离散值（通常为int8）的过程。\n通过以更少的位数表示浮点数据，可以有效降低 LLMs 对内存和带宽的需求，在一些低精度运算较快的处理器上可以增加推理速度。\nQuantize Object # 权重，最常规的量化对象 激活，activation 往往是占内存使用的大头，量化 activation 不仅可以减少内存占用，结合 weight 量化可以充分利用整数计算获得性能提升 KV Cache，有助于提高长序列生成的吞吐量 梯度 LLM Quantization Example # import numpy as np np.random.seed(0) # 生成维度为（5,5）的FP16格式的矩阵m1 m1 = np.random.rand(5, 5).astype(np.float16) print(m1) # 求scale oldMax = np.max(m1) scale = 127/oldMax print(oldMax,scale) # 量化为m2 m2 = np.round(scale * m1).astype(np.int8) print(m2) # 反量化为m3 m3 = (m2/scale).astype(np.float16) print(m3) 现有 FP16 格式的权重矩阵m1:\n[[0.549 0.7153 0.6025 0.545 0.4236 ] [0.646 0.4375 0.8916 0.964 0.3835 ] [0.7915 0.529 0.568 0.926 0.07104] [0.08716 0.02022 0.8325 0.7783 0.87 ] [0.9785 0.7993 0.4614 0.781 0.1183 ]] 量化为 INT8 格式的步骤：\n旧范围： FP16 格式中的最大权重值 - FP16 格式中的最小权重值 = 0.9785–0.07104 新范围： INT8 包含从 -128 到 127 的数字。因此，范围 = 127-(-128) 缩放比例(Scale)： 新范围中的最大值 / 旧范围中的最大值 = 127 / 0.9785 = 129.7884231536926 量化值： Scale * 原始值, 四舍五入 m2:\n[[ 71 93 78 71 55] [ 84 57 116 125 50] [103 69 74 120 9] [ 11 3 108 101 113] [127 104 60 101 15]] 反量化： 量化值 / Scale m3:\n[[0.547 0.7163 0.601 0.547 0.4238 ] [0.647 0.4392 0.8936 0.963 0.3853 ] [0.7935 0.5317 0.5703 0.925 0.06934] [0.0848 0.02312 0.832 0.7783 0.8706 ] [0.9785 0.8013 0.4624 0.7783 0.1156 ]] 量化往往以 group 为单位，group 的划分对旧范围有影响\n由于量化时产生了四舍五入和误差，导致反量化回到 FP16 格式后与原始数据略有误差\nAsymmetry and symmetry quantization # 量化的形式：\n根据原始数据范围是否均匀，可以将量化方法分为线性量化和非线性量化。DL中的权重和激活值通常是不均匀的，因此理论上使用非线性量化导致的精度损失更小，但在实际推理中非线性量化的计算复杂度较高，通常使用线性量化。\n线性量化的原理。假设 r 表示量化前的浮点数，量化后的整数 q 可以表示为：\n$q=clip(round(r/s+z),q_{min},q_{max})$\n其中，$round()$和$clip()$ 分别表示取整和截断操作，$q_{min}$和$q_{max}$表示量化后的上下限，$s$是数据量化的间隔，$z$是表示数据偏移的偏置，当z=0时称为对称（Symmetric）量化，不为0时称为非对称（Asymmetric）量化\n对称量化可以避免量化算子在推理中计算z相关的部分，降低推理时的计算复杂度；非对称量化可以根据实际数据的分布确定最小值和最小值，可以更加充分的利用量化数据信息，使得量化导致的损失更低\n量化的粒度 # 根据量化参数 s 和 z 的共享范围（即量化粒度），量化方法的粒度可以分为：\n逐层量化(per-tensor)，是范围最大的粒度，以一层网络为量化单位，每层网络一组量化参数 逐通道(per-token \u0026amp; per-channel 或 vector-wise quantization) 量化，以一层网络的每个量化通道为单位，每个通道单独使用一组量化参数 per-token：对激活来说，每行对应一个量化系数 per-channel：对权重来说，每列对应一个量化系数 逐组量化(per-group、Group-wise)，粒度处于 per-tensor 和 per-channel 之间，每个group（如 K 行或 K 列）使用一组 s 和 z 权重和激活可以选择不同的量化粒度。譬如权重用 per-tensor，激活用 per-token。并且对于激活分动态量化与静态量化\n量化方法的分类 # 根据应用量化压缩模型的阶段，可以将模型量化分为：\n量化感知训练（Quantization Aware Training, QAT）：在模型训练过程中加入伪量化算子，通过训练时统计输入输出的数据范围可以提升量化后模型的精度，适用于对模型精度要求较高的场景；其量化目标无缝地集成到模型的训练过程中。这种方法使LLM在训练过程中适应低精度表示，增强其处理由量化引起的精度损失的能力。这种适应旨在量化过程之后保持更高性能 LLM-QAT 量化感知微调（Quantization-Aware Fine-tuning，QAF）：在微调过程中对LLM进行量化。主要目标是确保经过微调的LLM在量化为较低位宽后仍保持性能。通过将量化感知整合到微调中，以在模型压缩和保持性能之间取得平衡 QLoRA 训练后量化（Post Training Quantization, PTQ）：在LLM训练完成后对其参数进行量化，只需要少量校准数据，适用于追求高易用性和缺乏训练资源的场景。主要目标是减少LLM的存储和计算复杂性，而无需对LLM架构进行修改或进行重新训练。PTQ的主要优势在于其简单性和高效性。但PTQ可能会在量化过程中引入一定程度的精度损失。 LLM.int8() GPTQ AWQ LLM 量化的挑战 # 量化激活比量化权重更难（LLM.int8()表明，使用 INT8 甚至 INT4 量化 LLM 的权重不会降低准确性） 异常值让量化激活更困难（激活的异常值比大多数激活值大 100 倍，使用 INT8 量化，大多数值将被清零） 异常值持续存在于固定的通道中（固定通道存在异常值，并且异常值通道值较大） Reference\nint8/fp16/bf16/tf32在AI芯片中什么作用？ fp16和fp32神经网络混合精度训练 大模型量化概述 ","date":"2024-04-28","externalUrl":null,"permalink":"/posts/llm/quantize_intro/","section":"Blogs","summary":"Introduction # LLMs的巨大模型规模和边缘设备的限","title":"Quantization Introduction","type":"posts"},{"content":"This is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing. 🚀\nYou can also use these content pages to define Hugo metadata like titles and descriptions that will be used for SEO and other purposes.\n","date":"0001-01-01","externalUrl":null,"permalink":"/tags/advanced/","section":"Tags","summary":"This is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing.","title":"Advanced","type":"tags"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/tags/_index.zh-cn/","section":"Tags","summary":"","title":"标签","type":"tags"},{"content":"欢迎来到我的博客!\n","date":"0001-01-01","externalUrl":null,"permalink":"/_index.zh-cn/","section":"Welcome to My Blog","summary":"欢迎来到我的博客!","title":"欢迎来到我的博客","type":"page"},{"content":"这是高级标记。类似其他 Blowfish 中的其他列表页面，你可以在分类列表页添加自定义内容，这部分内容会显示在顶部。🚀\n你也可以用这些内容来定义 Hugo 的元数据，比如标题和描述。这些内容可以被用来增强 SEO 或其他目的。\n","date":"0001-01-01","externalUrl":null,"permalink":"/tags/advanced/_index.zh-cn/","section":"Tags","summary":"这是高级标记。类似其他 Blowfish 中的其他列表页面","title":"高级","type":"tags"}]