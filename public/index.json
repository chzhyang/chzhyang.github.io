
[{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/posts/","section":"Blogs","summary":"","title":"Blogs","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/tags/cuda/","section":"Tags","summary":"","title":"CUDA","type":"tags"},{"content":"[Todo]\n","date":"2024-05-24","externalUrl":null,"permalink":"/posts/cuda/cuda_optimize/","section":"Blogs","summary":"[Todo]","title":"CUDA Optimization","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/series/cuda-parallel-programming/","section":"Series","summary":"","title":"CUDA Parallel Programming","type":"series"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/posts/cuda/","section":"Blogs","summary":"","title":"CUDA Parallel Programming","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"Hi, welcome to my blog.\n","date":"2024-05-24","externalUrl":null,"permalink":"/","section":"Welcome to My Blog","summary":"Hi, welcome to my blog.","title":"Welcome to My Blog","type":"page"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/attention/","section":"Tags","summary":"","title":"Attention","type":"tags"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/series/attention-and-optimization/","section":"Series","summary":"","title":"Attention and Optimization","type":"series"},{"content":"[WIP]\nFlashAttention V2:\n在V1的基础上减少了非矩阵乘法运算的FLOPs。 通过并行化和任务分配优化提高了计算速度和GPU利用率，性能提升了2-3倍。 Flash-Decoding借鉴了FlashAttention的优点，将并行化维度扩展到keys/values序列长度，提高了推理速度。 Flash-Decoding几乎不用额外存储大量数据到全局内存中，减少了内存开销。 Flash-Decoding++通过异步softmax和统一最大值、flat GEMM优化和双缓冲、启发式数据流和硬件资源适应等方法进一步提高了LLM推理的性能。 ","date":"2024-05-23","externalUrl":null,"permalink":"/posts/llm/flash_attention_2/","section":"Blogs","summary":"[WIP] FlashAttention V2: 在V1的基础上减少了非矩阵乘法运算","title":"Flash Attention V2","type":"posts"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/posts/llm/","section":"Blogs","summary":"","title":"LLM","type":"posts"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/nlp/","section":"Tags","summary":"","title":"NLP","type":"tags"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/transformer/","section":"Tags","summary":"","title":"Transformer","type":"tags"},{"content":"","date":"2024-05-21","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"2024-05-21","externalUrl":null,"permalink":"/tags/pytorch/","section":"Tags","summary":"","title":"Pytorch","type":"tags"},{"content":"","date":"2024-05-21","externalUrl":null,"permalink":"/series/pytorch/","section":"Series","summary":"","title":"PyTorch","type":"series"},{"content":"","date":"2024-05-21","externalUrl":null,"permalink":"/posts/pytorch/","section":"Blogs","summary":"","title":"PyTorch","type":"posts"},{"content":" PyTorch 代码结构 # PyTroch 主要由C10、ATen、torch三大部分组成：\ntorch/ 下包含 import 和使用的 Python 模块 torch/csrc/ 包含了 PyTorch 前端的 C++ 代码及C++前端代码。具体而言，它包含了 Python 和 C++ 之间转换的binding代码， autograd 引擎和 JIT 编译器等。 c10(Caffe Tensor Library), 包含 PyTorch 的核心抽象，存放最基础的Tensor库代码，包括 Tensor 和 Storage 数据结构的实际实现，可以运行在服务端和移动端。 最具代表性的class是 TensorImpl ，实现了Tensor的最基础框架。继承者和使用者有： Variable的Variable::Impl SparseTensorImpl detail::make_tensor(storage_impl, CUDATensorId(), false) Tensor(c10::intrusive_ptr\u0026lt;TensorImpl, UndefinedTensorImpl\u0026gt; tensor_impl) c10::make_intrusive\u0026lt;at::TensorImpl, at::UndefinedTensorImpl\u0026gt; ATen(A Tensor library for C++11)，包含声明和定义 Tensor 运算相关逻辑的代码，是实现张量运算的 C++ 库，kernel代码大多在这里 包含 C++ 实现的native算子和 C 实现的legacy算子(TH, THC, THNN, THCUNN) . aten/src/ATen/gen.py 用来动态生成一些ATen相关的代码 PyTroch 的编译过程 # 入口 setup.py； 提前检查依赖项； 使用 cmake 生成 Makefile Make: 产生中间源文件 Make: 编译三方库 Make: 生成静态库、动态库、可执行文件 Make: Copy文件到合适路径 setuptools, build_py setuptools, build_ext setuptools, install_lib PyTorch 工作流和计算图 # PyTorch 1.0 整体工作流：\n使用 imperative / eager 的范式，每一行代码都构建一个图作为完整计算图的一部分。即使完整的计算图还没有完成构建，也可以独立执行这些作为组件的小计算图，这种动态计算图被称为define-by-run Eager 模式适合块做原型、实验、debug，Script 模式(torch.jit)适合做优化与部署 动态图 # 假设PyTorch的autograd系统是一个 graph，那么每个 Function 实例就是 graph 中的节点，各个 Function 实例之间通过 Edge 连接。Edge 是个 struct，(Function, input_nr) 组合可以代表一个 edge\nstruct Edge { ... std::shared_ptr\u0026lt;Function\u0026gt; function; uint32_t input_nr; }; Function 的成员变量 next_edges_ 就是一组 Edge 实例，代表当前Function实例的返回值要输出到哪个Function\nFunction 的 input, ouput 都是 Variable实例，因此，当一个 graph 被执行时，Variable 实例就在这些 edge 之间来流动，传输信息\nFunction 的成员变量 sequence number，随着Function实例的不断构建而单调增长\nJIT # Code/AST -\u0026gt; Parsing-\u0026gt; Checking -\u0026gt; Optimization -\u0026gt; Translation -\u0026gt; Execution\nJIT 主要会输入代码或 Python 的抽象句法树（AST），其中 AST 会用树结构表征 Python 源代码的句法结构。 Parsing可能是解析句法结构和计算图，然后语法检测接连着代码优化过程，最后只要编译并执行就可以 优化计算图，如展开循环、指令转换等 执行，与 Python 解释器可以执行代码一样，PyTorch JIT 过程中也有一个解释器执行中间表征指令 PyTorch 从 Python 代码到 kernel # PyTorch 从 Python 代码到 kernel 的中间过程十分复杂, 在进入内核之前，所有代码都是自动生成的\n假设调用 torch.add()，流程如下：\nPython 域转换到 C++ 域（Python 参数解析） 处理 VariableType dispatch 处理 DeviceType/布局 dispatch 执行kernel(native kernel 或 TH kernel) ATen 动态生成的代码 # Type继承体系，包含头文件和源文件 Type继承体系是联系 Tensor op 与 legacy 的 TH 或 native kernel 的纽带 Type继承体系维护了2/3级分发机制 Declarations.yaml，会被Torch模块动态生成代码调用 生成 Tensor 类 生成Type家族注册初始化的代码 生成 legacy 的 TH/THC 的kernel声明 生成 native kernel 的声明 PyTroch Tensor # #在python中定义了Parameter类 class Parameter(torch.Tensor) #在python中定义了torch.Tensor类 class Tensor(torch._C._TensorBase) #在C++中定义了Variable类 struct TORCH_API Variable : public at::Tensor //PyObject* Py_InitModule(char *name, PyMethodDef *methods) //创建torch._C Py_InitModule(\u0026#34;torch._C\u0026#34;, methods.data()） //创建 torch._C._TensorBase PyModule_AddObject(module, \u0026#34;_TensorBase\u0026#34;, (PyObject *)\u0026amp;THPVariableType); Tensor运算 Dispatch 机制中的 Type 继承体系 # Type类派生出了TypeExtendedInterface，TypeExtendedInterface又派生了TypeDefault。TypeDefault又派生了CUDATypeDefault、CPUTypeDefault、VariableType（实现了autograd）、UndefinedType等。其中，根据 density 和 scaler type 的不同：\nCUDATypeDefault派生了：\nCUDAIntType CUDAShortType SparseCUDACharType CUDADoubleType CUDAByteType CUDACharType SparseCUDAByteType CUDAFloatType SparseCUDALongType CUDALongType CUDAHalfType SparseCUDAShortType SparseCUDADoubleType SparseCUDAIntType SparseCUDAFloatType CPUTypeDefault派生了：\nSparseCPUShortType CPUFloatType CPUHalfType CPUDoubleType CPUByteType SparseCPUFloatType SparseCPUIntType SparseCPUDoubleType CPUCharType SparseCPUByteType CPUIntType CPULongType SparseCPULongType SparseCPUCharType CPUShortType Type继承体系的作用\nPyTorch Kernel 组成 # Error checking, TORCH CHECK Output allocation Dtype dispatch Parallelization Data access 未完待续\u0026hellip;\n","date":"2024-05-21","externalUrl":null,"permalink":"/posts/pytorch/pytorch/","section":"Blogs","summary":"PyTorch 代码结构 # PyTroch 主要由C10、ATen、t","title":"PyTorch Architecture","type":"posts"},{"content":"","date":"2024-05-20","externalUrl":null,"permalink":"/tags/paged-attention/","section":"Tags","summary":"","title":"Paged Attention","type":"tags"},{"content":" vLLM # vLLM是吞吐性能卓越的大模型推理框架，PagedAttention是vLLM最大的创新点： Efficient Memory Management for Large Language Model Serving with PagedAttention\nvLLM中的attention计算，在推理的prefill阶段, 使用第三方库xformers的优化实现，decoding阶段使用 CUDA kernel 实现(csrc/attention/attention_kernels.cu，大约800多行)。\nAttention计算时使用页式管理 KV Cache 来提高内存利用率，进而提高吞吐量。\nPaged Attention(PA) # vLLM中有两个版本的 PA，其中：\nV1 源于 FasterTransformers 的 MHA，适用于 len(seq) \u0026lt; 8192 或 num_seqs * num_heads \u0026gt; 512 的情况。 V2 参考了 Flash Decoding方式，对 sequence 的维度进行切分来增加并行粒度 Paged Attention V1 # Block table in PA\n一个 req 中包含多个 seq 时，可以共享blocks\nPaged Attention V1 CUDA Kernel(vLLM) # csrc/attention/attention_kernels.cu\nDispatch逻辑：\nCALL_KERNEL_LAUNCHER_BLOCK_SIZE 根据存储的kv blocksize进行派发，分别是 8， 16， 32 LAUNCH_ATTENTION_KERNEL 根据注意力头大小HEADSIZE静态派发 并行任务的划分：\ndim3 grid(num_heads, num_seqs， 1) dim3 block(NUM_THREADS), 线程数是128，每个 block 负责完成 output 矩阵一行（head_size个元素）结果的 attention 计算 block 的线程划分为若干个 Warp, 每个 Warp 的32个线程划分为 blk_size 个 thread group Kernel 输入参数\nout[num_seqs, num_heads, head_size] q[num_seqs, num_heads, head_size] k_cache[num_blocks, num_kv_heads, head_size/x, block_size, x] # x表示一个向量化的大小，如float16 -\u0026gt; 16 / sizeof(float16) = 8 v_cache[num_blocks, num_kv_heads, head_size, block_size] head_mapping[num_heads] # 使用MQA, GQA时的kv_head block_tables[num_seqs, max_num_blocks_per_seq] # 维护各个Q对应KVCache的哪些block context_lens[num_seqs] # 用于变长 num_head： Q 的 head 数 num_kv_heads：K, V 的 head 数，MHA 的 num_kv_heads = num_head，GQA、MQA 的 num_kv_heads \u0026lt; num_head blk_size # block_size，每个page block存储的元素数量，每个page存(blk_size, num_head，head_size)个K、V的元素\nKernel 的常量定义：\nTHREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1) 通过WARPSIZE / BLOCKSIZE 得到一个thread_group大小。注意这里的BLOCKSIZE不是cuda blocksize，而是一个kv block的大小(默认值16) NUM_TOKENS_PER_THREAD_GROUP = (BLOCK_SIZE + WARP_SIZE - 1) / - WARP_SIZE 表示每个thread_group处理多少个token NUM_WARPS 表示一个threadblock有多少个warp VEC_SIZE 表示向量化大小，保证每个thread_group一次性获取16bytes，MAX(16 / (THREAD_GROUP_SIZE * sizeof(scalar_t)), 1) NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE 表示每个thread要负责多少个数据计算 NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE 表示每个thread负责的数据经过向量化后，一共有多少个vec V_VEC_SIZE = MIN(16 / sizeof(scalar_t), BLOCK_SIZE) 每个thread一次性读取16bytes NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE。对于v_cache[head_size, block_size]，表示一行需要几个V_VEC NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW 表示一个warp可以处理多少行 NUM_ROWS_PER_THREAD 表示每个thread需要负责多少行 Kernel 代码逻辑：\n（1）循环从显存读取\\(Q\\)到 shared memory：\n迭代读取，每 CUDA block 负责读取\\(Q\\)的一行（head_size 个元素）存入 shared memory。其中，block 的每个 Warp 负责读取 16blk_size 字节的 Q，即每个 thread group 会读取16字节的 Q，16blk_size 字节的 Q 对应 sequence 的一个 head。\nconst int thread_group_idx = thread_idx / THREAD_GROUP_SIZE; const int thread_group_offset = thread_idx % THREAD_GROUP_SIZE; // Load the query to registers. // Each thread in a thread group has a different part of the query. // For example, if the the thread group size is 4, then the first thread in // the group has 0, 4, 8, ... th vectors of the query, and the second thread // has 1, 5, 9, ... th vectors of the query, and so on. NOTE(woosuk): Because // q is split from a qkv tensor, it may not be contiguous. const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE; __shared__ Q_vec q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD]; #pragma unroll for (int i = thread_group_idx; i \u0026lt; NUM_VECS_PER_THREAD; i += NUM_THREAD_GROUPS) { const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE; q_vecs[thread_group_offset][i] = *reinterpret_cast\u0026lt;const Q_vec*\u0026gt;(q_ptr + vec_idx * VEC_SIZE); } __syncthreads(); （2）循环从显存读取\\(K\\)到 register，并计算QK：\n每个 seq 包含 cxt_length * num_kv_heads * head_size 个元素 每个 CUDA block 负责计算一个 seq 的一个 head 的 \\(QK^T\\)， 只需要读取 ctx_length * head_size 个 K 的元素 因为页式内存管理，K 在 ctx_length 维度的存储不连续，以 blk_size 个 token 为粒度分布在不同的内存地址，所以需要根据 Q 的 head_idx 和 seq_idx 访问 block_table 找到 K 的 physical_block_num K Cache的布局为 [num_blocks, num_kv_heads, head_size/x, block_size, x]， 目的是优化写入 shared memory。Q和K的同一行元素被读入寄存器并进行点乘运算后，结果要写入shared memory。如果一个 Warp 中所有线程都计算 Q、K 同一行数据，会导致写入 shared memory 的同一个位置，这将造成 warp 内不同线程顺序地写入。所以 warp 的线程最好计算 Q和K 的不同行数据。在设计 K 布局时，将 block_size 放在比 head_size 更低的维度。由于warp size大于block_size，我们需要将head_size拆分为head_size/x和x两个维度，借x到最低维度，以确保每个线程读入的数据量和计算量都足够大。最后，每个线程组派一个线程去写入shared memory，这样一个warp有blk_size个线程并行写入shared memory，从而增加了shared memory的访问带宽。这种设计策略是为了实现高效的并行计算和内存访问，以提高整体的计算性能。 读取 K 需要一个循环，循环中每个CUDA block中的所有 warp 依次访问num_blocks 个 page block。每次迭代： 每个 warp 负责访问连续的 blk_size 个 KCache 的行数据（blk_size * head_size个元素）。每个 thread group 负责访问 KCache 的一行，将head_size 个元素读入寄存器 寄存器中的Q和K元素进行点乘，结果写入shared memory。一个 CUDA block 的 shared memory 存储了一行 QK^T 的结果，共 ctx_length 个元素 CUDA block 对 shared memory 中元素进行 max，sum 方式 reduction，然后计算得到 softmax 的结果 代码步骤：\n每个warp负责计算一个block key，而每个block key shape为 [block_size, num_head, head_size] 每个thread_group取一个key，即num_head个元素，计算QK dot 只有thread_group的第一个thread负责将QK结果写入shared memory // 每个warp负责 blocksize * headsize个元素 for (int block_idx = warp_idx; block_idx \u0026lt; num_blocks; block_idx += NUM_WARPS) { // TODO(Zhengzekang) const int physical_block_number = block_table[block_idx]; // ... K_vec k_vecs[NUM_VECS_PER_THREAD]; // 遍历每个thread_group处理多少个token for (int i = 0; i \u0026lt; NUM_TOKENS_PER_THREAD_GROUP; i++) { const int physical_block_offset = (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE; const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset; K_vec k_vecs[NUM_VECS_PER_THREAD]; // 遍历每个thread需要处理多少个VEC for (int j = 0; j \u0026lt; NUM_VECS_PER_THREAD; j++) { // vectorized取到key k_vecs[j] = xxxx; } // 计算QKdot，里面包含了一个thread_groupsize的WarpReduceSum， float qk = scale * Qk_dot\u0026lt;scalar_t, THREAD_GROUP_SIZE\u0026gt;::dot(q_vecs, k_vecs); // 只有thread_group的第一个thread负责将QK结果写入shared memory // 并且维护一个qk_max，用于后续softmax if (thread_group_offset == 0) { // Store the partial reductions to shared memory. // NOTE(woosuk): It is required to zero out the masked logits. const bool mask = token_idx \u0026gt;= context_len; logits[token_idx - start_token_idx] = mask ? 0.f : qk; // Update the max value. qk_max = mask ? qk_max : fmaxf(qk_max, qk); } } } 此时各个thread_group已经完成了自己的qk_dot操作，并且都维护了qk_max。下面就需要和其他thread_group做warp shuffle操作，得到一个warp内的qk max值。\n（3）从显存读取\\(V\\)到 register：\n和KCache一样，CUDA thread block依次访问num_blk个物理块到寄存器，每个warp负责blk_size个token的page内存，page的真实物理地址同样需要进行索引。不过这里不需要以thread group为单位访问16字节，而是每个thread访问16字节的元素。访问完就可以与shared memory的softmax(QK^T)中间结果对应位置16字节的数据进行点乘，得到一个float结果，写到output对应位置中。\n为什么 VCache 的 layout 是 [num_blocks, num_kv_heads, head_size, block_size]，和 KCache layout 不一样？ 因为 V 要去做点乘的对象在shared memory，只需要读，不涉及并行写。\nPA V1 和 Flash Attention 的区别 # 并行任务的划分方式不同\nFlashAttention 用了两层循环，每次写一个 Tile 的 output tensor，而 PA 只有一层循环，每次写一行 output tensor。因为每次迭代都有整行的 QK^T 中间结果，不需要online softmax PA V1 设计的 KCache layout 充分利用了 shared memory 写带宽 PA V1 的缺陷 # 不足：\n不适合 seq 很长的情况，因为没有沿着 ctx_length 或者 batch 维度做切分 和MHA相比，MQA和GAQ没有减少对KV Cache的读写次数。读K、V Cache时候只是做了一个head_idx的转换，会重复从显存读相同的head 未完待续\u0026hellip;\nReference:\nvllm Efficient Memory Management for Large Language Model Serving with PagedAttention ","date":"2024-05-20","externalUrl":null,"permalink":"/posts/llm/paged_attention_v1/","section":"Blogs","summary":"vLLM # vLLM是吞吐性能卓越的大模型推理框","title":"Paged Attention V1(vLLM)","type":"posts"},{"content":"","date":"2024-05-20","externalUrl":null,"permalink":"/tags/vllm/","section":"Tags","summary":"","title":"Vllm","type":"tags"},{"content":"","date":"2024-05-20","externalUrl":null,"permalink":"/series/vllm/","section":"Series","summary":"","title":"VLLM","type":"series"},{"content":"","date":"2024-05-20","externalUrl":null,"permalink":"/tags/conv/","section":"Tags","summary":"","title":"Conv","type":"tags"},{"content":" CUDA Conv # 在 PyTorch 上实现Conv很简单\nimport torch from torch.nn.functional import conv2d device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) import time width = 1000 height = 1000 img =torch.randn([width,height]) img = img.to(device) kernel = torch.tensor([[-1.0, 0.0, 1.0], [-1.0, 0.0, 1.0], [-1.0, 0.0, 1.0]]) img = torch.reshape(img, (1, 1, width, height)) kernel = torch.reshape(kernel, (1, 1, 3, 3)) kernel = kernel.to(device) start = time.perf_counter() output = F.conv2d(img, kernel, stride=1).to(device) end = time.perf_counter() print(f\u0026#39;total_cost: {end-start} ms\u0026#39;) print(f\u0026#39;output_size: {output.shape}\u0026#39;) print(f\u0026#39;output_tensor: {output}\u0026#39;) 用CUDA实现Conv，Steps:\nCheck Get thread info Memory allocation Copy data to device Call conv kernel Copy data to host Free memory //file: conv.cu #include\u0026lt;stdint.h\u0026gt; #include\u0026lt;cuda.h\u0026gt; #include \u0026#34;cuda_runtime.h\u0026#34; #include \u0026#34;device_launch_parameters.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;math.h\u0026gt; const int NUM_REPEATS = 10; #define CHECK(call) \\ do \\ { \\ const cudaError_t error_code = call; \\ if (error_code != cudaSuccess) \\ { \\ printf(\u0026#34;CUDA Error:\\n\u0026#34;); \\ printf(\u0026#34; File: %s\\n\u0026#34;, __FILE__); \\ printf(\u0026#34; Line: %d\\n\u0026#34;, __LINE__); \\ printf(\u0026#34; Error code: %d\\n\u0026#34;, error_code); \\ printf(\u0026#34; Error text: %s\\n\u0026#34;, \\ cudaGetErrorString(error_code)); \\ exit(1); \\ } \\ } while (0) static void HandleError(cudaError_t err, const char* file, int line) { if (err != cudaSuccess) { printf(\u0026#34;%s in %s at line %d\\n\u0026#34;, cudaGetErrorString(err), file, line); exit(EXIT_FAILURE); } } #define HANDLE_ERROR(err) (HandleError(err, __FILE__, __LINE__)) int getThreadNum() { cudaDeviceProp prop; int count; CHECK(cudaGetDeviceCount(\u0026amp;count)); printf(\u0026#34;GPU num: %d\\n\u0026#34;, count); CHECK(cudaGetDeviceProperties(\u0026amp;prop, 0)); printf(\u0026#34;Max thread num per block: %d\\n\u0026#34;, prop.maxThreadsPerBlock); printf(\u0026#34;Max grid dim: %d, %d, %d)\\n\u0026#34;, prop.maxGridSize[0], prop.maxGridSize[1], prop.maxGridSize[2]); return prop.maxThreadsPerBlock; } __global__ void conv(float* img, float* kernel, float* result, int width, int height, int kernelSize) { int ti = threadIdx.x; int bi = blockIdx.x; int id = (bi * blockDim.x + ti); if (id \u0026gt;= width * height) { return; } int row = id / width; int col = id % width; for (int i = 0; i \u0026lt; kernelSize; ++i) { for (int j = 0; j \u0026lt; kernelSize; ++j) { float imgValue = 0; int curRow = row - kernelSize / 2 + i; int curCol = col - kernelSize / 2 + j; if (curRow \u0026lt; 0 || curCol \u0026lt; 0 || curRow \u0026gt;= height || curCol \u0026gt;= width) { } else { imgValue = img[curRow * width + curCol]; } result[id] += kernel[i * kernelSize + j] * imgValue; } } } int main() { int width = 1000; int height = 1000; float* img = new float[width * height]; for (int row = 0; row \u0026lt; height; ++row) { for (int col = 0; col \u0026lt; width; ++col) { img[col + row * width] = (col + row) % 256; } } int kernelSize = 3; float* kernel = new float[kernelSize * kernelSize]; for (int i = 0; i \u0026lt; kernelSize * kernelSize; ++i) { kernel[i] = i % kernelSize - 1; } float* imgGpu; float* kernelGpu; float* resultGpu; CHECK(cudaMalloc((void**)\u0026amp;imgGpu, width * height * sizeof(float))); CHECK(cudaMalloc((void**)\u0026amp;kernelGpu, kernelSize * kernelSize * sizeof(float))); CHECK(cudaMalloc((void**)\u0026amp;resultGpu, width * height * sizeof(float))); CHECK(cudaMemcpy(imgGpu, img, width * height * sizeof(float), cudaMemcpyHostToDevice)); CHECK(cudaMemcpy(kernelGpu, kernel, kernelSize * kernelSize * sizeof(float), cudaMemcpyHostToDevice)); int threadNum = getThreadNum(); int blockNum = (width * height - 0.5) / threadNum + 1; float t_sum = 0; float t2_sum = 0; for (int repeat = 0; repeat \u0026lt;= NUM_REPEATS; ++repeat) { cudaEvent_t start, stop; CHECK(cudaEventCreate(\u0026amp;start)); CHECK(cudaEventCreate(\u0026amp;stop)); CHECK(cudaEventRecord(start)); cudaEventQuery(start); conv \u0026lt;\u0026lt; \u0026lt;blockNum, threadNum \u0026gt;\u0026gt; \u0026gt; (imgGpu, kernelGpu, resultGpu, width, height, kernelSize); CHECK(cudaEventRecord(stop)); CHECK(cudaEventSynchronize(stop)); float elapsed_time; CHECK(cudaEventElapsedTime(\u0026amp;elapsed_time, start, stop)); printf(\u0026#34;Time = %g ms.\\n\u0026#34;, elapsed_time); if (repeat \u0026gt; 0) { t_sum += elapsed_time; t2_sum += elapsed_time * elapsed_time; } CHECK(cudaEventDestroy(start)); CHECK(cudaEventDestroy(stop)); } const float t_ave = t_sum / NUM_REPEATS; const float t_err = sqrt(t2_sum / NUM_REPEATS - t_ave * t_ave); printf(\u0026#34;Time = %g +- %g ms.\\n\u0026#34;, t_ave, t_err); float* result = new float[width * height]; CHECK(cudaMemcpy(result, resultGpu, width * height * sizeof(float), cudaMemcpyDeviceToHost)); // visualization printf(\u0026#34;img\\n\u0026#34;); for (int row = 0; row \u0026lt; 10; ++row) { for (int col = 0; col \u0026lt; 10; ++col) { printf(\u0026#34;%2.0f \u0026#34;, img[col + row * width]); } printf(\u0026#34;\\n\u0026#34;); } printf(\u0026#34;kernel\\n\u0026#34;); for (int row = 0; row \u0026lt; kernelSize; ++row) { for (int col = 0; col \u0026lt; kernelSize; ++col) { printf(\u0026#34;%2.0f \u0026#34;, kernel[col + row * kernelSize]); } printf(\u0026#34;\\n\u0026#34;); } printf(\u0026#34;result\\n\u0026#34;); for (int row = 0; row \u0026lt; 10; ++row) { for (int col = 0; col \u0026lt; 10; ++col) { printf(\u0026#34;%2.0f \u0026#34;, result[col + row * width]); } printf(\u0026#34;\\n\u0026#34;); } return 0; } ","date":"2024-05-20","externalUrl":null,"permalink":"/posts/cuda/cuda_conv/","section":"Blogs","summary":"CUDA Conv # 在 PyTorch 上实现Conv很简单 import torch from torch.nn.functional import","title":"CUDA Conv","type":"posts"},{"content":"","date":"2024-05-18","externalUrl":null,"permalink":"/tags/gpu/","section":"Tags","summary":"","title":"GPU","type":"tags"},{"content":" CPU 与 GPU 的不同 # CPU，4个 ALU，主要负责逻辑计算，1个控制单元 Control，1个 DRAM，1个 Cache GPU，绿色小方块看作 ALU，红色框看作一个 SM，SM 中的多个 ALU share 一个Control 和 Cache，SM 可以看作一个多核 CPU，但是 ALU 更多，control 更少，也就是算力提升，控制力减弱 所以，CPU 适合控制逻辑复杂的任务，GPU 适合逻辑简单、数据量大、计算量大的任务。\nGPU, CUDA, AI Framework 的关系 # Reference:\nNVIDIA CUDA Docs cuda编程学习 一张图了解GPU、CUDA、CUDA toolkit和pytorch的关系 GPU 内存概念浅析 GPU 内部结构 # 每一个 SM 有自己的 Wrap scheduler 、寄存器（Register）、指令缓存、L1缓存、共享内存。\nA100 中每个 SM 包括 4 个 SM partition（SMP），里边绿色的就是 Streaming Processor（SP），也叫 CUDA cores，它们是实际执行计算的基本单元。\n所有的 SM 共享 L2 缓存。整个 GPU 内存结构如下图所示\nGPU 内存结构 # 按照存储功能进行细分，GPU 内存可以分为：局部内存（local memory）、全局内存（global memory）、常量内存（constant memory）、共享内存（shared memory）、寄存器（register）、L1/L2 缓存等。\n其中全局内存、局部内存、常量内存都是片下内存(off-chip)，储存在 HBM 上。所以 HBM 的大部分作为全局内存。\non-chip：L1/L2 cache：多级缓存，在 GPU 芯片内部\noff-chip：GPU DRAM/HBM, global memory\nL2 缓存可以被所有 SM 访问，速度比全局内存快。Flash attention 的思路就是尽可能地利用 L2 缓存，减少 HBM 的数据读写时间\nL1 缓存用于存储 SM 内的数据，被 SM 内的 CUDA cores 共享，但是跨 SM 之间的 L1 不能相互访问\n局部内存 (local memory) 是线程独享的内存资源，线程之间不可以相互访问。局部内存属于off-chip，所以访问速度跟全局内存一样。它主要是用来应对寄存器不足时的场景，即在线程申请的变量超过可用的寄存器大小时，nvcc 会自动将一部数据放置到片下内存里。\n寄存器（register）是线程能独立访问的资源，它是片上（on chip）存储，用来存储一些线程的暂存数据。寄存器的速度是访问中最快的，但是它的容量较小，只有几百甚至几十 KB，而且要被许多线程均分\n共享内存（shared memory） 是一种在线程块内能访问的内存，是片上（on chip）存储，访问速度较快。共享内存主要是缓存一些需要反复读写的数据。共享内存与 L1 缓存的位置、速度极其类似，区别在于共享内存的控制与生命周期管理与 L1 不同：共享内存受用户控制，L1 受系统控制。共享内存更利于线程块之间数据交互。\n常量内存（constant memory）是片下（off chip）存储，但是通过特殊的常量内存缓存（constant cache）进行缓存读取，它是只读内存。常量内存主要是解决一个 warp scheduler 内多个线程访问相同数据时速度太慢的问题。假设所有线程都需要访问一个 constant_A 的常量，在存储介质上 constant_A 的数据只保存了一份，而内存的物理读取方式决定了多个线程不能在同一时刻读取到该变量，所以会出现先后访问的问题，这样使得并行计算的线程出现了运算时差。常量内存正是解决这样的问题而设置的，它有对应的 cache 位置产生多个副本，让线程访问时不存在冲突，从而保证并行度。\nTensor Core # CUDA core 和 Tensor core 的区别：\nTensor core 是在 Volta 以及之后的架构中才有的, 相比于CUDA core，它可以提供更高效的运算。 每个 GPU clock，CUDA core 可以进行一次单精度乘加运算，即：in fp32: x += y * z。 每个 GPU clock，Tensor core 可以完成 4 × 4 的混合精度矩阵乘加 (matrix multiply-accumulate, MMA)：D=A * B + C，其中 A、B、C、D 都是 4 × 4 矩阵。A 和 B是 FP16 矩阵，而累加矩阵 C 和 D 可以是 FP16 或 FP32 矩阵（FP16/FP16 或 FP16/FP32 两种模式。所以每个 GPU clock，Tensor core 可以执行 64 个浮点 FMA 混合精度运算（4 × 4 × 4）。 Turing 架构中新增了 INT8/INT32, INT4/INT32, INT1/INT32 等模式 V100 中，一个 SM 中有 8 个 Tensor core，每个 GPU clock 共可以执行 1024 个浮点运算（64 × 8 × 2，乘以 2 因为乘加是两个浮点运算）\nReference:\nTENSOR CORE DL PERFORMANCE GUIDE GPU内存概念浅析 ","date":"2024-05-18","externalUrl":null,"permalink":"/posts/cuda/gpu/","section":"Blogs","summary":"CPU 与 GPU 的不同 # CPU，4个 ALU，主要负","title":"GPU 结构","type":"posts"},{"content":" Main Concepts # gridDim\ngridDim.x、gridDim.y、gridDim.z分别表示 grid 各个维度的大小\nblockDim\nblockDim.x、blockDim.y、blockDim.z分别表示 block 各个维度的大小\nblockIdx\nblockIdx.x、blockIdx.y、blockIdx.z分别表示当前 block 在 grid 中的坐标\nthreadIdx\nthreadIdx.x、threadIdx.y、threadIdx.z分别表示当前 thread 在 block 的坐标\ngrid 里总的线程个数 N = gridDim.x * gridDim.y * gridDim.z * blockDim.x * blockDim.y * blockDim.z\n通过 blockIdx.x、blockIdx.y、blockIdx.z、threadIdx.x、threadIdx.y、threadIdx.z 可以定位一个线程的坐标。\n将所有的线程排成一个序列，序列号为 0 , 1 , 2 , … , N ，如何找到当前 thread 的序列号 ?\n先找到该thread所在的 block的序号 blockId = blockIdx.x + blockIdx.ygridDim.x + blockIdx.zgridDim.x*gridDim.y 然后找到当前 thread 在 block 中的序号 threadId = threadIdx.x + threadIdx.yblockDim.x + threadIdx.zblockDim.x*blockDim.y 计算一个 block 中一共有多少个 thread， M = blockDim.xblockDim.yblockDim.z 求得当前的线程的序列号 idx = threadId + M*blockId Steps of CUDA Program # Config GPU(device), cudaSetDevice() Allocate device memory, cudaMalloc(), cudaMallocManaged() Allocate CPU(Host) memory Copy data from host to device, cudaMemcpy() Run kernel on device Copy result from device to host, cudaMemcpy() Print result on host Release host and device memory, cudaFree(), free() CPU is always called host, the GPU is called device\nC Example:\n// file: sample.cu #include\u0026lt;stdint.h\u0026gt; #include\u0026lt;cuda.h\u0026gt; #include \u0026#34;cuda_runtime.h\u0026#34; #include \u0026#34;device_launch_parameters.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; // This is a sample CUDA kernel, called on host and execte on device __global__ void add(float* a) { a[threadIdx.x] = 1; } int main(int argc, char** argv) { // 1. Set device cudaSetDevice(0); // 2. Allocate device memory float* dx; cudaMalloc((void**)\u0026amp;dx, 16 * sizeof(float)); // 3. Allocate host memory float hx[16] = { 0 }; // 4. Copy data from host to device cudaMemcpy(dx, hx, 16 * sizeof(float), cudaMemcpyHostToDevice); // 5. Run kernel on device add \u0026lt;\u0026lt; \u0026lt;1, 16 \u0026gt;\u0026gt; \u0026gt; (dx); // 6. Copy result from device to host cudaMemcpy(hx, dx, 16 * sizeof(float), cudaMemcpyDeviceToHost); // 7. Print result on host for (int i = 0; i \u0026lt; 16; i++) { printf(\u0026#34;%f \\n\u0026#34;, hx[i]); } 8. Release host and device memory cudaFree(dx); free(hx); //cudaDeviceReset(); return 0; } 使用nvcc编译,然后运行\nnvcc sample.cu - o sample ./sample CUDA 提供了统一内存: gpu和cpu可访问的单一内存空间. 调用cudaMallocManaged()，它返回一个指针，从host code或device code都可以访问。要释放数据，只需将指针传递给cudaFree()。\nCUDA Kernel and Parrallel Computing # 前置知识: 理解 GPU 结构, Grid, Block, Thread 这几个逻辑概念之间的关系\nCUDA kernel 的编程模型\n[Todo] Dim and size detail\n调用kernel: add \u0026lt;\u0026lt; \u0026lt;blockNumber, threadNumber \u0026gt;\u0026gt; \u0026gt; (dx);\n编写kernel：\n用关键字描述符 __global__ 声明kernel: __global__ void add(){} 调用 kernel 时的参数 \u0026lt;\u0026lt;\u0026lt;blockNumber per grid, threadNumber per block\u0026gt;\u0026gt;\u0026gt; 决定了共有 TotalThreadNum = blockNumber * threadNumber 个线程可以并行执行任务 kernel 内的每一次迭代，意味着 TotalThreadNum 个线程并行执行了一次循环体中的任务（即每个线程完成对一份数据的处理），也就是每次迭代能处理 TotalThreadNum 份数据，TotalThreadNum 也等价于跨步(stride)的大小 kernel 中 threadIdx.x 代表 the index of the thread within the block， blockDim.x 代表 the size of block（number of threads in block（假设 这里的 grid 和 block 的 dim 只有一维） kernel 内 threadIdx.x 和 blockIdx.x 的组合对应线程的唯一标识 以add_3这个 kernel 为例，可以用 index = blockIdx.x * blockDim.x + threadIdx.x 获得当前线程的要处理的数据的数组下标（见下图），\n__global__ void add_3(int n, float *x, float *y) { int index = blockIdx.x * blockDim.x + threadIdx.x; // stride为grid的线程总数:blockDim.x*gridDim.x int stride = blockDim.x * gridDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] = x[i] + y[i]; } Kernel examples(下面的 C++ Example)\n1个block,1个线程: add_1() 1个block,多个线程: add_2() 多个block,多个线程: add_3() 多个block，多个线程也称为网格跨步循环，其中每次循环的跨步(stride)为 grid 的线程总数: stride = blockDim.x * gridDim.x\nC++ Example:\n// file: add.cu #include \u0026lt;iostream\u0026gt; #include \u0026lt;math.h\u0026gt; // Kernel function to add the elements of two arrays // single thread __global__ void add_1(int n, float *x, float *y) { for (int i = 0; i \u0026lt; n; i++) y[i] = x[i] + y[i]; } // single block, multi threads __global__ void add_2(int n, float *x, float *y) { int index = threadIdx.x; int stride = blockDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] = x[i] + y[i]; } // multi block, multi threads // 网格跨步(stride)循环 __global__ void add_3(int n, float *x, float *y) { int index = blockIdx.x * blockDim.x + threadIdx.x; // stride为grid的线程总数:blockDim.x*gridDim.x int stride = blockDim.x * gridDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] = x[i] + y[i]; } int main(void) { int N = 1\u0026lt;\u0026lt;20; float *x, *y; // Allocate Unified Memory – accessible from CPU or GPU cudaMallocManaged(\u0026amp;x, N*sizeof(float)); cudaMallocManaged(\u0026amp;y, N*sizeof(float)); // initialize x and y arrays on the host for (int i = 0; i \u0026lt; N; i++) { x[i] = 1.0f; y[i] = 2.0f; } // Run kernel on 1M elements on the GPU // Just run with single thread :) add_1\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Run with 1 block and multi threads add_2\u0026lt;\u0026lt;\u0026lt;1, 256\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Run with multi block and multi threads int blockSize = 256;//并行线程数量 int numBlocks = (N + blockSize - 1) / blockSize;//线程块数量 add_3\u0026lt;\u0026lt;\u0026lt;numBlocks, blockSize\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Wait for GPU to finish before accessing on host cudaDeviceSynchronize(); // Check for errors (all values should be 3.0f) float maxError = 0.0f; for (int i = 0; i \u0026lt; N; i++) maxError = fmax(maxError, fabs(y[i]-3.0f)); std::cout \u0026lt;\u0026lt; \u0026#34;Max error: \u0026#34; \u0026lt;\u0026lt; maxError \u0026lt;\u0026lt; std::endl; // Free memory cudaFree(x); cudaFree(y); return 0; } CUDA Kernel for Conv # CUDA Code Profiling # nvprof是CUDA工具包附带的命令行GPU分析器\nReference:\nNVIDIA CUDA Docs ","date":"2024-05-10","externalUrl":null,"permalink":"/posts/cuda/cuda_1/","section":"Blogs","summary":"Main Concepts # gridDim gridDim.x、gridDi","title":"CUDA Programming","type":"posts"},{"content":"","date":"2024-05-05","externalUrl":null,"permalink":"/tags/flash-attention/","section":"Tags","summary":"","title":"Flash Attention","type":"tags"},{"content":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n与标准 attention 相比，Flash Attention 有以下三点特点：\n运算速度更快 (Fast) 更节省显存 (Memory-Efficient) 计算结果相同 (Exact) FlashAttention 目的不是节约 FLOPs，而是减少对HBM的访问。它没有改变原有的计算公式，整体计算复杂度并未降低。\n背景 # GPU中存储单元主要有 HBM 和 SRAM：HBM 容量大但是访问速度慢，SRAM容量小却有着较高的访问速度。例如：A100 GPU有40-80GB的HBM，带宽为1.5-2.0TB/s；每108个流式多核处理器各有192KB的片上SRAM，带宽估计约为 19TB/s。可以看出，片上的SRAM比HBM快一个数量级，但尺寸要小许多数量级。\n当输入序列（sequence length）较长时，Transformer的计算过程缓慢且耗费内存，这是因为 self-attention 的 time 和 memory complexity 会随着 sequence length 的增加成二次增长。\n标准Attention的计算过程： $$ S=Q K^T \\in \\mathbb{R}^{N \\times N} $$ $$ P=\\operatorname{softmax}(S) \\in \\mathbb{R}^{N \\times N} $$ $$ O=P V \\in \\mathbb{R}^{N \\times N} $$\n标准Attention的中间结果 𝑆, 𝑃 通常需要通过高带宽内存（HBM）进行存取，两者所需内存空间复杂度为\\(O(Nd+N^2)\\), 对 HBM 的重复读写是主要瓶颈。要解决这个问题，需要做两件事：\n在不访问整个输入的情况下计算 softmax 不为反向传播存储大的中间 attention 矩阵(\\(N^2\\)) FlashAttention V1 # FlashAttention 提出了两种方法来解决上述问题：tiling 和 recomputation。\ntiling - 注意力计算被重新构造，将输入分割成块，并通过在输入块上进行多次传递来递增地执行softmax操作。 recomputation - 存储来自前向的 softmax 归一化因子，以便在反向中快速重新计算芯片上的 attention，这比从HBM读取中间矩阵的标准注意力方法更快。可以把它看作基于 tiling 的特殊的 gradient checkpointing 正常的softmax计算：\n$$m(x):=\\max _i x i$$ $$f(x):=\\left[e^{x_1-m(x)} \\ldots e^{x_B-m(x)}\\right]$$ $$\\ell(x):=\\sum_i f(x)_i$$ $$\\operatorname{softmax}(x):=\\frac{f(x)}{\\ell(x)}$$\nsoftmax 伪代码: softmax 函数需要三个循环，第一个循环计算数组的最大值，第二个循环计算 softmax 的分母，第三个循环计算 softmax 输出。\n分块的 softmax 计算(假设分2块并行计算)：\n$$m(x)=m\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)=\\max \\left(m\\left(x^{(1)}\\right), m\\left(x^{(2)}\\right)\\right) $$ $$f(x)=\\left[e^{m\\left(x^{(1)}\\right)-m(x)} f\\left(x^{(1)}\\right) \\quad e^{m\\left(x^{(2)}\\right)-m(x)} f\\left(x^{(2)}\\right)\\right] $$ $$\\ell(x)=\\ell\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)=e^{m\\left(x^{(1)}\\right)-m(x)} \\ell\\left(x^{(1)}\\right)+e^{m\\left(x^{(2)}\\right)-m(x)} \\ell\\left(x^{(2)}\\right) $$ $$\\operatorname{softmax}(x)=\\frac{f(x)}{\\ell(x)}$$\n分块的 softmax 伪代码: 在第一个循环中同时对最大值\\(m\\)以及 softmax 的分母\\(d\\)进行更新，从而减少了一个循环。通过 tiling 的方式，softmax 的循环数从三个减到了两个，从而可以降低内存消耗。\nflashattention 伪代码： 中间变量：\\(O_i\\)(最终乘积)、\\(l_i\\)（softmax的分母，即累加和）、\\(m_i\\)（遍历到当前块为止的最大值），再也不用保存全部的S和P了。\n由于重新计算导致 FLOPs 增加，但是由于大量减少HBM访问，FlashAttention 运行速度更快\nFlashAttention的 FLOPs 为 \\(𝑂(𝑁^2𝑑)\\)，除了 input 和 output，额外需要的内存为 \\(𝑂(𝑁)\\), 对HBM访问的次数为 \\(𝑂(𝑁^2𝑑^2𝑀^{−1})\\), 比标准 Attention 的 \\(O(Nd+N^2)\\)更高效\nPyTorch 2.0已将 FlashAttention 集成到官方库中，可以直接调用 torch.nn.functional.scaled_dot_product_attention\n总结 # FlashAttention V1:\n通过切块技术减少了内存访问次数，提高了计算速度和内存利用率。 内存访问复杂度为 \\(𝑂(𝑁^2𝑑^2𝑀^{−1})\\), 比标准 Attention 的 \\(O(Nd+N^2)\\)更高效 ","date":"2024-05-05","externalUrl":null,"permalink":"/posts/llm/flash_attention/","section":"Blogs","summary":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness 与标准 attention 相比，Flash","title":"Flash Attention","type":"posts"},{"content":" Self-Attention # 对于self-attention，由于 Q, K, V 都来自输入 X ，在计算 \\(QT^T\\) 时，模型很容易关注到自身的位置上，也就是 \\(QT^T\\) 对角线上的激活值会明显比较大, 这会削弱模型关注其他高价值位置的能力，也就限制模型的理解和表达能力。MHA 对这个问题有一定的缓解作用。\nMHA # MHA（multi-head attention）\nQKV 经过线性变换后，将他们分别在 hidden states 维度上切分成 heads 份。\nMHA 相比单头的情况，相当于只是把 QKV 切成多份并行计算了，对于实际需要缓存的大小没有影响\nKV Cache # Decoding阶段，下一个step的输入其实包含了上一个step的内容，而且只在最后面多了一个token的数据，下一个step的计算应该也包含了上一个step的计算。\nKV Cache 的目的：空间换时间，用缓存把需要重复利用的中间计算结果存下来，减少重复计算。而 K 和 V 就是要缓存的对象。\nQ K V\n对于输入长度为 \\(s)\\ ，层数为 \\(L\\) ，hidden size为 \\(d\\) 的模型:\n当 batch size=1 时 需要缓存的参数量为: \\(2Ls*d\\)，其中 2 表示 K + V 需要的空间为（使用半精度浮点数 float16）：\\(22Lsd Bytes\\) ，其中第一个 2 表示 float16 占用 2 Bytes 当 batch size=B 时 需要缓存的参数量为: \\(2LsdB\\) 需要的空间为（使用半精度浮点数 float16）：\\(22Lsd*B Bytes\\) MHA 相比单头的情况，相当于只是把 QKV 切成多份并行计算了，对于实际需要缓存的大小没有影响 GQA、MQA [Todo] 以Llama2 7B为例，\\(L=32, d=4096\\)，此时每个 token 需要的 cache 空间为 524,288 Bytes(512 KB)，当 \\(s=1024, batch size=1)\\) 时，需要 512 MB\n主流显卡配置：\nNV A100(Ampere Arch)，HBM2e 40/80GB，L2 Cache 40MB, CUDA Cores - 16896/14592 NV H100(Hopper Arch), HBM2e/HBM3 80GB, L2 Cache 50MB, CUDA Cores 6912 NV V100(Volta Arch), HBM2 16/32GB, L2 Cache 6MB, CUDA Cores 5120 H100也只有50M的L2 Cache，只能支持Llama2 7B共100个tokens的seq，超出L2 Cache的部分只能走到显存中去了，但是 HBM 的 bandwidth 比 L2 Cache 小多了，A100 memory bandwidth 如下图所示：\nMQA # 《Fast Transformer Decoding: One Write-Head is All You Need》\nQ 经过线性变换后，MQA 只对 Q 进行 多个 head 的切分，每个 head 的 Q 的维度变为 \\(Q_s*(d/heads)\\)， K和V并不切分，而是线性变换时直接把hidden state维度降低为 d/heads， 然后 heads 个 Q 分别于 同一份 K，V 继续宁 attention 计算，最后将结果 concat 起来。\n比如在Llama2 7B中用了32个头，MQA后，1024个 tokens 需要 KVCache 就变成MHA 的 1/32，即 512MB/32=16MB，基本可以全部放入A100的L2 Cache\n由于共享了多个头的参数，限制了模型的表达能力，MQA虽然能好地支持推理加速，但是在效果上略略比MHA差一点\nGQA # 《GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》\nGQA（Grouped-Query Attention）提出了一个折中的办法，既能减少 MQA 的损失，又比 MHA 需要更少的缓存。\nGQA里， Q 还是按原来MHA/MQA的做法不变，但是 用多份 K和V，不过数量小于 Q 的 heads。相当于把 Q 的多个头给分了group，同一个group内的 Q 共享同一套 KV，不同group的 Q 所用的 KV 不同。\nGQA的速度相比MHA有明显提升，而效果上比MQA也好一些，能做到和MHA基本没差距。Llama2 70B 用的就是GQA。\n未完待续\u0026hellip;\nTodo: 代码，GQA量化计算\nReference:\n理解Attention:从起源到MHA,MQA和GQA self-attention code ","date":"2024-05-05","externalUrl":null,"permalink":"/posts/llm/attention/","section":"Blogs","summary":"Self-Attention # 对于self-attention，由","title":"Attention and KV Cache","type":"posts"},{"content":"","date":"2024-05-05","externalUrl":null,"permalink":"/tags/kvcache/","section":"Tags","summary":"","title":"KVCache","type":"tags"},{"content":" Me # class Me: def __init__(self): self.names = \u0026#34;Victor Yang\u0026#34; self.prefer_name = \u0026#34;Victor\u0026#34; self.profession = \u0026#34;Programmer\u0026#34; self.hometown = \u0026#34;Shandong, CN\u0026#34; self.current_location = \u0026#34;Beijing, CN\u0026#34; self.graduate = \u0026#34;University of Science and Technology of China(USTC), Software Engineering\u0026#34; self.undergraduate = \u0026#34;China University of Petroleum(UPC), Computer Science\u0026#34; self.interests = [\u0026#34;AI\u0026#34;, \u0026#34;Taoist\u0026#34;, \u0026#34;Traditional Chinese Medicine\u0026#34;, \u0026#34;History\u0026#34;, \u0026#34;Finance\u0026#34;, \u0026#34;Science Fiction\u0026#34;] self.hoppies = [\u0026#34;Reading\u0026#34;, \u0026#34;Yoga\u0026#34;, \u0026#34;Meditation\u0026#34;, \u0026#34;Swimming\u0026#34;] My Skills # Generative AI/AIGC: Transformer, LLM, VLM, Llama, Llava, MoE, Attention, vLLM, DeepSpeed, RAG, Langchain, Agent Parallel Computing: CUDA, GPU, Horovod, MPI Machine Learning: Deep Learning, PyTorch, Federated Learning Programming Language: Python, C++, GoLang Cloud Native: Knative, Ray, Docker Others: Linux ","date":"2024-05-01","externalUrl":null,"permalink":"/about/","section":"Welcome to My Blog","summary":"Me # class Me: def __init__(self): self.names = \u0026#34;Victor Yang\u0026#34; self.prefer_name = \u0026#34;Victor\u0026#34; self.profession = \u0026#34;Programmer\u0026#34; self.hometown = \u0026#34;Shandong, CN\u0026#34; self.","title":"About","type":"page"},{"content":"This is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing. 🚀\nYou can also use these content pages to define Hugo metadata like titles and descriptions that will be used for SEO and other purposes.\n","date":"0001-01-01","externalUrl":null,"permalink":"/tags/advanced/","section":"Tags","summary":"This is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing.","title":"Advanced","type":"tags"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/series/attention/","section":"Series","summary":"","title":"Attention","type":"series"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/tags/_index.zh-cn/","section":"Tags","summary":"","title":"标签","type":"tags"},{"content":"欢迎来到我的博客!\n","date":"0001-01-01","externalUrl":null,"permalink":"/_index.zh-cn/","section":"Welcome to My Blog","summary":"欢迎来到我的博客!","title":"欢迎来到我的博客","type":"page"},{"content":"这是高级标记。类似其他 Blowfish 中的其他列表页面，你可以在分类列表页添加自定义内容，这部分内容会显示在顶部。🚀\n你也可以用这些内容来定义 Hugo 的元数据，比如标题和描述。这些内容可以被用来增强 SEO 或其他目的。\n","date":"0001-01-01","externalUrl":null,"permalink":"/tags/advanced/_index.zh-cn/","section":"Tags","summary":"这是高级标记。类似其他 Blowfish 中的其他列表页面","title":"高级","type":"tags"}]