
[{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/posts/","section":"Blogs","summary":"","title":"Blogs","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/tags/cuda/","section":"Tags","summary":"","title":"CUDA","type":"tags"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/series/cuda-parallel-programming/","section":"Series","summary":"","title":"CUDA Parallel Programming","type":"series"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/posts/cuda/","section":"Blogs","summary":"","title":"CUDA Parallel Programming","type":"posts"},{"content":" CUDA 程序的基本步骤 # Config GPU(device), cudaSetDevice() Allocate device memory, cudaMalloc(), cudaMallocManaged() Allocate CPU(Host) memory Copy data from host to device, cudaMemcpy() Run kernel on device Copy result from device to host, cudaMemcpy() Print result on host Release host and device memory, cudaFree(), free() CPU is always called host, the GPU is called device\nC Example:\n// file: sample.cu #include\u0026lt;stdint.h\u0026gt; #include\u0026lt;cuda.h\u0026gt; #include \u0026#34;cuda_runtime.h\u0026#34; #include \u0026#34;device_launch_parameters.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; // This is a sample CUDA kernel, called on host and execte on device __global__ void add(float* a) { a[threadIdx.x] = 1; } int main(int argc, char** argv) { // 1. Set device cudaSetDevice(0); // 2. Allocate device memory float* dx; cudaMalloc((void**)\u0026amp;dx, 16 * sizeof(float)); // 3. Allocate host memory float hx[16] = { 0 }; // 4. Copy data from host to device cudaMemcpy(dx, hx, 16 * sizeof(float), cudaMemcpyHostToDevice); // 5. Run kernel on device add \u0026lt;\u0026lt; \u0026lt;1, 16 \u0026gt;\u0026gt; \u0026gt; (dx); // 6. Copy result from device to host cudaMemcpy(hx, dx, 16 * sizeof(float), cudaMemcpyDeviceToHost); // 7. Print result on host for (int i = 0; i \u0026lt; 16; i++) { printf(\u0026#34;%f \\n\u0026#34;, hx[i]); } 8. Release host and device memory cudaFree(dx); free(hx); //cudaDeviceReset(); return 0; } 使用nvcc编译,然后运行\nnvcc sample.cu - o sample ./sample CUDA 提供了统一内存: gpu和cpu可访问的单一内存空间. 调用cudaMallocManaged()，它返回一个指针，从host code或device code都可以访问。要释放数据，只需将指针传递给cudaFree()。\nCUDA Kernel and Parrallel Computing # 前置知识: 理解 GPU 结构, Grid, Block, Thread 这几个逻辑概念之间的关系\n用关键字描述符 __global__ 声明kernel: __global__ void add(){}\n调用kernel: add \u0026lt;\u0026lt; \u0026lt;blockNumber, threadNumber \u0026gt;\u0026gt; \u0026gt; (dx);\n[Todo] Dim and size detail\nthreadIdx.x：the index of the thread within the block\nblockDim.x：the size of block（number of threads in the thread block, blockSize = block中的线程数量\nKernel examples(下面的code)\n1个block,1个线程: add_1() 1个block,多个线程: add_2() 多个block,多个线程: add_3() 多个block，多个线程也称为网格跨步循环，其中每次循环的跨步(stride)为 grid 的线程总数: stride = blockDim.x * gridDim.x\nC++ Example:\n// file: add.cu #include \u0026lt;iostream\u0026gt; #include \u0026lt;math.h\u0026gt; // Kernel function to add the elements of two arrays // single thread __global__ void add_1(int n, float *x, float *y) { for (int i = 0; i \u0026lt; n; i++) y[i] = x[i] + y[i]; } // single block, multi threads __global__ void add_2(int n, float *x, float *y) { int index = threadIdx.x; int stride = blockDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] = x[i] + y[i]; } // multi block, multi threads // 网格跨步(stride)循环 __global__ void add_3(int n, float *x, float *y) { int index = blockIdx.x * blockDim.x + threadIdx.x; // stride为grid的线程总数:blockDim.x*gridDim.x int stride = blockDim.x * gridDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] = x[i] + y[i]; } int main(void) { int N = 1\u0026lt;\u0026lt;20; float *x, *y; // Allocate Unified Memory – accessible from CPU or GPU cudaMallocManaged(\u0026amp;x, N*sizeof(float)); cudaMallocManaged(\u0026amp;y, N*sizeof(float)); // initialize x and y arrays on the host for (int i = 0; i \u0026lt; N; i++) { x[i] = 1.0f; y[i] = 2.0f; } // Run kernel on 1M elements on the GPU // Just run with single thread :) add_1\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Run with 1 block and multi threads add_2\u0026lt;\u0026lt;\u0026lt;1, 256\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Run with multi block and multi threads int blockSize = 256;//并行线程数量 int numBlocks = (N + blockSize - 1) / blockSize;//线程块数量 add_3\u0026lt;\u0026lt;\u0026lt;numBlocks, blockSize\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Wait for GPU to finish before accessing on host cudaDeviceSynchronize(); // Check for errors (all values should be 3.0f) float maxError = 0.0f; for (int i = 0; i \u0026lt; N; i++) maxError = fmax(maxError, fabs(y[i]-3.0f)); std::cout \u0026lt;\u0026lt; \u0026#34;Max error: \u0026#34; \u0026lt;\u0026lt; maxError \u0026lt;\u0026lt; std::endl; // Free memory cudaFree(x); cudaFree(y); return 0; } CUDA Code Profiling # nvprof是CUDA工具包附带的命令行GPU分析器\nReference:\nNVIDIA CUDA Docs ","date":"2024-05-24","externalUrl":null,"permalink":"/posts/cuda/cuda_1/","section":"Blogs","summary":"CUDA 程序的基本步骤 # Config GPU(device), cudaSetDevice() Allocate device memory, cudaMalloc(), cudaMallocManaged() Allocate CPU(Host) memory","title":"CUDA 编程","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/tags/gpu/","section":"Tags","summary":"","title":"GPU","type":"tags"},{"content":" GPU # Reference:\nNVIDIA CUDA Docs ","date":"2024-05-24","externalUrl":null,"permalink":"/posts/cuda/gpu/","section":"Blogs","summary":"GPU # Reference: NVIDIA CUDA Docs","title":"GPU 结构","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"Hi, welcome to my blog.\n","date":"2024-05-24","externalUrl":null,"permalink":"/","section":"Welcome to Blowfish! 🎉","summary":"Hi, welcome to my blog.","title":"Welcome to Blowfish! 🎉","type":"page"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/attention/","section":"Tags","summary":"","title":"Attention","type":"tags"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/series/attention/","section":"Series","summary":"","title":"Attention","type":"series"},{"content":"[WIP]\nFlashAttention V2:\n在V1的基础上减少了非矩阵乘法运算的FLOPs。 通过并行化和任务分配优化提高了计算速度和GPU利用率，性能提升了2-3倍。 Flash-Decoding借鉴了FlashAttention的优点，将并行化维度扩展到keys/values序列长度，提高了推理速度。 Flash-Decoding几乎不用额外存储大量数据到全局内存中，减少了内存开销。 Flash-Decoding++通过异步softmax和统一最大值、flat GEMM优化和双缓冲、启发式数据流和硬件资源适应等方法进一步提高了LLM推理的性能。 ","date":"2024-05-23","externalUrl":null,"permalink":"/posts/llm/flash_attention_2/","section":"Blogs","summary":"[WIP] FlashAttention V2: 在V1的基础上减少了非矩阵乘法运算","title":"Flash Attention V2","type":"posts"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/posts/llm/","section":"Blogs","summary":"","title":"LLM","type":"posts"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/nlp/","section":"Tags","summary":"","title":"NLP","type":"tags"},{"content":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n与标准 attention 相比，Flash Attention 有以下三点特点：\n运算速度更快 (Fast) 更节省显存 (Memory-Efficient) 计算结果相同 (Exact) FlashAttention 目的不是节约 FLOPs，而是减少对HBM的访问。它没有改变原有的计算公式，整体计算复杂度并未降低。\n背景 # GPU中存储单元主要有 HBM 和 SRAM：HBM 容量大但是访问速度慢，SRAM容量小却有着较高的访问速度。例如：A100 GPU有40-80GB的HBM，带宽为1.5-2.0TB/s；每108个流式多核处理器各有192KB的片上SRAM，带宽估计约为 19TB/s。可以看出，片上的SRAM比HBM快一个数量级，但尺寸要小许多数量级。\n当输入序列（sequence length）较长时，Transformer的计算过程缓慢且耗费内存，这是因为 self-attention 的 time 和 memory complexity 会随着 sequence length 的增加成二次增长。\n标准Attention的计算过程： $$ S=Q K^T \\in \\mathbb{R}^{N \\times N} $$ $$ P=\\operatorname{softmax}(S) \\in \\mathbb{R}^{N \\times N} $$ $$ O=P V \\in \\mathbb{R}^{N \\times N} $$\n标准Attention的中间结果 𝑆, 𝑃 通常需要通过高带宽内存（HBM）进行存取，两者所需内存空间复杂度为\\(O(Nd+N^2)), 对 HBM 的重复读写是主要瓶颈。要解决这个问题，需要做两件事：\n在不访问整个输入的情况下计算 softmax 不为反向传播存储大的中间 attention 矩阵(\\(N^2\\)) FlashAttention V1 # FlashAttention 提出了两种方法来解决上述问题：tiling 和 recomputation。\ntiling - 注意力计算被重新构造，将输入分割成块，并通过在输入块上进行多次传递来递增地执行softmax操作。 recomputation - 存储来自前向的 softmax 归一化因子，以便在反向中快速重新计算芯片上的 attention，这比从HBM读取中间矩阵的标准注意力方法更快。可以把它看作基于 tiling 的特殊的 gradient checkpointing 正常的softmax计算：\n$$m(x):=\\max _i x i$$ $$f(x):=\\left[e^{x_1-m(x)} \\ldots e^{x_B-m(x)}\\right]$$ $$\\ell(x):=\\sum_i f(x)_i$$ $$\\operatorname{softmax}(x):=\\frac{f(x)}{\\ell(x)}$$\nsoftmax 伪代码: softmax 函数需要三个循环，第一个循环计算数组的最大值，第二个循环计算 softmax 的分母，第三个循环计算 softmax 输出。\n分块的 softmax 计算(假设分2块并行计算)：\n$$m(x)=m\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)=\\max \\left(m\\left(x^{(1)}\\right), m\\left(x^{(2)}\\right)\\right) $$ $$f(x)=\\left[e^{m\\left(x^{(1)}\\right)-m(x)} f\\left(x^{(1)}\\right) \\quad e^{m\\left(x^{(2)}\\right)-m(x)} f\\left(x^{(2)}\\right)\\right] $$ $$\\ell(x)=\\ell\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)=e^{m\\left(x^{(1)}\\right)-m(x)} \\ell\\left(x^{(1)}\\right)+e^{m\\left(x^{(2)}\\right)-m(x)} \\ell\\left(x^{(2)}\\right) $$ $$\\operatorname{softmax}(x)=\\frac{f(x)}{\\ell(x)}$$\n分块的 softmax 伪代码: 在第一个循环中同时对最大值\\(m\\)以及 softmax 的分母\\(d\\)进行更新，从而减少了一个循环。通过 tiling 的方式，softmax 的循环数从三个减到了两个，从而可以降低内存消耗。\nflashattention 伪代码： 中间变量：\\(O_i\\)(最终乘积)、\\(l_i\\)（softmax的分母，即累加和）、\\(m_i\\)（遍历到当前块为止的最大值），再也不用保存全部的S和P了。\n由于重新计算导致 FLOPs 增加，但是由于大量减少HBM访问，FlashAttention 运行速度更快\nFlashAttention的 FLOPs 为 \\(𝑂(𝑁^2𝑑)\\)，除了 input 和 output，额外需要的内存为 \\(𝑂(𝑁)\\), 对HBM访问的次数为 \\(𝑂(𝑁^2𝑑^2𝑀^{−1})\\), 比标准 Attention 的 \\(O(Nd+N^2)\\)更高效\nPyTorch 2.0已将 FlashAttention 集成到官方库中，可以直接调用 torch.nn.functional.scaled_dot_product_attention\n总结 # FlashAttention V1:\n通过切块技术减少了内存访问次数，提高了计算速度和内存利用率。 内存访问复杂度为 \\(𝑂(𝑁^2𝑑^2𝑀^{−1})\\), 比标准 Attention 的 \\(O(Nd+N^2)\\)更高效 ","date":"2024-05-23","externalUrl":null,"permalink":"/posts/llm/flash_attention/","section":"Blogs","summary":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness 与标准 attention 相比，Flash","title":"Flash Attention","type":"posts"},{"content":"class Me: def __init__(self): self.names = \u0026#34;Victor Yang\u0026#34; self.prefer_name = \u0026#34;Victor\u0026#34; self.profession = \u0026#34;Programmer\u0026#34; self.hometown = \u0026#34;Shandong, CN\u0026#34; self.current_location = \u0026#34;Beijing, CN\u0026#34; self.grad_school = \u0026#34;USTC\u0026#34; self.undergrad_school = \u0026#34;UPC\u0026#34; self.interests = [\u0026#34;ML\u0026#34;, \u0026#34;Robot\u0026#34;] self.hoppies = [\u0026#34;Reading\u0026#34;, \u0026#34;Yoga\u0026#34;, \u0026#34;Meditation\u0026#34;, \u0026#34;Taoist\u0026#34;, \u0026#34;Swimming\u0026#34;] ","date":"2024-05-19","externalUrl":null,"permalink":"/about/","section":"Welcome to Blowfish! 🎉","summary":"class Me: def __init__(self): self.names = \u0026#34;Victor Yang\u0026#34; self.prefer_name = \u0026#34;Victor\u0026#34; self.profession = \u0026#34;Programmer\u0026#34; self.hometown = \u0026#34;Shandong, CN\u0026#34; self.current_location = \u0026#34;Beijing, CN\u0026#34; self.","title":"About","type":"page"},{"content":"This is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing. 🚀\nYou can also use these content pages to define Hugo metadata like titles and descriptions that will be used for SEO and other purposes.\n","date":"0001-01-01","externalUrl":null,"permalink":"/tags/advanced/","section":"Tags","summary":"This is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing.","title":"Advanced","type":"tags"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/posts/ai-framework/","section":"Blogs","summary":"","title":"AI Framework","type":"posts"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/tags/_index.zh-cn/","section":"Tags","summary":"","title":"标签","type":"tags"},{"content":"Hell\n","date":"0001-01-01","externalUrl":null,"permalink":"/_index.zh-cn/","section":"Welcome to Blowfish! 🎉","summary":"Hell","title":"欢迎来到 Blowfish! 🎉","type":"page"},{"content":"这是高级标记。类似其他 Blowfish 中的其他列表页面，你可以在分类列表页添加自定义内容，这部分内容会显示在顶部。🚀\n你也可以用这些内容来定义 Hugo 的元数据，比如标题和描述。这些内容可以被用来增强 SEO 或其他目的。\n","date":"0001-01-01","externalUrl":null,"permalink":"/tags/advanced/_index.zh-cn/","section":"Tags","summary":"这是高级标记。类似其他 Blowfish 中的其他列表页面","title":"高级","type":"tags"}]