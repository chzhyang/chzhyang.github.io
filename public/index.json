
[{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/posts/","section":"Blogs","summary":"","title":"Blogs","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/tags/conv/","section":"Tags","summary":"","title":"Conv","type":"tags"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/tags/cuda/","section":"Tags","summary":"","title":"CUDA","type":"tags"},{"content":" CUDA Conv # Steps:\nCheck Get thread info Memory allocation Copy data to device Call conv kernel Copy data to host Free memory //file: conv.cu #include\u0026lt;stdint.h\u0026gt; #include\u0026lt;cuda.h\u0026gt; #include \u0026#34;cuda_runtime.h\u0026#34; #include \u0026#34;device_launch_parameters.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;math.h\u0026gt; const int NUM_REPEATS = 10; #define CHECK(call) \\ do \\ { \\ const cudaError_t error_code = call; \\ if (error_code != cudaSuccess) \\ { \\ printf(\u0026#34;CUDA Error:\\n\u0026#34;); \\ printf(\u0026#34; File: %s\\n\u0026#34;, __FILE__); \\ printf(\u0026#34; Line: %d\\n\u0026#34;, __LINE__); \\ printf(\u0026#34; Error code: %d\\n\u0026#34;, error_code); \\ printf(\u0026#34; Error text: %s\\n\u0026#34;, \\ cudaGetErrorString(error_code)); \\ exit(1); \\ } \\ } while (0) static void HandleError(cudaError_t err, const char* file, int line) { if (err != cudaSuccess) { printf(\u0026#34;%s in %s at line %d\\n\u0026#34;, cudaGetErrorString(err), file, line); exit(EXIT_FAILURE); } } #define HANDLE_ERROR(err) (HandleError(err, __FILE__, __LINE__)) int getThreadNum() { cudaDeviceProp prop; int count; CHECK(cudaGetDeviceCount(\u0026amp;count)); printf(\u0026#34;gpu num %d\\n\u0026#34;, count); CHECK(cudaGetDeviceProperties(\u0026amp;prop, 0)); printf(\u0026#34;max thread num: %d\\n\u0026#34;, prop.maxThreadsPerBlock); printf(\u0026#34;max grid dimensions: %d, %d, %d)\\n\u0026#34;, prop.maxGridSize[0], prop.maxGridSize[1], prop.maxGridSize[2]); return prop.maxThreadsPerBlock; } __global__ void conv(float* img, float* kernel, float* result, int width, int height, int kernelSize) { int ti = threadIdx.x; int bi = blockIdx.x; int id = (bi * blockDim.x + ti); if (id \u0026gt;= width * height) { return; } int row = id / width; int col = id % width; for (int i = 0; i \u0026lt; kernelSize; ++i) { for (int j = 0; j \u0026lt; kernelSize; ++j) { float imgValue = 0; int curRow = row - kernelSize / 2 + i; int curCol = col - kernelSize / 2 + j; if (curRow \u0026lt; 0 || curCol \u0026lt; 0 || curRow \u0026gt;= height || curCol \u0026gt;= width) { } else { imgValue = img[curRow * width + curCol]; } result[id] += kernel[i * kernelSize + j] * imgValue; } } } int main() { int width = 1000; int height = 1000; float* img = new float[width * height]; for (int row = 0; row \u0026lt; height; ++row) { for (int col = 0; col \u0026lt; width; ++col) { img[col + row * width] = (col + row) % 256; } } int kernelSize = 3; float* kernel = new float[kernelSize * kernelSize]; for (int i = 0; i \u0026lt; kernelSize * kernelSize; ++i) { kernel[i] = i % kernelSize - 1; } float* imgGpu; float* kernelGpu; float* resultGpu; CHECK(cudaMalloc((void**)\u0026amp;imgGpu, width * height * sizeof(float))); CHECK(cudaMalloc((void**)\u0026amp;kernelGpu, kernelSize * kernelSize * sizeof(float))); CHECK(cudaMalloc((void**)\u0026amp;resultGpu, width * height * sizeof(float))); CHECK(cudaMemcpy(imgGpu, img, width * height * sizeof(float), cudaMemcpyHostToDevice)); CHECK(cudaMemcpy(kernelGpu, kernel, kernelSize * kernelSize * sizeof(float), cudaMemcpyHostToDevice)); int threadNum = getThreadNum(); int blockNum = (width * height - 0.5) / threadNum + 1; float t_sum = 0; float t2_sum = 0; for (int repeat = 0; repeat \u0026lt;= NUM_REPEATS; ++repeat) { cudaEvent_t start, stop; CHECK(cudaEventCreate(\u0026amp;start)); CHECK(cudaEventCreate(\u0026amp;stop)); CHECK(cudaEventRecord(start)); cudaEventQuery(start); conv \u0026lt;\u0026lt; \u0026lt;blockNum, threadNum \u0026gt;\u0026gt; \u0026gt; (imgGpu, kernelGpu, resultGpu, width, height, kernelSize); CHECK(cudaEventRecord(stop)); CHECK(cudaEventSynchronize(stop)); float elapsed_time; CHECK(cudaEventElapsedTime(\u0026amp;elapsed_time, start, stop)); printf(\u0026#34;Time = %g ms.\\n\u0026#34;, elapsed_time); if (repeat \u0026gt; 0) { t_sum += elapsed_time; t2_sum += elapsed_time * elapsed_time; } CHECK(cudaEventDestroy(start)); CHECK(cudaEventDestroy(stop)); } const float t_ave = t_sum / NUM_REPEATS; const float t_err = sqrt(t2_sum / NUM_REPEATS - t_ave * t_ave); printf(\u0026#34;Time = %g +- %g ms.\\n\u0026#34;, t_ave, t_err); float* result = new float[width * height]; CHECK(cudaMemcpy(result, resultGpu, width * height * sizeof(float), cudaMemcpyDeviceToHost)); // visualization printf(\u0026#34;img\\n\u0026#34;); for (int row = 0; row \u0026lt; 10; ++row) { for (int col = 0; col \u0026lt; 10; ++col) { printf(\u0026#34;%2.0f \u0026#34;, img[col + row * width]); } printf(\u0026#34;\\n\u0026#34;); } printf(\u0026#34;kernel\\n\u0026#34;); for (int row = 0; row \u0026lt; kernelSize; ++row) { for (int col = 0; col \u0026lt; kernelSize; ++col) { printf(\u0026#34;%2.0f \u0026#34;, kernel[col + row * kernelSize]); } printf(\u0026#34;\\n\u0026#34;); } printf(\u0026#34;result\\n\u0026#34;); for (int row = 0; row \u0026lt; 10; ++row) { for (int col = 0; col \u0026lt; 10; ++col) { printf(\u0026#34;%2.0f \u0026#34;, result[col + row * width]); } printf(\u0026#34;\\n\u0026#34;); } return 0; } ","date":"2024-05-24","externalUrl":null,"permalink":"/posts/cuda/cuda_conv/","section":"Blogs","summary":"CUDA Conv # Steps:\nCheck Get thread info Memory allocation Copy data to device Call conv kernel Copy data to host Free memory //file: conv.","title":"CUDA Conv","type":"posts"},{"content":"[Todo]\n","date":"2024-05-24","externalUrl":null,"permalink":"/posts/cuda/cuda_optimize/","section":"Blogs","summary":"[Todo]","title":"CUDA Optimization","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/series/cuda-parallel-programming/","section":"Series","summary":"","title":"CUDA Parallel Programming","type":"series"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/posts/cuda/","section":"Blogs","summary":"","title":"CUDA Parallel Programming","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"Hi, welcome to my blog.\n","date":"2024-05-24","externalUrl":null,"permalink":"/","section":"Welcome to Blowfish! ğŸ‰","summary":"Hi, welcome to my blog.","title":"Welcome to Blowfish! ğŸ‰","type":"page"},{"content":" Steps of CUDA Program # Config GPU(device), cudaSetDevice() Allocate device memory, cudaMalloc(), cudaMallocManaged() Allocate CPU(Host) memory Copy data from host to device, cudaMemcpy() Run kernel on device Copy result from device to host, cudaMemcpy() Print result on host Release host and device memory, cudaFree(), free() CPU is always called host, the GPU is called device\nC Example:\n// file: sample.cu #include\u0026lt;stdint.h\u0026gt; #include\u0026lt;cuda.h\u0026gt; #include \u0026#34;cuda_runtime.h\u0026#34; #include \u0026#34;device_launch_parameters.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; // This is a sample CUDA kernel, called on host and execte on device __global__ void add(float* a) { a[threadIdx.x] = 1; } int main(int argc, char** argv) { // 1. Set device cudaSetDevice(0); // 2. Allocate device memory float* dx; cudaMalloc((void**)\u0026amp;dx, 16 * sizeof(float)); // 3. Allocate host memory float hx[16] = { 0 }; // 4. Copy data from host to device cudaMemcpy(dx, hx, 16 * sizeof(float), cudaMemcpyHostToDevice); // 5. Run kernel on device add \u0026lt;\u0026lt; \u0026lt;1, 16 \u0026gt;\u0026gt; \u0026gt; (dx); // 6. Copy result from device to host cudaMemcpy(hx, dx, 16 * sizeof(float), cudaMemcpyDeviceToHost); // 7. Print result on host for (int i = 0; i \u0026lt; 16; i++) { printf(\u0026#34;%f \\n\u0026#34;, hx[i]); } 8. Release host and device memory cudaFree(dx); free(hx); //cudaDeviceReset(); return 0; } ä½¿ç”¨nvccç¼–è¯‘,ç„¶åè¿è¡Œ\nnvcc sample.cu - o sample ./sample CUDA æä¾›äº†ç»Ÿä¸€å†…å­˜: gpuå’Œcpuå¯è®¿é—®çš„å•ä¸€å†…å­˜ç©ºé—´. è°ƒç”¨cudaMallocManaged()ï¼Œå®ƒè¿”å›ä¸€ä¸ªæŒ‡é’ˆï¼Œä»host codeæˆ–device codeéƒ½å¯ä»¥è®¿é—®ã€‚è¦é‡Šæ”¾æ•°æ®ï¼Œåªéœ€å°†æŒ‡é’ˆä¼ é€’ç»™cudaFree()ã€‚\nCUDA Kernel and Parrallel Computing # å‰ç½®çŸ¥è¯†: ç†è§£ GPU ç»“æ„, Grid, Block, Thread è¿™å‡ ä¸ªé€»è¾‘æ¦‚å¿µä¹‹é—´çš„å…³ç³»\nCUDA kernel çš„ç¼–ç¨‹æ¨¡å‹\n[Todo] Dim and size detail\nè°ƒç”¨kernel: add \u0026lt;\u0026lt; \u0026lt;blockNumber, threadNumber \u0026gt;\u0026gt; \u0026gt; (dx);\nç¼–å†™kernelï¼š\nç”¨å…³é”®å­—æè¿°ç¬¦ __global__ å£°æ˜kernel: __global__ void add(){} è°ƒç”¨ kernel æ—¶çš„å‚æ•° \u0026lt;\u0026lt;\u0026lt;blockNumber per grid, threadNumber per block\u0026gt;\u0026gt;\u0026gt; å†³å®šäº†å…±æœ‰ TotalThreadNum = blockNumber * threadNumber ä¸ªçº¿ç¨‹å¯ä»¥å¹¶è¡Œæ‰§è¡Œä»»åŠ¡ kernel å†…çš„æ¯ä¸€æ¬¡è¿­ä»£ï¼Œæ„å‘³ç€ TotalThreadNum ä¸ªçº¿ç¨‹å¹¶è¡Œæ‰§è¡Œäº†ä¸€æ¬¡å¾ªç¯ä½“ä¸­çš„ä»»åŠ¡ï¼ˆå³æ¯ä¸ªçº¿ç¨‹å®Œæˆå¯¹ä¸€ä»½æ•°æ®çš„å¤„ç†ï¼‰ï¼Œä¹Ÿå°±æ˜¯æ¯æ¬¡è¿­ä»£èƒ½å¤„ç† TotalThreadNum ä»½æ•°æ®ï¼ŒTotalThreadNum ä¹Ÿç­‰ä»·äºè·¨æ­¥(stride)çš„å¤§å° kernel ä¸­ threadIdx.x ä»£è¡¨ the index of the thread within the blockï¼Œ blockDim.x ä»£è¡¨ the size of blockï¼ˆnumber of threads in blockï¼ˆå‡è®¾ è¿™é‡Œçš„ grid å’Œ block çš„ dim åªæœ‰ä¸€ç»´ï¼‰ kernel å†… threadIdx.x å’Œ blockIdx.x çš„ç»„åˆå¯¹åº”çº¿ç¨‹çš„å”¯ä¸€æ ‡è¯† ä»¥add_3è¿™ä¸ª kernel ä¸ºä¾‹ï¼Œå¯ä»¥ç”¨ index = blockIdx.x * blockDim.x + threadIdx.x è·å¾—å½“å‰çº¿ç¨‹çš„è¦å¤„ç†çš„æ•°æ®çš„æ•°ç»„ä¸‹æ ‡ï¼ˆè§ä¸‹å›¾ï¼‰ï¼Œ\n__global__ void add_3(int n, float *x, float *y) { int index = blockIdx.x * blockDim.x + threadIdx.x; // strideä¸ºgridçš„çº¿ç¨‹æ€»æ•°:blockDim.x*gridDim.x int stride = blockDim.x * gridDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] = x[i] + y[i]; } Kernel examples(ä¸‹é¢çš„ C++ Example)\n1ä¸ªblock,1ä¸ªçº¿ç¨‹: add_1() 1ä¸ªblock,å¤šä¸ªçº¿ç¨‹: add_2() å¤šä¸ªblock,å¤šä¸ªçº¿ç¨‹: add_3() å¤šä¸ªblockï¼Œå¤šä¸ªçº¿ç¨‹ä¹Ÿç§°ä¸ºç½‘æ ¼è·¨æ­¥å¾ªç¯ï¼Œå…¶ä¸­æ¯æ¬¡å¾ªç¯çš„è·¨æ­¥(stride)ä¸º grid çš„çº¿ç¨‹æ€»æ•°: stride = blockDim.x * gridDim.x\nC++ Example:\n// file: add.cu #include \u0026lt;iostream\u0026gt; #include \u0026lt;math.h\u0026gt; // Kernel function to add the elements of two arrays // single thread __global__ void add_1(int n, float *x, float *y) { for (int i = 0; i \u0026lt; n; i++) y[i] = x[i] + y[i]; } // single block, multi threads __global__ void add_2(int n, float *x, float *y) { int index = threadIdx.x; int stride = blockDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] = x[i] + y[i]; } // multi block, multi threads // ç½‘æ ¼è·¨æ­¥(stride)å¾ªç¯ __global__ void add_3(int n, float *x, float *y) { int index = blockIdx.x * blockDim.x + threadIdx.x; // strideä¸ºgridçš„çº¿ç¨‹æ€»æ•°:blockDim.x*gridDim.x int stride = blockDim.x * gridDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] = x[i] + y[i]; } int main(void) { int N = 1\u0026lt;\u0026lt;20; float *x, *y; // Allocate Unified Memory â€“ accessible from CPU or GPU cudaMallocManaged(\u0026amp;x, N*sizeof(float)); cudaMallocManaged(\u0026amp;y, N*sizeof(float)); // initialize x and y arrays on the host for (int i = 0; i \u0026lt; N; i++) { x[i] = 1.0f; y[i] = 2.0f; } // Run kernel on 1M elements on the GPU // Just run with single thread :) add_1\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Run with 1 block and multi threads add_2\u0026lt;\u0026lt;\u0026lt;1, 256\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Run with multi block and multi threads int blockSize = 256;//å¹¶è¡Œçº¿ç¨‹æ•°é‡ int numBlocks = (N + blockSize - 1) / blockSize;//çº¿ç¨‹å—æ•°é‡ add_3\u0026lt;\u0026lt;\u0026lt;numBlocks, blockSize\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Wait for GPU to finish before accessing on host cudaDeviceSynchronize(); // Check for errors (all values should be 3.0f) float maxError = 0.0f; for (int i = 0; i \u0026lt; N; i++) maxError = fmax(maxError, fabs(y[i]-3.0f)); std::cout \u0026lt;\u0026lt; \u0026#34;Max error: \u0026#34; \u0026lt;\u0026lt; maxError \u0026lt;\u0026lt; std::endl; // Free memory cudaFree(x); cudaFree(y); return 0; } CUDA Kernel for Conv # CUDA Code Profiling # nvprofæ˜¯CUDAå·¥å…·åŒ…é™„å¸¦çš„å‘½ä»¤è¡ŒGPUåˆ†æå™¨\nReference:\nNVIDIA CUDA Docs ","date":"2024-05-24","externalUrl":null,"permalink":"/posts/cuda/cuda_1/","section":"Blogs","summary":"Steps of CUDA Program # Config GPU(device), cudaSetDevice() Allocate device memory, cudaMalloc(), cudaMallocManaged() Allocate CPU(Host) memory Copy data from host","title":"CUDA Programming","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/tags/gpu/","section":"Tags","summary":"","title":"GPU","type":"tags"},{"content":" CPU ä¸ GPU çš„ä¸åŒ # CPUï¼Œ4ä¸ª ALUï¼Œä¸»è¦è´Ÿè´£é€»è¾‘è®¡ç®—ï¼Œ1ä¸ªæ§åˆ¶å•å…ƒ Controlï¼Œ1ä¸ª DRAMï¼Œ1ä¸ª Cache GPUï¼Œç»¿è‰²å°æ–¹å—çœ‹ä½œ ALUï¼Œçº¢è‰²æ¡†çœ‹ä½œä¸€ä¸ª SMï¼ŒSM ä¸­çš„å¤šä¸ª ALU share ä¸€ä¸ªControl å’Œ Cacheï¼ŒSM å¯ä»¥çœ‹ä½œä¸€ä¸ªå¤šæ ¸ CPUï¼Œä½†æ˜¯ ALU æ›´å¤šï¼Œcontrol æ›´å°‘ï¼Œä¹Ÿå°±æ˜¯ç®—åŠ›æå‡ï¼Œæ§åˆ¶åŠ›å‡å¼± æ‰€ä»¥ï¼ŒCPU é€‚åˆæ§åˆ¶é€»è¾‘å¤æ‚çš„ä»»åŠ¡ï¼ŒGPU é€‚åˆé€»è¾‘ç®€å•ã€æ•°æ®é‡å¤§ã€è®¡ç®—é‡å¤§çš„ä»»åŠ¡ã€‚\nGPU, CUDA, AI Framework çš„å…³ç³» # Reference:\nNVIDIA CUDA Docs cudaç¼–ç¨‹å­¦ä¹  ä¸€å¼ å›¾äº†è§£GPUã€CUDAã€CUDA toolkitå’Œpytorchçš„å…³ç³» ","date":"2024-05-24","externalUrl":null,"permalink":"/posts/cuda/gpu/","section":"Blogs","summary":"CPU ä¸ GPU çš„ä¸åŒ # CPUï¼Œ4ä¸ª ALUï¼Œä¸»è¦è´Ÿ","title":"GPU ç»“æ„","type":"posts"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/attention/","section":"Tags","summary":"","title":"Attention","type":"tags"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/series/attention/","section":"Series","summary":"","title":"Attention","type":"series"},{"content":"[WIP]\nFlashAttention V2:\nåœ¨V1çš„åŸºç¡€ä¸Šå‡å°‘äº†éçŸ©é˜µä¹˜æ³•è¿ç®—çš„FLOPsã€‚ é€šè¿‡å¹¶è¡ŒåŒ–å’Œä»»åŠ¡åˆ†é…ä¼˜åŒ–æé«˜äº†è®¡ç®—é€Ÿåº¦å’ŒGPUåˆ©ç”¨ç‡ï¼Œæ€§èƒ½æå‡äº†2-3å€ã€‚ Flash-Decodingå€Ÿé‰´äº†FlashAttentionçš„ä¼˜ç‚¹ï¼Œå°†å¹¶è¡ŒåŒ–ç»´åº¦æ‰©å±•åˆ°keys/valuesåºåˆ—é•¿åº¦ï¼Œæé«˜äº†æ¨ç†é€Ÿåº¦ã€‚ Flash-Decodingå‡ ä¹ä¸ç”¨é¢å¤–å­˜å‚¨å¤§é‡æ•°æ®åˆ°å…¨å±€å†…å­˜ä¸­ï¼Œå‡å°‘äº†å†…å­˜å¼€é”€ã€‚ Flash-Decoding++é€šè¿‡å¼‚æ­¥softmaxå’Œç»Ÿä¸€æœ€å¤§å€¼ã€flat GEMMä¼˜åŒ–å’ŒåŒç¼“å†²ã€å¯å‘å¼æ•°æ®æµå’Œç¡¬ä»¶èµ„æºé€‚åº”ç­‰æ–¹æ³•è¿›ä¸€æ­¥æé«˜äº†LLMæ¨ç†çš„æ€§èƒ½ã€‚ ","date":"2024-05-23","externalUrl":null,"permalink":"/posts/llm/flash_attention_2/","section":"Blogs","summary":"[WIP] FlashAttention V2: åœ¨V1çš„åŸºç¡€ä¸Šå‡å°‘äº†éçŸ©é˜µä¹˜æ³•è¿ç®—","title":"Flash Attention V2","type":"posts"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/posts/llm/","section":"Blogs","summary":"","title":"LLM","type":"posts"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/nlp/","section":"Tags","summary":"","title":"NLP","type":"tags"},{"content":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\nä¸æ ‡å‡† attention ç›¸æ¯”ï¼ŒFlash Attention æœ‰ä»¥ä¸‹ä¸‰ç‚¹ç‰¹ç‚¹ï¼š\nè¿ç®—é€Ÿåº¦æ›´å¿« (Fast) æ›´èŠ‚çœæ˜¾å­˜ (Memory-Efficient) è®¡ç®—ç»“æœç›¸åŒ (Exact) FlashAttention ç›®çš„ä¸æ˜¯èŠ‚çº¦ FLOPsï¼Œè€Œæ˜¯å‡å°‘å¯¹HBMçš„è®¿é—®ã€‚å®ƒæ²¡æœ‰æ”¹å˜åŸæœ‰çš„è®¡ç®—å…¬å¼ï¼Œæ•´ä½“è®¡ç®—å¤æ‚åº¦å¹¶æœªé™ä½ã€‚\nèƒŒæ™¯ # GPUä¸­å­˜å‚¨å•å…ƒä¸»è¦æœ‰ HBM å’Œ SRAMï¼šHBM å®¹é‡å¤§ä½†æ˜¯è®¿é—®é€Ÿåº¦æ…¢ï¼ŒSRAMå®¹é‡å°å´æœ‰ç€è¾ƒé«˜çš„è®¿é—®é€Ÿåº¦ã€‚ä¾‹å¦‚ï¼šA100 GPUæœ‰40-80GBçš„HBMï¼Œå¸¦å®½ä¸º1.5-2.0TB/sï¼›æ¯108ä¸ªæµå¼å¤šæ ¸å¤„ç†å™¨å„æœ‰192KBçš„ç‰‡ä¸ŠSRAMï¼Œå¸¦å®½ä¼°è®¡çº¦ä¸º 19TB/sã€‚å¯ä»¥çœ‹å‡ºï¼Œç‰‡ä¸Šçš„SRAMæ¯”HBMå¿«ä¸€ä¸ªæ•°é‡çº§ï¼Œä½†å°ºå¯¸è¦å°è®¸å¤šæ•°é‡çº§ã€‚\nå½“è¾“å…¥åºåˆ—ï¼ˆsequence lengthï¼‰è¾ƒé•¿æ—¶ï¼ŒTransformerçš„è®¡ç®—è¿‡ç¨‹ç¼“æ…¢ä¸”è€—è´¹å†…å­˜ï¼Œè¿™æ˜¯å› ä¸º self-attention çš„ time å’Œ memory complexity ä¼šéšç€ sequence length çš„å¢åŠ æˆäºŒæ¬¡å¢é•¿ã€‚\næ ‡å‡†Attentionçš„è®¡ç®—è¿‡ç¨‹ï¼š $$ S=Q K^T \\in \\mathbb{R}^{N \\times N} $$ $$ P=\\operatorname{softmax}(S) \\in \\mathbb{R}^{N \\times N} $$ $$ O=P V \\in \\mathbb{R}^{N \\times N} $$\næ ‡å‡†Attentionçš„ä¸­é—´ç»“æœ ğ‘†, ğ‘ƒ é€šå¸¸éœ€è¦é€šè¿‡é«˜å¸¦å®½å†…å­˜ï¼ˆHBMï¼‰è¿›è¡Œå­˜å–ï¼Œä¸¤è€…æ‰€éœ€å†…å­˜ç©ºé—´å¤æ‚åº¦ä¸º\\(O(Nd+N^2)), å¯¹ HBM çš„é‡å¤è¯»å†™æ˜¯ä¸»è¦ç“¶é¢ˆã€‚è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œéœ€è¦åšä¸¤ä»¶äº‹ï¼š\nåœ¨ä¸è®¿é—®æ•´ä¸ªè¾“å…¥çš„æƒ…å†µä¸‹è®¡ç®— softmax ä¸ä¸ºåå‘ä¼ æ’­å­˜å‚¨å¤§çš„ä¸­é—´ attention çŸ©é˜µ(\\(N^2\\)) FlashAttention V1 # FlashAttention æå‡ºäº†ä¸¤ç§æ–¹æ³•æ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼štiling å’Œ recomputationã€‚\ntiling - æ³¨æ„åŠ›è®¡ç®—è¢«é‡æ–°æ„é€ ï¼Œå°†è¾“å…¥åˆ†å‰²æˆå—ï¼Œå¹¶é€šè¿‡åœ¨è¾“å…¥å—ä¸Šè¿›è¡Œå¤šæ¬¡ä¼ é€’æ¥é€’å¢åœ°æ‰§è¡Œsoftmaxæ“ä½œã€‚ recomputation - å­˜å‚¨æ¥è‡ªå‰å‘çš„ softmax å½’ä¸€åŒ–å› å­ï¼Œä»¥ä¾¿åœ¨åå‘ä¸­å¿«é€Ÿé‡æ–°è®¡ç®—èŠ¯ç‰‡ä¸Šçš„ attentionï¼Œè¿™æ¯”ä»HBMè¯»å–ä¸­é—´çŸ©é˜µçš„æ ‡å‡†æ³¨æ„åŠ›æ–¹æ³•æ›´å¿«ã€‚å¯ä»¥æŠŠå®ƒçœ‹ä½œåŸºäº tiling çš„ç‰¹æ®Šçš„ gradient checkpointing æ­£å¸¸çš„softmaxè®¡ç®—ï¼š\n$$m(x):=\\max _i x i$$ $$f(x):=\\left[e^{x_1-m(x)} \\ldots e^{x_B-m(x)}\\right]$$ $$\\ell(x):=\\sum_i f(x)_i$$ $$\\operatorname{softmax}(x):=\\frac{f(x)}{\\ell(x)}$$\nsoftmax ä¼ªä»£ç : softmax å‡½æ•°éœ€è¦ä¸‰ä¸ªå¾ªç¯ï¼Œç¬¬ä¸€ä¸ªå¾ªç¯è®¡ç®—æ•°ç»„çš„æœ€å¤§å€¼ï¼Œç¬¬äºŒä¸ªå¾ªç¯è®¡ç®— softmax çš„åˆ†æ¯ï¼Œç¬¬ä¸‰ä¸ªå¾ªç¯è®¡ç®— softmax è¾“å‡ºã€‚\nåˆ†å—çš„ softmax è®¡ç®—(å‡è®¾åˆ†2å—å¹¶è¡Œè®¡ç®—)ï¼š\n$$m(x)=m\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)=\\max \\left(m\\left(x^{(1)}\\right), m\\left(x^{(2)}\\right)\\right) $$ $$f(x)=\\left[e^{m\\left(x^{(1)}\\right)-m(x)} f\\left(x^{(1)}\\right) \\quad e^{m\\left(x^{(2)}\\right)-m(x)} f\\left(x^{(2)}\\right)\\right] $$ $$\\ell(x)=\\ell\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)=e^{m\\left(x^{(1)}\\right)-m(x)} \\ell\\left(x^{(1)}\\right)+e^{m\\left(x^{(2)}\\right)-m(x)} \\ell\\left(x^{(2)}\\right) $$ $$\\operatorname{softmax}(x)=\\frac{f(x)}{\\ell(x)}$$\nåˆ†å—çš„ softmax ä¼ªä»£ç : åœ¨ç¬¬ä¸€ä¸ªå¾ªç¯ä¸­åŒæ—¶å¯¹æœ€å¤§å€¼\\(m\\)ä»¥åŠ softmax çš„åˆ†æ¯\\(d\\)è¿›è¡Œæ›´æ–°ï¼Œä»è€Œå‡å°‘äº†ä¸€ä¸ªå¾ªç¯ã€‚é€šè¿‡ tiling çš„æ–¹å¼ï¼Œsoftmax çš„å¾ªç¯æ•°ä»ä¸‰ä¸ªå‡åˆ°äº†ä¸¤ä¸ªï¼Œä»è€Œå¯ä»¥é™ä½å†…å­˜æ¶ˆè€—ã€‚\nflashattention ä¼ªä»£ç ï¼š ä¸­é—´å˜é‡ï¼š\\(O_i\\)(æœ€ç»ˆä¹˜ç§¯)ã€\\(l_i\\)ï¼ˆsoftmaxçš„åˆ†æ¯ï¼Œå³ç´¯åŠ å’Œï¼‰ã€\\(m_i\\)ï¼ˆéå†åˆ°å½“å‰å—ä¸ºæ­¢çš„æœ€å¤§å€¼ï¼‰ï¼Œå†ä¹Ÿä¸ç”¨ä¿å­˜å…¨éƒ¨çš„Så’ŒPäº†ã€‚\nç”±äºé‡æ–°è®¡ç®—å¯¼è‡´ FLOPs å¢åŠ ï¼Œä½†æ˜¯ç”±äºå¤§é‡å‡å°‘HBMè®¿é—®ï¼ŒFlashAttention è¿è¡Œé€Ÿåº¦æ›´å¿«\nFlashAttentionçš„ FLOPs ä¸º \\(ğ‘‚(ğ‘^2ğ‘‘)\\)ï¼Œé™¤äº† input å’Œ outputï¼Œé¢å¤–éœ€è¦çš„å†…å­˜ä¸º \\(ğ‘‚(ğ‘)\\), å¯¹HBMè®¿é—®çš„æ¬¡æ•°ä¸º \\(ğ‘‚(ğ‘^2ğ‘‘^2ğ‘€^{âˆ’1})\\), æ¯”æ ‡å‡† Attention çš„ \\(O(Nd+N^2)\\)æ›´é«˜æ•ˆ\nPyTorch 2.0å·²å°† FlashAttention é›†æˆåˆ°å®˜æ–¹åº“ä¸­ï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨ torch.nn.functional.scaled_dot_product_attention\næ€»ç»“ # FlashAttention V1:\né€šè¿‡åˆ‡å—æŠ€æœ¯å‡å°‘äº†å†…å­˜è®¿é—®æ¬¡æ•°ï¼Œæé«˜äº†è®¡ç®—é€Ÿåº¦å’Œå†…å­˜åˆ©ç”¨ç‡ã€‚ å†…å­˜è®¿é—®å¤æ‚åº¦ä¸º \\(ğ‘‚(ğ‘^2ğ‘‘^2ğ‘€^{âˆ’1})\\), æ¯”æ ‡å‡† Attention çš„ \\(O(Nd+N^2)\\)æ›´é«˜æ•ˆ ","date":"2024-05-23","externalUrl":null,"permalink":"/posts/llm/flash_attention/","section":"Blogs","summary":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness ä¸æ ‡å‡† attention ç›¸æ¯”ï¼ŒFlash","title":"Flash Attention","type":"posts"},{"content":"class Me: def __init__(self): self.names = \u0026#34;Victor Yang\u0026#34; self.prefer_name = \u0026#34;Victor\u0026#34; self.profession = \u0026#34;Programmer\u0026#34; self.hometown = \u0026#34;Shandong, CN\u0026#34; self.current_location = \u0026#34;Beijing, CN\u0026#34; self.grad_school = \u0026#34;USTC\u0026#34; self.undergrad_school = \u0026#34;UPC\u0026#34; self.interests = [\u0026#34;ML\u0026#34;, \u0026#34;Robot\u0026#34;] self.hoppies = [\u0026#34;Reading\u0026#34;, \u0026#34;Yoga\u0026#34;, \u0026#34;Meditation\u0026#34;, \u0026#34;Taoist\u0026#34;, \u0026#34;Swimming\u0026#34;] ","date":"2024-05-19","externalUrl":null,"permalink":"/about/","section":"Welcome to Blowfish! ğŸ‰","summary":"class Me: def __init__(self): self.names = \u0026#34;Victor Yang\u0026#34; self.prefer_name = \u0026#34;Victor\u0026#34; self.profession = \u0026#34;Programmer\u0026#34; self.hometown = \u0026#34;Shandong, CN\u0026#34; self.current_location = \u0026#34;Beijing, CN\u0026#34; self.","title":"About","type":"page"},{"content":"This is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing. ğŸš€\nYou can also use these content pages to define Hugo metadata like titles and descriptions that will be used for SEO and other purposes.\n","date":"0001-01-01","externalUrl":null,"permalink":"/tags/advanced/","section":"Tags","summary":"This is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing.","title":"Advanced","type":"tags"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/posts/ai-framework/","section":"Blogs","summary":"","title":"AI Framework","type":"posts"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/tags/_index.zh-cn/","section":"Tags","summary":"","title":"æ ‡ç­¾","type":"tags"},{"content":"Hell\n","date":"0001-01-01","externalUrl":null,"permalink":"/_index.zh-cn/","section":"Welcome to Blowfish! ğŸ‰","summary":"Hell","title":"æ¬¢è¿æ¥åˆ° Blowfish! ğŸ‰","type":"page"},{"content":"è¿™æ˜¯é«˜çº§æ ‡è®°ã€‚ç±»ä¼¼å…¶ä»– Blowfish ä¸­çš„å…¶ä»–åˆ—è¡¨é¡µé¢ï¼Œä½ å¯ä»¥åœ¨åˆ†ç±»åˆ—è¡¨é¡µæ·»åŠ è‡ªå®šä¹‰å†…å®¹ï¼Œè¿™éƒ¨åˆ†å†…å®¹ä¼šæ˜¾ç¤ºåœ¨é¡¶éƒ¨ã€‚ğŸš€\nä½ ä¹Ÿå¯ä»¥ç”¨è¿™äº›å†…å®¹æ¥å®šä¹‰ Hugo çš„å…ƒæ•°æ®ï¼Œæ¯”å¦‚æ ‡é¢˜å’Œæè¿°ã€‚è¿™äº›å†…å®¹å¯ä»¥è¢«ç”¨æ¥å¢å¼º SEO æˆ–å…¶ä»–ç›®çš„ã€‚\n","date":"0001-01-01","externalUrl":null,"permalink":"/tags/advanced/_index.zh-cn/","section":"Tags","summary":"è¿™æ˜¯é«˜çº§æ ‡è®°ã€‚ç±»ä¼¼å…¶ä»– Blowfish ä¸­çš„å…¶ä»–åˆ—è¡¨é¡µé¢","title":"é«˜çº§","type":"tags"}]