
[{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/posts/","section":"Blogs","summary":"","title":"Blogs","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/tags/cuda/","section":"Tags","summary":"","title":"CUDA","type":"tags"},{"content":"[Todo]\n","date":"2024-05-24","externalUrl":null,"permalink":"/posts/cuda/cuda_optimize/","section":"Blogs","summary":"[Todo]","title":"CUDA Optimization","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/series/cuda-parallel-programming/","section":"Series","summary":"","title":"CUDA Parallel Programming","type":"series"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/posts/cuda/","section":"Blogs","summary":"","title":"CUDA Parallel Programming","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"Hi, welcome to my blog.\n","date":"2024-05-24","externalUrl":null,"permalink":"/","section":"Welcome to My Blog","summary":"Hi, welcome to my blog.","title":"Welcome to My Blog","type":"page"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/attention/","section":"Tags","summary":"","title":"Attention","type":"tags"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/series/attention/","section":"Series","summary":"","title":"Attention","type":"series"},{"content":"[WIP]\nFlashAttention V2:\nåœ¨V1çš„åŸºç¡€ä¸Šå‡å°‘äº†éçŸ©é˜µä¹˜æ³•è¿ç®—çš„FLOPsã€‚ é€šè¿‡å¹¶è¡ŒåŒ–å’Œä»»åŠ¡åˆ†é…ä¼˜åŒ–æé«˜äº†è®¡ç®—é€Ÿåº¦å’ŒGPUåˆ©ç”¨ç‡ï¼Œæ€§èƒ½æå‡äº†2-3å€ã€‚ Flash-Decodingå€Ÿé‰´äº†FlashAttentionçš„ä¼˜ç‚¹ï¼Œå°†å¹¶è¡ŒåŒ–ç»´åº¦æ‰©å±•åˆ°keys/valuesåºåˆ—é•¿åº¦ï¼Œæé«˜äº†æ¨ç†é€Ÿåº¦ã€‚ Flash-Decodingå‡ ä¹ä¸ç”¨é¢å¤–å­˜å‚¨å¤§é‡æ•°æ®åˆ°å…¨å±€å†…å­˜ä¸­ï¼Œå‡å°‘äº†å†…å­˜å¼€é”€ã€‚ Flash-Decoding++é€šè¿‡å¼‚æ­¥softmaxå’Œç»Ÿä¸€æœ€å¤§å€¼ã€flat GEMMä¼˜åŒ–å’ŒåŒç¼“å†²ã€å¯å‘å¼æ•°æ®æµå’Œç¡¬ä»¶èµ„æºé€‚åº”ç­‰æ–¹æ³•è¿›ä¸€æ­¥æé«˜äº†LLMæ¨ç†çš„æ€§èƒ½ã€‚ ","date":"2024-05-23","externalUrl":null,"permalink":"/posts/llm/flash_attention_2/","section":"Blogs","summary":"[WIP] FlashAttention V2: åœ¨V1çš„åŸºç¡€ä¸Šå‡å°‘äº†éçŸ©é˜µä¹˜æ³•è¿ç®—","title":"Flash Attention V2","type":"posts"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/posts/llm/","section":"Blogs","summary":"","title":"LLM","type":"posts"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/nlp/","section":"Tags","summary":"","title":"NLP","type":"tags"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/transformer/","section":"Tags","summary":"","title":"Transformer","type":"tags"},{"content":"","date":"2024-05-21","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"2024-05-21","externalUrl":null,"permalink":"/tags/pytorch/","section":"Tags","summary":"","title":"Pytorch","type":"tags"},{"content":"","date":"2024-05-21","externalUrl":null,"permalink":"/series/pytorch/","section":"Series","summary":"","title":"PyTorch","type":"series"},{"content":"","date":"2024-05-21","externalUrl":null,"permalink":"/posts/pytorch/","section":"Blogs","summary":"","title":"PyTorch","type":"posts"},{"content":" PyTorch ä»£ç ç»“æ„ # PyTroch ä¸»è¦ç”±C10ã€ATenã€torchä¸‰å¤§éƒ¨åˆ†ç»„æˆï¼š\ntorch/Â ä¸‹åŒ…å« import å’Œä½¿ç”¨çš„ Python æ¨¡å— torch/csrc/Â åŒ…å«äº† PyTorch å‰ç«¯çš„ C++ ä»£ç åŠC++å‰ç«¯ä»£ç ã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒåŒ…å«äº† Python å’Œ C++ ä¹‹é—´è½¬æ¢çš„bindingä»£ç ï¼Œ autograd å¼•æ“å’Œ JIT ç¼–è¯‘å™¨ç­‰ã€‚ c10(CaffeÂ Tensor Library), åŒ…å« PyTorch çš„æ ¸å¿ƒæŠ½è±¡ï¼Œå­˜æ”¾æœ€åŸºç¡€çš„Tensoråº“ä»£ç ï¼ŒåŒ…æ‹¬ Tensor å’Œ Storage æ•°æ®ç»“æ„çš„å®é™…å®ç°ï¼Œå¯ä»¥è¿è¡Œåœ¨æœåŠ¡ç«¯å’Œç§»åŠ¨ç«¯ã€‚ æœ€å…·ä»£è¡¨æ€§çš„classæ˜¯ TensorImpl ï¼Œå®ç°äº†Tensorçš„æœ€åŸºç¡€æ¡†æ¶ã€‚ç»§æ‰¿è€…å’Œä½¿ç”¨è€…æœ‰ï¼š Variableçš„Variable::Impl SparseTensorImpl detail::make_tensor(storage_impl, CUDATensorId(), false) Tensor(c10::intrusive_ptr\u0026lt;TensorImpl, UndefinedTensorImpl\u0026gt; tensor_impl) c10::make_intrusive\u0026lt;at::TensorImpl, at::UndefinedTensorImpl\u0026gt; ATen(A Tensor library for C++11)ï¼ŒåŒ…å«å£°æ˜å’Œå®šä¹‰ Tensor è¿ç®—ç›¸å…³é€»è¾‘çš„ä»£ç ï¼Œæ˜¯å®ç°å¼ é‡è¿ç®—çš„ C++ åº“ï¼Œkernelä»£ç å¤§å¤šåœ¨è¿™é‡Œ åŒ…å« C++ å®ç°çš„nativeç®—å­å’Œ C å®ç°çš„legacyç®—å­(TH, THC, THNN, THCUNN) . aten/src/ATen/gen.py ç”¨æ¥åŠ¨æ€ç”Ÿæˆä¸€äº›ATenç›¸å…³çš„ä»£ç  PyTroch çš„ç¼–è¯‘è¿‡ç¨‹ # å…¥å£ setup.pyï¼› æå‰æ£€æŸ¥ä¾èµ–é¡¹ï¼› ä½¿ç”¨ cmake ç”Ÿæˆ Makefile Make: äº§ç”Ÿä¸­é—´æºæ–‡ä»¶ Make: ç¼–è¯‘ä¸‰æ–¹åº“ Make: ç”Ÿæˆé™æ€åº“ã€åŠ¨æ€åº“ã€å¯æ‰§è¡Œæ–‡ä»¶ Make: Copyæ–‡ä»¶åˆ°åˆé€‚è·¯å¾„ setuptools, build_py setuptools, build_ext setuptools, install_lib PyTorch å·¥ä½œæµå’Œè®¡ç®—å›¾ # PyTorch 1.0 æ•´ä½“å·¥ä½œæµï¼š\nä½¿ç”¨ imperative / eager çš„èŒƒå¼ï¼Œæ¯ä¸€è¡Œä»£ç éƒ½æ„å»ºä¸€ä¸ªå›¾ä½œä¸ºå®Œæ•´è®¡ç®—å›¾çš„ä¸€éƒ¨åˆ†ã€‚å³ä½¿å®Œæ•´çš„è®¡ç®—å›¾è¿˜æ²¡æœ‰å®Œæˆæ„å»ºï¼Œä¹Ÿå¯ä»¥ç‹¬ç«‹æ‰§è¡Œè¿™äº›ä½œä¸ºç»„ä»¶çš„å°è®¡ç®—å›¾ï¼Œè¿™ç§åŠ¨æ€è®¡ç®—å›¾è¢«ç§°ä¸ºdefine-by-run Eager æ¨¡å¼é€‚åˆå—åšåŸå‹ã€å®éªŒã€debugï¼ŒScript æ¨¡å¼(torch.jit)é€‚åˆåšä¼˜åŒ–ä¸éƒ¨ç½² åŠ¨æ€å›¾ # å‡è®¾PyTorchçš„autogradç³»ç»Ÿæ˜¯ä¸€ä¸ª graphï¼Œé‚£ä¹ˆæ¯ä¸ª Function å®ä¾‹å°±æ˜¯ graph ä¸­çš„èŠ‚ç‚¹ï¼Œå„ä¸ª Function å®ä¾‹ä¹‹é—´é€šè¿‡ Edge è¿æ¥ã€‚Edge æ˜¯ä¸ª structï¼Œ(Function, input_nr) ç»„åˆå¯ä»¥ä»£è¡¨ä¸€ä¸ª edge\nstruct Edge { ... std::shared_ptr\u0026lt;Function\u0026gt; function; uint32_t input_nr; }; Function çš„æˆå‘˜å˜é‡ next_edges_ å°±æ˜¯ä¸€ç»„ Edge å®ä¾‹ï¼Œä»£è¡¨å½“å‰Functionå®ä¾‹çš„è¿”å›å€¼è¦è¾“å‡ºåˆ°å“ªä¸ªFunction\nFunction çš„ input, ouput éƒ½æ˜¯ Variableå®ä¾‹ï¼Œå› æ­¤ï¼Œå½“ä¸€ä¸ª graph è¢«æ‰§è¡Œæ—¶ï¼ŒVariable å®ä¾‹å°±åœ¨è¿™äº› edge ä¹‹é—´æ¥æµåŠ¨ï¼Œä¼ è¾“ä¿¡æ¯\nFunction çš„æˆå‘˜å˜é‡ sequence numberï¼Œéšç€Functionå®ä¾‹çš„ä¸æ–­æ„å»ºè€Œå•è°ƒå¢é•¿\nJIT # Code/AST -\u0026gt; Parsing-\u0026gt; Checking -\u0026gt; Optimization -\u0026gt; Translation -\u0026gt; Execution\nJIT ä¸»è¦ä¼šè¾“å…¥ä»£ç æˆ– Python çš„æŠ½è±¡å¥æ³•æ ‘ï¼ˆASTï¼‰ï¼Œå…¶ä¸­ AST ä¼šç”¨æ ‘ç»“æ„è¡¨å¾ Python æºä»£ç çš„å¥æ³•ç»“æ„ã€‚ Parsingå¯èƒ½æ˜¯è§£æå¥æ³•ç»“æ„å’Œè®¡ç®—å›¾ï¼Œç„¶åè¯­æ³•æ£€æµ‹æ¥è¿ç€ä»£ç ä¼˜åŒ–è¿‡ç¨‹ï¼Œæœ€ååªè¦ç¼–è¯‘å¹¶æ‰§è¡Œå°±å¯ä»¥ ä¼˜åŒ–è®¡ç®—å›¾ï¼Œå¦‚å±•å¼€å¾ªç¯ã€æŒ‡ä»¤è½¬æ¢ç­‰ æ‰§è¡Œï¼Œä¸ Python è§£é‡Šå™¨å¯ä»¥æ‰§è¡Œä»£ç ä¸€æ ·ï¼ŒPyTorch JIT è¿‡ç¨‹ä¸­ä¹Ÿæœ‰ä¸€ä¸ªè§£é‡Šå™¨æ‰§è¡Œä¸­é—´è¡¨å¾æŒ‡ä»¤ PyTorch ä» Python ä»£ç åˆ° kernel # PyTorch ä» Python ä»£ç åˆ° kernel çš„ä¸­é—´è¿‡ç¨‹ååˆ†å¤æ‚, åœ¨è¿›å…¥å†…æ ¸ä¹‹å‰ï¼Œæ‰€æœ‰ä»£ç éƒ½æ˜¯è‡ªåŠ¨ç”Ÿæˆçš„\nå‡è®¾è°ƒç”¨ torch.add()ï¼Œæµç¨‹å¦‚ä¸‹ï¼š\nPython åŸŸè½¬æ¢åˆ° C++ åŸŸï¼ˆPython å‚æ•°è§£æï¼‰ å¤„ç† VariableType dispatch å¤„ç† DeviceType/å¸ƒå±€ dispatch æ‰§è¡Œkernel(native kernel æˆ– TH kernel) ATen åŠ¨æ€ç”Ÿæˆçš„ä»£ç  # Typeç»§æ‰¿ä½“ç³»ï¼ŒåŒ…å«å¤´æ–‡ä»¶å’Œæºæ–‡ä»¶ Typeç»§æ‰¿ä½“ç³»æ˜¯è”ç³» Tensor op ä¸ legacy çš„ TH æˆ– native kernel çš„çº½å¸¦ Typeç»§æ‰¿ä½“ç³»ç»´æŠ¤äº†2/3çº§åˆ†å‘æœºåˆ¶ Declarations.yamlï¼Œä¼šè¢«Torchæ¨¡å—åŠ¨æ€ç”Ÿæˆä»£ç è°ƒç”¨ ç”Ÿæˆ Tensor ç±» ç”ŸæˆTypeå®¶æ—æ³¨å†Œåˆå§‹åŒ–çš„ä»£ç  ç”Ÿæˆ legacy çš„ TH/THC çš„kernelå£°æ˜ ç”Ÿæˆ native kernel çš„å£°æ˜ PyTroch Tensor # #åœ¨pythonä¸­å®šä¹‰äº†Parameterç±» class Parameter(torch.Tensor) #åœ¨pythonä¸­å®šä¹‰äº†torch.Tensorç±» class Tensor(torch._C._TensorBase) #åœ¨C++ä¸­å®šä¹‰äº†Variableç±» struct TORCH_API Variable : public at::Tensor //PyObject* Py_InitModule(char *name, PyMethodDef *methods) //åˆ›å»ºtorch._C Py_InitModule(\u0026#34;torch._C\u0026#34;, methods.data()ï¼‰ //åˆ›å»º torch._C._TensorBase PyModule_AddObject(module, \u0026#34;_TensorBase\u0026#34;, (PyObject *)\u0026amp;THPVariableType); Tensorè¿ç®— Dispatch æœºåˆ¶ä¸­çš„ Type ç»§æ‰¿ä½“ç³» # Typeç±»æ´¾ç”Ÿå‡ºäº†TypeExtendedInterfaceï¼ŒTypeExtendedInterfaceåˆæ´¾ç”Ÿäº†TypeDefaultã€‚TypeDefaultåˆæ´¾ç”Ÿäº†CUDATypeDefaultã€CPUTypeDefaultã€VariableTypeï¼ˆå®ç°äº†autogradï¼‰ã€UndefinedTypeç­‰ã€‚å…¶ä¸­ï¼Œæ ¹æ® density å’Œ scaler type çš„ä¸åŒï¼š\nCUDATypeDefaultæ´¾ç”Ÿäº†ï¼š\nCUDAIntType CUDAShortType SparseCUDACharType CUDADoubleType CUDAByteType CUDACharType SparseCUDAByteType CUDAFloatType SparseCUDALongType CUDALongType CUDAHalfType SparseCUDAShortType SparseCUDADoubleType SparseCUDAIntType SparseCUDAFloatType CPUTypeDefaultæ´¾ç”Ÿäº†ï¼š\nSparseCPUShortType CPUFloatType CPUHalfType CPUDoubleType CPUByteType SparseCPUFloatType SparseCPUIntType SparseCPUDoubleType CPUCharType SparseCPUByteType CPUIntType CPULongType SparseCPULongType SparseCPUCharType CPUShortType Typeç»§æ‰¿ä½“ç³»çš„ä½œç”¨\nPyTorch Kernel ç»„æˆ # Error checking, TORCH CHECK Output allocation Dtype dispatch Parallelization Data access æœªå®Œå¾…ç»­\u0026hellip;\n","date":"2024-05-21","externalUrl":null,"permalink":"/posts/pytorch/pytorch/","section":"Blogs","summary":"PyTorch ä»£ç ç»“æ„ # PyTroch ä¸»è¦ç”±C10ã€ATenã€t","title":"PyTorch Architecture","type":"posts"},{"content":"","date":"2024-05-20","externalUrl":null,"permalink":"/posts/vllm/","section":"Blogs","summary":"","title":"PyTorch","type":"posts"},{"content":"","date":"2024-05-20","externalUrl":null,"permalink":"/tags/vllm/","section":"Tags","summary":"","title":"Vllm","type":"tags"},{"content":"","date":"2024-05-20","externalUrl":null,"permalink":"/series/vllm/","section":"Series","summary":"","title":"VLLM","type":"series"},{"content":" # æœªå®Œå¾…ç»­\u0026hellip;\n","date":"2024-05-20","externalUrl":null,"permalink":"/posts/vllm/vllm_1/","section":"Blogs","summary":"# æœªå®Œå¾…ç»­\u0026hellip;","title":"vLLM Decoding","type":"posts"},{"content":"","date":"2024-05-20","externalUrl":null,"permalink":"/tags/conv/","section":"Tags","summary":"","title":"Conv","type":"tags"},{"content":" CUDA Conv # åœ¨ PyTorch ä¸Šå®ç°Convå¾ˆç®€å•\nimport torch from torch.nn.functional import conv2d device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) import time width = 1000 height = 1000 img =torch.randn([width,height]) img = img.to(device) kernel = torch.tensor([[-1.0, 0.0, 1.0], [-1.0, 0.0, 1.0], [-1.0, 0.0, 1.0]]) img = torch.reshape(img, (1, 1, width, height)) kernel = torch.reshape(kernel, (1, 1, 3, 3)) kernel = kernel.to(device) start = time.perf_counter() output = F.conv2d(img, kernel, stride=1).to(device) end = time.perf_counter() print(f\u0026#39;total_cost: {end-start} ms\u0026#39;) print(f\u0026#39;output_size: {output.shape}\u0026#39;) print(f\u0026#39;output_tensor: {output}\u0026#39;) ç”¨CUDAå®ç°Convï¼ŒSteps:\nCheck Get thread info Memory allocation Copy data to device Call conv kernel Copy data to host Free memory //file: conv.cu #include\u0026lt;stdint.h\u0026gt; #include\u0026lt;cuda.h\u0026gt; #include \u0026#34;cuda_runtime.h\u0026#34; #include \u0026#34;device_launch_parameters.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;math.h\u0026gt; const int NUM_REPEATS = 10; #define CHECK(call) \\ do \\ { \\ const cudaError_t error_code = call; \\ if (error_code != cudaSuccess) \\ { \\ printf(\u0026#34;CUDA Error:\\n\u0026#34;); \\ printf(\u0026#34; File: %s\\n\u0026#34;, __FILE__); \\ printf(\u0026#34; Line: %d\\n\u0026#34;, __LINE__); \\ printf(\u0026#34; Error code: %d\\n\u0026#34;, error_code); \\ printf(\u0026#34; Error text: %s\\n\u0026#34;, \\ cudaGetErrorString(error_code)); \\ exit(1); \\ } \\ } while (0) static void HandleError(cudaError_t err, const char* file, int line) { if (err != cudaSuccess) { printf(\u0026#34;%s in %s at line %d\\n\u0026#34;, cudaGetErrorString(err), file, line); exit(EXIT_FAILURE); } } #define HANDLE_ERROR(err) (HandleError(err, __FILE__, __LINE__)) int getThreadNum() { cudaDeviceProp prop; int count; CHECK(cudaGetDeviceCount(\u0026amp;count)); printf(\u0026#34;GPU num: %d\\n\u0026#34;, count); CHECK(cudaGetDeviceProperties(\u0026amp;prop, 0)); printf(\u0026#34;Max thread num per block: %d\\n\u0026#34;, prop.maxThreadsPerBlock); printf(\u0026#34;Max grid dim: %d, %d, %d)\\n\u0026#34;, prop.maxGridSize[0], prop.maxGridSize[1], prop.maxGridSize[2]); return prop.maxThreadsPerBlock; } __global__ void conv(float* img, float* kernel, float* result, int width, int height, int kernelSize) { int ti = threadIdx.x; int bi = blockIdx.x; int id = (bi * blockDim.x + ti); if (id \u0026gt;= width * height) { return; } int row = id / width; int col = id % width; for (int i = 0; i \u0026lt; kernelSize; ++i) { for (int j = 0; j \u0026lt; kernelSize; ++j) { float imgValue = 0; int curRow = row - kernelSize / 2 + i; int curCol = col - kernelSize / 2 + j; if (curRow \u0026lt; 0 || curCol \u0026lt; 0 || curRow \u0026gt;= height || curCol \u0026gt;= width) { } else { imgValue = img[curRow * width + curCol]; } result[id] += kernel[i * kernelSize + j] * imgValue; } } } int main() { int width = 1000; int height = 1000; float* img = new float[width * height]; for (int row = 0; row \u0026lt; height; ++row) { for (int col = 0; col \u0026lt; width; ++col) { img[col + row * width] = (col + row) % 256; } } int kernelSize = 3; float* kernel = new float[kernelSize * kernelSize]; for (int i = 0; i \u0026lt; kernelSize * kernelSize; ++i) { kernel[i] = i % kernelSize - 1; } float* imgGpu; float* kernelGpu; float* resultGpu; CHECK(cudaMalloc((void**)\u0026amp;imgGpu, width * height * sizeof(float))); CHECK(cudaMalloc((void**)\u0026amp;kernelGpu, kernelSize * kernelSize * sizeof(float))); CHECK(cudaMalloc((void**)\u0026amp;resultGpu, width * height * sizeof(float))); CHECK(cudaMemcpy(imgGpu, img, width * height * sizeof(float), cudaMemcpyHostToDevice)); CHECK(cudaMemcpy(kernelGpu, kernel, kernelSize * kernelSize * sizeof(float), cudaMemcpyHostToDevice)); int threadNum = getThreadNum(); int blockNum = (width * height - 0.5) / threadNum + 1; float t_sum = 0; float t2_sum = 0; for (int repeat = 0; repeat \u0026lt;= NUM_REPEATS; ++repeat) { cudaEvent_t start, stop; CHECK(cudaEventCreate(\u0026amp;start)); CHECK(cudaEventCreate(\u0026amp;stop)); CHECK(cudaEventRecord(start)); cudaEventQuery(start); conv \u0026lt;\u0026lt; \u0026lt;blockNum, threadNum \u0026gt;\u0026gt; \u0026gt; (imgGpu, kernelGpu, resultGpu, width, height, kernelSize); CHECK(cudaEventRecord(stop)); CHECK(cudaEventSynchronize(stop)); float elapsed_time; CHECK(cudaEventElapsedTime(\u0026amp;elapsed_time, start, stop)); printf(\u0026#34;Time = %g ms.\\n\u0026#34;, elapsed_time); if (repeat \u0026gt; 0) { t_sum += elapsed_time; t2_sum += elapsed_time * elapsed_time; } CHECK(cudaEventDestroy(start)); CHECK(cudaEventDestroy(stop)); } const float t_ave = t_sum / NUM_REPEATS; const float t_err = sqrt(t2_sum / NUM_REPEATS - t_ave * t_ave); printf(\u0026#34;Time = %g +- %g ms.\\n\u0026#34;, t_ave, t_err); float* result = new float[width * height]; CHECK(cudaMemcpy(result, resultGpu, width * height * sizeof(float), cudaMemcpyDeviceToHost)); // visualization printf(\u0026#34;img\\n\u0026#34;); for (int row = 0; row \u0026lt; 10; ++row) { for (int col = 0; col \u0026lt; 10; ++col) { printf(\u0026#34;%2.0f \u0026#34;, img[col + row * width]); } printf(\u0026#34;\\n\u0026#34;); } printf(\u0026#34;kernel\\n\u0026#34;); for (int row = 0; row \u0026lt; kernelSize; ++row) { for (int col = 0; col \u0026lt; kernelSize; ++col) { printf(\u0026#34;%2.0f \u0026#34;, kernel[col + row * kernelSize]); } printf(\u0026#34;\\n\u0026#34;); } printf(\u0026#34;result\\n\u0026#34;); for (int row = 0; row \u0026lt; 10; ++row) { for (int col = 0; col \u0026lt; 10; ++col) { printf(\u0026#34;%2.0f \u0026#34;, result[col + row * width]); } printf(\u0026#34;\\n\u0026#34;); } return 0; } ","date":"2024-05-20","externalUrl":null,"permalink":"/posts/cuda/cuda_conv/","section":"Blogs","summary":"CUDA Conv # åœ¨ PyTorch ä¸Šå®ç°Convå¾ˆç®€å• import torch from torch.nn.functional import","title":"CUDA Conv","type":"posts"},{"content":" Steps of CUDA Program # Config GPU(device), cudaSetDevice() Allocate device memory, cudaMalloc(), cudaMallocManaged() Allocate CPU(Host) memory Copy data from host to device, cudaMemcpy() Run kernel on device Copy result from device to host, cudaMemcpy() Print result on host Release host and device memory, cudaFree(), free() CPU is always called host, the GPU is called device\nC Example:\n// file: sample.cu #include\u0026lt;stdint.h\u0026gt; #include\u0026lt;cuda.h\u0026gt; #include \u0026#34;cuda_runtime.h\u0026#34; #include \u0026#34;device_launch_parameters.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; // This is a sample CUDA kernel, called on host and execte on device __global__ void add(float* a) { a[threadIdx.x] = 1; } int main(int argc, char** argv) { // 1. Set device cudaSetDevice(0); // 2. Allocate device memory float* dx; cudaMalloc((void**)\u0026amp;dx, 16 * sizeof(float)); // 3. Allocate host memory float hx[16] = { 0 }; // 4. Copy data from host to device cudaMemcpy(dx, hx, 16 * sizeof(float), cudaMemcpyHostToDevice); // 5. Run kernel on device add \u0026lt;\u0026lt; \u0026lt;1, 16 \u0026gt;\u0026gt; \u0026gt; (dx); // 6. Copy result from device to host cudaMemcpy(hx, dx, 16 * sizeof(float), cudaMemcpyDeviceToHost); // 7. Print result on host for (int i = 0; i \u0026lt; 16; i++) { printf(\u0026#34;%f \\n\u0026#34;, hx[i]); } 8. Release host and device memory cudaFree(dx); free(hx); //cudaDeviceReset(); return 0; } ä½¿ç”¨nvccç¼–è¯‘,ç„¶åè¿è¡Œ\nnvcc sample.cu - o sample ./sample CUDA æä¾›äº†ç»Ÿä¸€å†…å­˜: gpuå’Œcpuå¯è®¿é—®çš„å•ä¸€å†…å­˜ç©ºé—´. è°ƒç”¨cudaMallocManaged()ï¼Œå®ƒè¿”å›ä¸€ä¸ªæŒ‡é’ˆï¼Œä»host codeæˆ–device codeéƒ½å¯ä»¥è®¿é—®ã€‚è¦é‡Šæ”¾æ•°æ®ï¼Œåªéœ€å°†æŒ‡é’ˆä¼ é€’ç»™cudaFree()ã€‚\nCUDA Kernel and Parrallel Computing # å‰ç½®çŸ¥è¯†: ç†è§£ GPU ç»“æ„, Grid, Block, Thread è¿™å‡ ä¸ªé€»è¾‘æ¦‚å¿µä¹‹é—´çš„å…³ç³»\nCUDA kernel çš„ç¼–ç¨‹æ¨¡å‹\n[Todo] Dim and size detail\nè°ƒç”¨kernel: add \u0026lt;\u0026lt; \u0026lt;blockNumber, threadNumber \u0026gt;\u0026gt; \u0026gt; (dx);\nç¼–å†™kernelï¼š\nç”¨å…³é”®å­—æè¿°ç¬¦ __global__ å£°æ˜kernel: __global__ void add(){} è°ƒç”¨ kernel æ—¶çš„å‚æ•° \u0026lt;\u0026lt;\u0026lt;blockNumber per grid, threadNumber per block\u0026gt;\u0026gt;\u0026gt; å†³å®šäº†å…±æœ‰ TotalThreadNum = blockNumber * threadNumber ä¸ªçº¿ç¨‹å¯ä»¥å¹¶è¡Œæ‰§è¡Œä»»åŠ¡ kernel å†…çš„æ¯ä¸€æ¬¡è¿­ä»£ï¼Œæ„å‘³ç€ TotalThreadNum ä¸ªçº¿ç¨‹å¹¶è¡Œæ‰§è¡Œäº†ä¸€æ¬¡å¾ªç¯ä½“ä¸­çš„ä»»åŠ¡ï¼ˆå³æ¯ä¸ªçº¿ç¨‹å®Œæˆå¯¹ä¸€ä»½æ•°æ®çš„å¤„ç†ï¼‰ï¼Œä¹Ÿå°±æ˜¯æ¯æ¬¡è¿­ä»£èƒ½å¤„ç† TotalThreadNum ä»½æ•°æ®ï¼ŒTotalThreadNum ä¹Ÿç­‰ä»·äºè·¨æ­¥(stride)çš„å¤§å° kernel ä¸­ threadIdx.x ä»£è¡¨ the index of the thread within the blockï¼Œ blockDim.x ä»£è¡¨ the size of blockï¼ˆnumber of threads in blockï¼ˆå‡è®¾ è¿™é‡Œçš„ grid å’Œ block çš„ dim åªæœ‰ä¸€ç»´ï¼‰ kernel å†… threadIdx.x å’Œ blockIdx.x çš„ç»„åˆå¯¹åº”çº¿ç¨‹çš„å”¯ä¸€æ ‡è¯† ä»¥add_3è¿™ä¸ª kernel ä¸ºä¾‹ï¼Œå¯ä»¥ç”¨ index = blockIdx.x * blockDim.x + threadIdx.x è·å¾—å½“å‰çº¿ç¨‹çš„è¦å¤„ç†çš„æ•°æ®çš„æ•°ç»„ä¸‹æ ‡ï¼ˆè§ä¸‹å›¾ï¼‰ï¼Œ\n__global__ void add_3(int n, float *x, float *y) { int index = blockIdx.x * blockDim.x + threadIdx.x; // strideä¸ºgridçš„çº¿ç¨‹æ€»æ•°:blockDim.x*gridDim.x int stride = blockDim.x * gridDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] = x[i] + y[i]; } Kernel examples(ä¸‹é¢çš„ C++ Example)\n1ä¸ªblock,1ä¸ªçº¿ç¨‹: add_1() 1ä¸ªblock,å¤šä¸ªçº¿ç¨‹: add_2() å¤šä¸ªblock,å¤šä¸ªçº¿ç¨‹: add_3() å¤šä¸ªblockï¼Œå¤šä¸ªçº¿ç¨‹ä¹Ÿç§°ä¸ºç½‘æ ¼è·¨æ­¥å¾ªç¯ï¼Œå…¶ä¸­æ¯æ¬¡å¾ªç¯çš„è·¨æ­¥(stride)ä¸º grid çš„çº¿ç¨‹æ€»æ•°: stride = blockDim.x * gridDim.x\nC++ Example:\n// file: add.cu #include \u0026lt;iostream\u0026gt; #include \u0026lt;math.h\u0026gt; // Kernel function to add the elements of two arrays // single thread __global__ void add_1(int n, float *x, float *y) { for (int i = 0; i \u0026lt; n; i++) y[i] = x[i] + y[i]; } // single block, multi threads __global__ void add_2(int n, float *x, float *y) { int index = threadIdx.x; int stride = blockDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] = x[i] + y[i]; } // multi block, multi threads // ç½‘æ ¼è·¨æ­¥(stride)å¾ªç¯ __global__ void add_3(int n, float *x, float *y) { int index = blockIdx.x * blockDim.x + threadIdx.x; // strideä¸ºgridçš„çº¿ç¨‹æ€»æ•°:blockDim.x*gridDim.x int stride = blockDim.x * gridDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] = x[i] + y[i]; } int main(void) { int N = 1\u0026lt;\u0026lt;20; float *x, *y; // Allocate Unified Memory â€“ accessible from CPU or GPU cudaMallocManaged(\u0026amp;x, N*sizeof(float)); cudaMallocManaged(\u0026amp;y, N*sizeof(float)); // initialize x and y arrays on the host for (int i = 0; i \u0026lt; N; i++) { x[i] = 1.0f; y[i] = 2.0f; } // Run kernel on 1M elements on the GPU // Just run with single thread :) add_1\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Run with 1 block and multi threads add_2\u0026lt;\u0026lt;\u0026lt;1, 256\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Run with multi block and multi threads int blockSize = 256;//å¹¶è¡Œçº¿ç¨‹æ•°é‡ int numBlocks = (N + blockSize - 1) / blockSize;//çº¿ç¨‹å—æ•°é‡ add_3\u0026lt;\u0026lt;\u0026lt;numBlocks, blockSize\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Wait for GPU to finish before accessing on host cudaDeviceSynchronize(); // Check for errors (all values should be 3.0f) float maxError = 0.0f; for (int i = 0; i \u0026lt; N; i++) maxError = fmax(maxError, fabs(y[i]-3.0f)); std::cout \u0026lt;\u0026lt; \u0026#34;Max error: \u0026#34; \u0026lt;\u0026lt; maxError \u0026lt;\u0026lt; std::endl; // Free memory cudaFree(x); cudaFree(y); return 0; } CUDA Kernel for Conv # CUDA Code Profiling # nvprofæ˜¯CUDAå·¥å…·åŒ…é™„å¸¦çš„å‘½ä»¤è¡ŒGPUåˆ†æå™¨\nReference:\nNVIDIA CUDA Docs ","date":"2024-05-18","externalUrl":null,"permalink":"/posts/cuda/cuda_1/","section":"Blogs","summary":"Steps of CUDA Program # Config GPU(device), cudaSetDevice() Allocate device memory, cudaMalloc(), cudaMallocManaged() Allocate CPU(Host) memory Copy data from host","title":"CUDA Programming","type":"posts"},{"content":"","date":"2024-05-18","externalUrl":null,"permalink":"/tags/gpu/","section":"Tags","summary":"","title":"GPU","type":"tags"},{"content":" CPU ä¸ GPU çš„ä¸åŒ # CPUï¼Œ4ä¸ª ALUï¼Œä¸»è¦è´Ÿè´£é€»è¾‘è®¡ç®—ï¼Œ1ä¸ªæ§åˆ¶å•å…ƒ Controlï¼Œ1ä¸ª DRAMï¼Œ1ä¸ª Cache GPUï¼Œç»¿è‰²å°æ–¹å—çœ‹ä½œ ALUï¼Œçº¢è‰²æ¡†çœ‹ä½œä¸€ä¸ª SMï¼ŒSM ä¸­çš„å¤šä¸ª ALU share ä¸€ä¸ªControl å’Œ Cacheï¼ŒSM å¯ä»¥çœ‹ä½œä¸€ä¸ªå¤šæ ¸ CPUï¼Œä½†æ˜¯ ALU æ›´å¤šï¼Œcontrol æ›´å°‘ï¼Œä¹Ÿå°±æ˜¯ç®—åŠ›æå‡ï¼Œæ§åˆ¶åŠ›å‡å¼± æ‰€ä»¥ï¼ŒCPU é€‚åˆæ§åˆ¶é€»è¾‘å¤æ‚çš„ä»»åŠ¡ï¼ŒGPU é€‚åˆé€»è¾‘ç®€å•ã€æ•°æ®é‡å¤§ã€è®¡ç®—é‡å¤§çš„ä»»åŠ¡ã€‚\nGPU, CUDA, AI Framework çš„å…³ç³» # Reference:\nNVIDIA CUDA Docs cudaç¼–ç¨‹å­¦ä¹  ä¸€å¼ å›¾äº†è§£GPUã€CUDAã€CUDA toolkitå’Œpytorchçš„å…³ç³» GPU å†…å­˜æ¦‚å¿µæµ…æ ","date":"2024-05-18","externalUrl":null,"permalink":"/posts/cuda/gpu/","section":"Blogs","summary":"CPU ä¸ GPU çš„ä¸åŒ # CPUï¼Œ4ä¸ª ALUï¼Œä¸»è¦è´Ÿ","title":"GPU ç»“æ„","type":"posts"},{"content":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\nä¸æ ‡å‡† attention ç›¸æ¯”ï¼ŒFlash Attention æœ‰ä»¥ä¸‹ä¸‰ç‚¹ç‰¹ç‚¹ï¼š\nè¿ç®—é€Ÿåº¦æ›´å¿« (Fast) æ›´èŠ‚çœæ˜¾å­˜ (Memory-Efficient) è®¡ç®—ç»“æœç›¸åŒ (Exact) FlashAttention ç›®çš„ä¸æ˜¯èŠ‚çº¦ FLOPsï¼Œè€Œæ˜¯å‡å°‘å¯¹HBMçš„è®¿é—®ã€‚å®ƒæ²¡æœ‰æ”¹å˜åŸæœ‰çš„è®¡ç®—å…¬å¼ï¼Œæ•´ä½“è®¡ç®—å¤æ‚åº¦å¹¶æœªé™ä½ã€‚\nèƒŒæ™¯ # GPUä¸­å­˜å‚¨å•å…ƒä¸»è¦æœ‰ HBM å’Œ SRAMï¼šHBM å®¹é‡å¤§ä½†æ˜¯è®¿é—®é€Ÿåº¦æ…¢ï¼ŒSRAMå®¹é‡å°å´æœ‰ç€è¾ƒé«˜çš„è®¿é—®é€Ÿåº¦ã€‚ä¾‹å¦‚ï¼šA100 GPUæœ‰40-80GBçš„HBMï¼Œå¸¦å®½ä¸º1.5-2.0TB/sï¼›æ¯108ä¸ªæµå¼å¤šæ ¸å¤„ç†å™¨å„æœ‰192KBçš„ç‰‡ä¸ŠSRAMï¼Œå¸¦å®½ä¼°è®¡çº¦ä¸º 19TB/sã€‚å¯ä»¥çœ‹å‡ºï¼Œç‰‡ä¸Šçš„SRAMæ¯”HBMå¿«ä¸€ä¸ªæ•°é‡çº§ï¼Œä½†å°ºå¯¸è¦å°è®¸å¤šæ•°é‡çº§ã€‚\nå½“è¾“å…¥åºåˆ—ï¼ˆsequence lengthï¼‰è¾ƒé•¿æ—¶ï¼ŒTransformerçš„è®¡ç®—è¿‡ç¨‹ç¼“æ…¢ä¸”è€—è´¹å†…å­˜ï¼Œè¿™æ˜¯å› ä¸º self-attention çš„ time å’Œ memory complexity ä¼šéšç€ sequence length çš„å¢åŠ æˆäºŒæ¬¡å¢é•¿ã€‚\næ ‡å‡†Attentionçš„è®¡ç®—è¿‡ç¨‹ï¼š $$ S=Q K^T \\in \\mathbb{R}^{N \\times N} $$ $$ P=\\operatorname{softmax}(S) \\in \\mathbb{R}^{N \\times N} $$ $$ O=P V \\in \\mathbb{R}^{N \\times N} $$\næ ‡å‡†Attentionçš„ä¸­é—´ç»“æœ ğ‘†, ğ‘ƒ é€šå¸¸éœ€è¦é€šè¿‡é«˜å¸¦å®½å†…å­˜ï¼ˆHBMï¼‰è¿›è¡Œå­˜å–ï¼Œä¸¤è€…æ‰€éœ€å†…å­˜ç©ºé—´å¤æ‚åº¦ä¸º\\(O(Nd+N^2)), å¯¹ HBM çš„é‡å¤è¯»å†™æ˜¯ä¸»è¦ç“¶é¢ˆã€‚è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œéœ€è¦åšä¸¤ä»¶äº‹ï¼š\nåœ¨ä¸è®¿é—®æ•´ä¸ªè¾“å…¥çš„æƒ…å†µä¸‹è®¡ç®— softmax ä¸ä¸ºåå‘ä¼ æ’­å­˜å‚¨å¤§çš„ä¸­é—´ attention çŸ©é˜µ(\\(N^2\\)) FlashAttention V1 # FlashAttention æå‡ºäº†ä¸¤ç§æ–¹æ³•æ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼štiling å’Œ recomputationã€‚\ntiling - æ³¨æ„åŠ›è®¡ç®—è¢«é‡æ–°æ„é€ ï¼Œå°†è¾“å…¥åˆ†å‰²æˆå—ï¼Œå¹¶é€šè¿‡åœ¨è¾“å…¥å—ä¸Šè¿›è¡Œå¤šæ¬¡ä¼ é€’æ¥é€’å¢åœ°æ‰§è¡Œsoftmaxæ“ä½œã€‚ recomputation - å­˜å‚¨æ¥è‡ªå‰å‘çš„ softmax å½’ä¸€åŒ–å› å­ï¼Œä»¥ä¾¿åœ¨åå‘ä¸­å¿«é€Ÿé‡æ–°è®¡ç®—èŠ¯ç‰‡ä¸Šçš„ attentionï¼Œè¿™æ¯”ä»HBMè¯»å–ä¸­é—´çŸ©é˜µçš„æ ‡å‡†æ³¨æ„åŠ›æ–¹æ³•æ›´å¿«ã€‚å¯ä»¥æŠŠå®ƒçœ‹ä½œåŸºäº tiling çš„ç‰¹æ®Šçš„ gradient checkpointing æ­£å¸¸çš„softmaxè®¡ç®—ï¼š\n$$m(x):=\\max _i x i$$ $$f(x):=\\left[e^{x_1-m(x)} \\ldots e^{x_B-m(x)}\\right]$$ $$\\ell(x):=\\sum_i f(x)_i$$ $$\\operatorname{softmax}(x):=\\frac{f(x)}{\\ell(x)}$$\nsoftmax ä¼ªä»£ç : softmax å‡½æ•°éœ€è¦ä¸‰ä¸ªå¾ªç¯ï¼Œç¬¬ä¸€ä¸ªå¾ªç¯è®¡ç®—æ•°ç»„çš„æœ€å¤§å€¼ï¼Œç¬¬äºŒä¸ªå¾ªç¯è®¡ç®— softmax çš„åˆ†æ¯ï¼Œç¬¬ä¸‰ä¸ªå¾ªç¯è®¡ç®— softmax è¾“å‡ºã€‚\nåˆ†å—çš„ softmax è®¡ç®—(å‡è®¾åˆ†2å—å¹¶è¡Œè®¡ç®—)ï¼š\n$$m(x)=m\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)=\\max \\left(m\\left(x^{(1)}\\right), m\\left(x^{(2)}\\right)\\right) $$ $$f(x)=\\left[e^{m\\left(x^{(1)}\\right)-m(x)} f\\left(x^{(1)}\\right) \\quad e^{m\\left(x^{(2)}\\right)-m(x)} f\\left(x^{(2)}\\right)\\right] $$ $$\\ell(x)=\\ell\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)=e^{m\\left(x^{(1)}\\right)-m(x)} \\ell\\left(x^{(1)}\\right)+e^{m\\left(x^{(2)}\\right)-m(x)} \\ell\\left(x^{(2)}\\right) $$ $$\\operatorname{softmax}(x)=\\frac{f(x)}{\\ell(x)}$$\nåˆ†å—çš„ softmax ä¼ªä»£ç : åœ¨ç¬¬ä¸€ä¸ªå¾ªç¯ä¸­åŒæ—¶å¯¹æœ€å¤§å€¼\\(m\\)ä»¥åŠ softmax çš„åˆ†æ¯\\(d\\)è¿›è¡Œæ›´æ–°ï¼Œä»è€Œå‡å°‘äº†ä¸€ä¸ªå¾ªç¯ã€‚é€šè¿‡ tiling çš„æ–¹å¼ï¼Œsoftmax çš„å¾ªç¯æ•°ä»ä¸‰ä¸ªå‡åˆ°äº†ä¸¤ä¸ªï¼Œä»è€Œå¯ä»¥é™ä½å†…å­˜æ¶ˆè€—ã€‚\nflashattention ä¼ªä»£ç ï¼š ä¸­é—´å˜é‡ï¼š\\(O_i\\)(æœ€ç»ˆä¹˜ç§¯)ã€\\(l_i\\)ï¼ˆsoftmaxçš„åˆ†æ¯ï¼Œå³ç´¯åŠ å’Œï¼‰ã€\\(m_i\\)ï¼ˆéå†åˆ°å½“å‰å—ä¸ºæ­¢çš„æœ€å¤§å€¼ï¼‰ï¼Œå†ä¹Ÿä¸ç”¨ä¿å­˜å…¨éƒ¨çš„Så’ŒPäº†ã€‚\nç”±äºé‡æ–°è®¡ç®—å¯¼è‡´ FLOPs å¢åŠ ï¼Œä½†æ˜¯ç”±äºå¤§é‡å‡å°‘HBMè®¿é—®ï¼ŒFlashAttention è¿è¡Œé€Ÿåº¦æ›´å¿«\nFlashAttentionçš„ FLOPs ä¸º \\(ğ‘‚(ğ‘^2ğ‘‘)\\)ï¼Œé™¤äº† input å’Œ outputï¼Œé¢å¤–éœ€è¦çš„å†…å­˜ä¸º \\(ğ‘‚(ğ‘)\\), å¯¹HBMè®¿é—®çš„æ¬¡æ•°ä¸º \\(ğ‘‚(ğ‘^2ğ‘‘^2ğ‘€^{âˆ’1})\\), æ¯”æ ‡å‡† Attention çš„ \\(O(Nd+N^2)\\)æ›´é«˜æ•ˆ\nPyTorch 2.0å·²å°† FlashAttention é›†æˆåˆ°å®˜æ–¹åº“ä¸­ï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨ torch.nn.functional.scaled_dot_product_attention\næ€»ç»“ # FlashAttention V1:\né€šè¿‡åˆ‡å—æŠ€æœ¯å‡å°‘äº†å†…å­˜è®¿é—®æ¬¡æ•°ï¼Œæé«˜äº†è®¡ç®—é€Ÿåº¦å’Œå†…å­˜åˆ©ç”¨ç‡ã€‚ å†…å­˜è®¿é—®å¤æ‚åº¦ä¸º \\(ğ‘‚(ğ‘^2ğ‘‘^2ğ‘€^{âˆ’1})\\), æ¯”æ ‡å‡† Attention çš„ \\(O(Nd+N^2)\\)æ›´é«˜æ•ˆ ","date":"2024-05-05","externalUrl":null,"permalink":"/posts/llm/flash_attention/","section":"Blogs","summary":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness ä¸æ ‡å‡† attention ç›¸æ¯”ï¼ŒFlash","title":"Flash Attention","type":"posts"},{"content":" Me # class Me: def __init__(self): self.names = \u0026#34;Victor Yang\u0026#34; self.prefer_name = \u0026#34;Victor\u0026#34; self.profession = \u0026#34;Programmer\u0026#34; self.hometown = \u0026#34;Shandong, CN\u0026#34; self.current_location = \u0026#34;Beijing, CN\u0026#34; self.graduate = \u0026#34;University of Science and Technology of China(USTC), Software Engineering\u0026#34; self.undergraduate = \u0026#34;China University of Petroleum(UPC), Computer Science\u0026#34; self.interests = [\u0026#34;AI\u0026#34;, \u0026#34;Taoist\u0026#34;, \u0026#34;Traditional Chinese Medicine\u0026#34;, \u0026#34;History\u0026#34;, \u0026#34;Finance\u0026#34;, \u0026#34;Science Fiction\u0026#34;] self.hoppies = [\u0026#34;Reading\u0026#34;, \u0026#34;Yoga\u0026#34;, \u0026#34;Meditation\u0026#34;, \u0026#34;Swimming\u0026#34;] My Skills # Generative AI/AIGC: Transformer, LLM, VLM, Llama, Llava, MoE, Attention, vLLM, DeepSpeed, RAG, Langchain, Agent Parallel Computing: CUDA, GPU, Horovod, MPI Machine Learning: Deep Learning, PyTorch, Federated Learning Programming Language: Python, C++, GoLang Cloud Native: Knative, Ray, Docker Others: Linux ","date":"2024-05-01","externalUrl":null,"permalink":"/about/","section":"Welcome to My Blog","summary":"Me # class Me: def __init__(self): self.names = \u0026#34;Victor Yang\u0026#34; self.prefer_name = \u0026#34;Victor\u0026#34; self.profession = \u0026#34;Programmer\u0026#34; self.hometown = \u0026#34;Shandong, CN\u0026#34; self.","title":"About","type":"page"},{"content":"This is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing. ğŸš€\nYou can also use these content pages to define Hugo metadata like titles and descriptions that will be used for SEO and other purposes.\n","date":"0001-01-01","externalUrl":null,"permalink":"/tags/advanced/","section":"Tags","summary":"This is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing.","title":"Advanced","type":"tags"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/tags/_index.zh-cn/","section":"Tags","summary":"","title":"æ ‡ç­¾","type":"tags"},{"content":"æ¬¢è¿æ¥åˆ°æˆ‘çš„åšå®¢!\n","date":"0001-01-01","externalUrl":null,"permalink":"/_index.zh-cn/","section":"Welcome to My Blog","summary":"æ¬¢è¿æ¥åˆ°æˆ‘çš„åšå®¢!","title":"æ¬¢è¿æ¥åˆ°æˆ‘çš„åšå®¢","type":"page"},{"content":"è¿™æ˜¯é«˜çº§æ ‡è®°ã€‚ç±»ä¼¼å…¶ä»– Blowfish ä¸­çš„å…¶ä»–åˆ—è¡¨é¡µé¢ï¼Œä½ å¯ä»¥åœ¨åˆ†ç±»åˆ—è¡¨é¡µæ·»åŠ è‡ªå®šä¹‰å†…å®¹ï¼Œè¿™éƒ¨åˆ†å†…å®¹ä¼šæ˜¾ç¤ºåœ¨é¡¶éƒ¨ã€‚ğŸš€\nä½ ä¹Ÿå¯ä»¥ç”¨è¿™äº›å†…å®¹æ¥å®šä¹‰ Hugo çš„å…ƒæ•°æ®ï¼Œæ¯”å¦‚æ ‡é¢˜å’Œæè¿°ã€‚è¿™äº›å†…å®¹å¯ä»¥è¢«ç”¨æ¥å¢å¼º SEO æˆ–å…¶ä»–ç›®çš„ã€‚\n","date":"0001-01-01","externalUrl":null,"permalink":"/tags/advanced/_index.zh-cn/","section":"Tags","summary":"è¿™æ˜¯é«˜çº§æ ‡è®°ã€‚ç±»ä¼¼å…¶ä»– Blowfish ä¸­çš„å…¶ä»–åˆ—è¡¨é¡µé¢","title":"é«˜çº§","type":"tags"}]