
[{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/posts/","section":"Blogs","summary":"","title":"Blogs","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/tags/conv/","section":"Tags","summary":"","title":"Conv","type":"tags"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/tags/cuda/","section":"Tags","summary":"","title":"CUDA","type":"tags"},{"content":" CUDA Conv # Steps:\nCheck Get thread info Memory allocation Copy data to device Call conv kernel Copy data to host Free memory //file: conv.cu #include\u0026lt;stdint.h\u0026gt; #include\u0026lt;cuda.h\u0026gt; #include \u0026#34;cuda_runtime.h\u0026#34; #include \u0026#34;device_launch_parameters.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;math.h\u0026gt; const int NUM_REPEATS = 10; #define CHECK(call) \\ do \\ { \\ const cudaError_t error_code = call; \\ if (error_code != cudaSuccess) \\ { \\ printf(\u0026#34;CUDA Error:\\n\u0026#34;); \\ printf(\u0026#34; File: %s\\n\u0026#34;, __FILE__); \\ printf(\u0026#34; Line: %d\\n\u0026#34;, __LINE__); \\ printf(\u0026#34; Error code: %d\\n\u0026#34;, error_code); \\ printf(\u0026#34; Error text: %s\\n\u0026#34;, \\ cudaGetErrorString(error_code)); \\ exit(1); \\ } \\ } while (0) static void HandleError(cudaError_t err, const char* file, int line) { if (err != cudaSuccess) { printf(\u0026#34;%s in %s at line %d\\n\u0026#34;, cudaGetErrorString(err), file, line); exit(EXIT_FAILURE); } } #define HANDLE_ERROR(err) (HandleError(err, __FILE__, __LINE__)) int getThreadNum() { cudaDeviceProp prop; int count; CHECK(cudaGetDeviceCount(\u0026amp;count)); printf(\u0026#34;gpu num %d\\n\u0026#34;, count); CHECK(cudaGetDeviceProperties(\u0026amp;prop, 0)); printf(\u0026#34;max thread num: %d\\n\u0026#34;, prop.maxThreadsPerBlock); printf(\u0026#34;max grid dimensions: %d, %d, %d)\\n\u0026#34;, prop.maxGridSize[0], prop.maxGridSize[1], prop.maxGridSize[2]); return prop.maxThreadsPerBlock; } __global__ void conv(float* img, float* kernel, float* result, int width, int height, int kernelSize) { int ti = threadIdx.x; int bi = blockIdx.x; int id = (bi * blockDim.x + ti); if (id \u0026gt;= width * height) { return; } int row = id / width; int col = id % width; for (int i = 0; i \u0026lt; kernelSize; ++i) { for (int j = 0; j \u0026lt; kernelSize; ++j) { float imgValue = 0; int curRow = row - kernelSize / 2 + i; int curCol = col - kernelSize / 2 + j; if (curRow \u0026lt; 0 || curCol \u0026lt; 0 || curRow \u0026gt;= height || curCol \u0026gt;= width) { } else { imgValue = img[curRow * width + curCol]; } result[id] += kernel[i * kernelSize + j] * imgValue; } } } int main() { int width = 1000; int height = 1000; float* img = new float[width * height]; for (int row = 0; row \u0026lt; height; ++row) { for (int col = 0; col \u0026lt; width; ++col) { img[col + row * width] = (col + row) % 256; } } int kernelSize = 3; float* kernel = new float[kernelSize * kernelSize]; for (int i = 0; i \u0026lt; kernelSize * kernelSize; ++i) { kernel[i] = i % kernelSize - 1; } float* imgGpu; float* kernelGpu; float* resultGpu; CHECK(cudaMalloc((void**)\u0026amp;imgGpu, width * height * sizeof(float))); CHECK(cudaMalloc((void**)\u0026amp;kernelGpu, kernelSize * kernelSize * sizeof(float))); CHECK(cudaMalloc((void**)\u0026amp;resultGpu, width * height * sizeof(float))); CHECK(cudaMemcpy(imgGpu, img, width * height * sizeof(float), cudaMemcpyHostToDevice)); CHECK(cudaMemcpy(kernelGpu, kernel, kernelSize * kernelSize * sizeof(float), cudaMemcpyHostToDevice)); int threadNum = getThreadNum(); int blockNum = (width * height - 0.5) / threadNum + 1; float t_sum = 0; float t2_sum = 0; for (int repeat = 0; repeat \u0026lt;= NUM_REPEATS; ++repeat) { cudaEvent_t start, stop; CHECK(cudaEventCreate(\u0026amp;start)); CHECK(cudaEventCreate(\u0026amp;stop)); CHECK(cudaEventRecord(start)); cudaEventQuery(start); conv \u0026lt;\u0026lt; \u0026lt;blockNum, threadNum \u0026gt;\u0026gt; \u0026gt; (imgGpu, kernelGpu, resultGpu, width, height, kernelSize); CHECK(cudaEventRecord(stop)); CHECK(cudaEventSynchronize(stop)); float elapsed_time; CHECK(cudaEventElapsedTime(\u0026amp;elapsed_time, start, stop)); printf(\u0026#34;Time = %g ms.\\n\u0026#34;, elapsed_time); if (repeat \u0026gt; 0) { t_sum += elapsed_time; t2_sum += elapsed_time * elapsed_time; } CHECK(cudaEventDestroy(start)); CHECK(cudaEventDestroy(stop)); } const float t_ave = t_sum / NUM_REPEATS; const float t_err = sqrt(t2_sum / NUM_REPEATS - t_ave * t_ave); printf(\u0026#34;Time = %g +- %g ms.\\n\u0026#34;, t_ave, t_err); float* result = new float[width * height]; CHECK(cudaMemcpy(result, resultGpu, width * height * sizeof(float), cudaMemcpyDeviceToHost)); // visualization printf(\u0026#34;img\\n\u0026#34;); for (int row = 0; row \u0026lt; 10; ++row) { for (int col = 0; col \u0026lt; 10; ++col) { printf(\u0026#34;%2.0f \u0026#34;, img[col + row * width]); } printf(\u0026#34;\\n\u0026#34;); } printf(\u0026#34;kernel\\n\u0026#34;); for (int row = 0; row \u0026lt; kernelSize; ++row) { for (int col = 0; col \u0026lt; kernelSize; ++col) { printf(\u0026#34;%2.0f \u0026#34;, kernel[col + row * kernelSize]); } printf(\u0026#34;\\n\u0026#34;); } printf(\u0026#34;result\\n\u0026#34;); for (int row = 0; row \u0026lt; 10; ++row) { for (int col = 0; col \u0026lt; 10; ++col) { printf(\u0026#34;%2.0f \u0026#34;, result[col + row * width]); } printf(\u0026#34;\\n\u0026#34;); } return 0; } ","date":"2024-05-24","externalUrl":null,"permalink":"/posts/cuda/cuda_conv/","section":"Blogs","summary":"CUDA Conv # Steps:\nCheck Get thread info Memory allocation Copy data to device Call conv kernel Copy data to host Free memory //file: conv.","title":"CUDA Conv","type":"posts"},{"content":"[Todo]\n","date":"2024-05-24","externalUrl":null,"permalink":"/posts/cuda/cuda_optimize/","section":"Blogs","summary":"[Todo]","title":"CUDA Optimization","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/series/cuda-parallel-programming/","section":"Series","summary":"","title":"CUDA Parallel Programming","type":"series"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/posts/cuda/","section":"Blogs","summary":"","title":"CUDA Parallel Programming","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"Hi, welcome to my blog.\n","date":"2024-05-24","externalUrl":null,"permalink":"/","section":"Welcome to Blowfish! 🎉","summary":"Hi, welcome to my blog.","title":"Welcome to Blowfish! 🎉","type":"page"},{"content":" Steps of CUDA Program # Config GPU(device), cudaSetDevice() Allocate device memory, cudaMalloc(), cudaMallocManaged() Allocate CPU(Host) memory Copy data from host to device, cudaMemcpy() Run kernel on device Copy result from device to host, cudaMemcpy() Print result on host Release host and device memory, cudaFree(), free() CPU is always called host, the GPU is called device\nC Example:\n// file: sample.cu #include\u0026lt;stdint.h\u0026gt; #include\u0026lt;cuda.h\u0026gt; #include \u0026#34;cuda_runtime.h\u0026#34; #include \u0026#34;device_launch_parameters.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; // This is a sample CUDA kernel, called on host and execte on device __global__ void add(float* a) { a[threadIdx.x] = 1; } int main(int argc, char** argv) { // 1. Set device cudaSetDevice(0); // 2. Allocate device memory float* dx; cudaMalloc((void**)\u0026amp;dx, 16 * sizeof(float)); // 3. Allocate host memory float hx[16] = { 0 }; // 4. Copy data from host to device cudaMemcpy(dx, hx, 16 * sizeof(float), cudaMemcpyHostToDevice); // 5. Run kernel on device add \u0026lt;\u0026lt; \u0026lt;1, 16 \u0026gt;\u0026gt; \u0026gt; (dx); // 6. Copy result from device to host cudaMemcpy(hx, dx, 16 * sizeof(float), cudaMemcpyDeviceToHost); // 7. Print result on host for (int i = 0; i \u0026lt; 16; i++) { printf(\u0026#34;%f \\n\u0026#34;, hx[i]); } 8. Release host and device memory cudaFree(dx); free(hx); //cudaDeviceReset(); return 0; } 使用nvcc编译,然后运行\nnvcc sample.cu - o sample ./sample CUDA 提供了统一内存: gpu和cpu可访问的单一内存空间. 调用cudaMallocManaged()，它返回一个指针，从host code或device code都可以访问。要释放数据，只需将指针传递给cudaFree()。\nCUDA Kernel and Parrallel Computing # 前置知识: 理解 GPU 结构, Grid, Block, Thread 这几个逻辑概念之间的关系\nCUDA kernel 的编程模型\n[Todo] Dim and size detail\n调用kernel: add \u0026lt;\u0026lt; \u0026lt;blockNumber, threadNumber \u0026gt;\u0026gt; \u0026gt; (dx);\n编写kernel：\n用关键字描述符 __global__ 声明kernel: __global__ void add(){} 调用 kernel 时的参数 \u0026lt;\u0026lt;\u0026lt;blockNumber per grid, threadNumber per block\u0026gt;\u0026gt;\u0026gt; 决定了共有 TotalThreadNum = blockNumber * threadNumber 个线程可以并行执行任务 kernel 内的每一次迭代，意味着 TotalThreadNum 个线程并行执行了一次循环体中的任务（即每个线程完成对一份数据的处理），也就是每次迭代能处理 TotalThreadNum 份数据，TotalThreadNum 也等价于跨步(stride)的大小 kernel 中 threadIdx.x 代表 the index of the thread within the block， blockDim.x 代表 the size of block（number of threads in block（假设 这里的 grid 和 block 的 dim 只有一维） kernel 内 threadIdx.x 和 blockIdx.x 的组合对应线程的唯一标识 以add_3这个 kernel 为例，可以用 index = blockIdx.x * blockDim.x + threadIdx.x 获得当前线程的要处理的数据的数组下标（见下图），\n__global__ void add_3(int n, float *x, float *y) { int index = blockIdx.x * blockDim.x + threadIdx.x; // stride为grid的线程总数:blockDim.x*gridDim.x int stride = blockDim.x * gridDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] = x[i] + y[i]; } Kernel examples(下面的 C++ Example)\n1个block,1个线程: add_1() 1个block,多个线程: add_2() 多个block,多个线程: add_3() 多个block，多个线程也称为网格跨步循环，其中每次循环的跨步(stride)为 grid 的线程总数: stride = blockDim.x * gridDim.x\nC++ Example:\n// file: add.cu #include \u0026lt;iostream\u0026gt; #include \u0026lt;math.h\u0026gt; // Kernel function to add the elements of two arrays // single thread __global__ void add_1(int n, float *x, float *y) { for (int i = 0; i \u0026lt; n; i++) y[i] = x[i] + y[i]; } // single block, multi threads __global__ void add_2(int n, float *x, float *y) { int index = threadIdx.x; int stride = blockDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] = x[i] + y[i]; } // multi block, multi threads // 网格跨步(stride)循环 __global__ void add_3(int n, float *x, float *y) { int index = blockIdx.x * blockDim.x + threadIdx.x; // stride为grid的线程总数:blockDim.x*gridDim.x int stride = blockDim.x * gridDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] = x[i] + y[i]; } int main(void) { int N = 1\u0026lt;\u0026lt;20; float *x, *y; // Allocate Unified Memory – accessible from CPU or GPU cudaMallocManaged(\u0026amp;x, N*sizeof(float)); cudaMallocManaged(\u0026amp;y, N*sizeof(float)); // initialize x and y arrays on the host for (int i = 0; i \u0026lt; N; i++) { x[i] = 1.0f; y[i] = 2.0f; } // Run kernel on 1M elements on the GPU // Just run with single thread :) add_1\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Run with 1 block and multi threads add_2\u0026lt;\u0026lt;\u0026lt;1, 256\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Run with multi block and multi threads int blockSize = 256;//并行线程数量 int numBlocks = (N + blockSize - 1) / blockSize;//线程块数量 add_3\u0026lt;\u0026lt;\u0026lt;numBlocks, blockSize\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Wait for GPU to finish before accessing on host cudaDeviceSynchronize(); // Check for errors (all values should be 3.0f) float maxError = 0.0f; for (int i = 0; i \u0026lt; N; i++) maxError = fmax(maxError, fabs(y[i]-3.0f)); std::cout \u0026lt;\u0026lt; \u0026#34;Max error: \u0026#34; \u0026lt;\u0026lt; maxError \u0026lt;\u0026lt; std::endl; // Free memory cudaFree(x); cudaFree(y); return 0; } CUDA Kernel for Conv # CUDA Code Profiling # nvprof是CUDA工具包附带的命令行GPU分析器\nReference:\nNVIDIA CUDA Docs ","date":"2024-05-24","externalUrl":null,"permalink":"/posts/cuda/cuda_1/","section":"Blogs","summary":"Steps of CUDA Program # Config GPU(device), cudaSetDevice() Allocate device memory, cudaMalloc(), cudaMallocManaged() Allocate CPU(Host) memory Copy data from host","title":"CUDA Programming","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/tags/gpu/","section":"Tags","summary":"","title":"GPU","type":"tags"},{"content":" CPU 与 GPU 的不同 # CPU，4个 ALU，主要负责逻辑计算，1个控制单元 Control，1个 DRAM，1个 Cache GPU，绿色小方块看作 ALU，红色框看作一个 SM，SM 中的多个 ALU share 一个Control 和 Cache，SM 可以看作一个多核 CPU，但是 ALU 更多，control 更少，也就是算力提升，控制力减弱 所以，CPU 适合控制逻辑复杂的任务，GPU 适合逻辑简单、数据量大、计算量大的任务。\nGPU, CUDA, AI Framework 的关系 # Reference:\nNVIDIA CUDA Docs cuda编程学习 一张图了解GPU、CUDA、CUDA toolkit和pytorch的关系 ","date":"2024-05-24","externalUrl":null,"permalink":"/posts/cuda/gpu/","section":"Blogs","summary":"CPU 与 GPU 的不同 # CPU，4个 ALU，主要负","title":"GPU 结构","type":"posts"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/attention/","section":"Tags","summary":"","title":"Attention","type":"tags"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/series/attention/","section":"Series","summary":"","title":"Attention","type":"series"},{"content":"[WIP]\nFlashAttention V2:\n在V1的基础上减少了非矩阵乘法运算的FLOPs。 通过并行化和任务分配优化提高了计算速度和GPU利用率，性能提升了2-3倍。 Flash-Decoding借鉴了FlashAttention的优点，将并行化维度扩展到keys/values序列长度，提高了推理速度。 Flash-Decoding几乎不用额外存储大量数据到全局内存中，减少了内存开销。 Flash-Decoding++通过异步softmax和统一最大值、flat GEMM优化和双缓冲、启发式数据流和硬件资源适应等方法进一步提高了LLM推理的性能。 ","date":"2024-05-23","externalUrl":null,"permalink":"/posts/llm/flash_attention_2/","section":"Blogs","summary":"[WIP] FlashAttention V2: 在V1的基础上减少了非矩阵乘法运算","title":"Flash Attention V2","type":"posts"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/posts/llm/","section":"Blogs","summary":"","title":"LLM","type":"posts"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/nlp/","section":"Tags","summary":"","title":"NLP","type":"tags"},{"content":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n与标准 attention 相比，Flash Attention 有以下三点特点：\n运算速度更快 (Fast) 更节省显存 (Memory-Efficient) 计算结果相同 (Exact) FlashAttention 目的不是节约 FLOPs，而是减少对HBM的访问。它没有改变原有的计算公式，整体计算复杂度并未降低。\n背景 # GPU中存储单元主要有 HBM 和 SRAM：HBM 容量大但是访问速度慢，SRAM容量小却有着较高的访问速度。例如：A100 GPU有40-80GB的HBM，带宽为1.5-2.0TB/s；每108个流式多核处理器各有192KB的片上SRAM，带宽估计约为 19TB/s。可以看出，片上的SRAM比HBM快一个数量级，但尺寸要小许多数量级。\n当输入序列（sequence length）较长时，Transformer的计算过程缓慢且耗费内存，这是因为 self-attention 的 time 和 memory complexity 会随着 sequence length 的增加成二次增长。\n标准Attention的计算过程： $$ S=Q K^T \\in \\mathbb{R}^{N \\times N} $$ $$ P=\\operatorname{softmax}(S) \\in \\mathbb{R}^{N \\times N} $$ $$ O=P V \\in \\mathbb{R}^{N \\times N} $$\n标准Attention的中间结果 𝑆, 𝑃 通常需要通过高带宽内存（HBM）进行存取，两者所需内存空间复杂度为\\(O(Nd+N^2)), 对 HBM 的重复读写是主要瓶颈。要解决这个问题，需要做两件事：\n在不访问整个输入的情况下计算 softmax 不为反向传播存储大的中间 attention 矩阵(\\(N^2\\)) FlashAttention V1 # FlashAttention 提出了两种方法来解决上述问题：tiling 和 recomputation。\ntiling - 注意力计算被重新构造，将输入分割成块，并通过在输入块上进行多次传递来递增地执行softmax操作。 recomputation - 存储来自前向的 softmax 归一化因子，以便在反向中快速重新计算芯片上的 attention，这比从HBM读取中间矩阵的标准注意力方法更快。可以把它看作基于 tiling 的特殊的 gradient checkpointing 正常的softmax计算：\n$$m(x):=\\max _i x i$$ $$f(x):=\\left[e^{x_1-m(x)} \\ldots e^{x_B-m(x)}\\right]$$ $$\\ell(x):=\\sum_i f(x)_i$$ $$\\operatorname{softmax}(x):=\\frac{f(x)}{\\ell(x)}$$\nsoftmax 伪代码: softmax 函数需要三个循环，第一个循环计算数组的最大值，第二个循环计算 softmax 的分母，第三个循环计算 softmax 输出。\n分块的 softmax 计算(假设分2块并行计算)：\n$$m(x)=m\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)=\\max \\left(m\\left(x^{(1)}\\right), m\\left(x^{(2)}\\right)\\right) $$ $$f(x)=\\left[e^{m\\left(x^{(1)}\\right)-m(x)} f\\left(x^{(1)}\\right) \\quad e^{m\\left(x^{(2)}\\right)-m(x)} f\\left(x^{(2)}\\right)\\right] $$ $$\\ell(x)=\\ell\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)=e^{m\\left(x^{(1)}\\right)-m(x)} \\ell\\left(x^{(1)}\\right)+e^{m\\left(x^{(2)}\\right)-m(x)} \\ell\\left(x^{(2)}\\right) $$ $$\\operatorname{softmax}(x)=\\frac{f(x)}{\\ell(x)}$$\n分块的 softmax 伪代码: 在第一个循环中同时对最大值\\(m\\)以及 softmax 的分母\\(d\\)进行更新，从而减少了一个循环。通过 tiling 的方式，softmax 的循环数从三个减到了两个，从而可以降低内存消耗。\nflashattention 伪代码： 中间变量：\\(O_i\\)(最终乘积)、\\(l_i\\)（softmax的分母，即累加和）、\\(m_i\\)（遍历到当前块为止的最大值），再也不用保存全部的S和P了。\n由于重新计算导致 FLOPs 增加，但是由于大量减少HBM访问，FlashAttention 运行速度更快\nFlashAttention的 FLOPs 为 \\(𝑂(𝑁^2𝑑)\\)，除了 input 和 output，额外需要的内存为 \\(𝑂(𝑁)\\), 对HBM访问的次数为 \\(𝑂(𝑁^2𝑑^2𝑀^{−1})\\), 比标准 Attention 的 \\(O(Nd+N^2)\\)更高效\nPyTorch 2.0已将 FlashAttention 集成到官方库中，可以直接调用 torch.nn.functional.scaled_dot_product_attention\n总结 # FlashAttention V1:\n通过切块技术减少了内存访问次数，提高了计算速度和内存利用率。 内存访问复杂度为 \\(𝑂(𝑁^2𝑑^2𝑀^{−1})\\), 比标准 Attention 的 \\(O(Nd+N^2)\\)更高效 ","date":"2024-05-23","externalUrl":null,"permalink":"/posts/llm/flash_attention/","section":"Blogs","summary":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness 与标准 attention 相比，Flash","title":"Flash Attention","type":"posts"},{"content":"class Me: def __init__(self): self.names = \u0026#34;Victor Yang\u0026#34; self.prefer_name = \u0026#34;Victor\u0026#34; self.profession = \u0026#34;Programmer\u0026#34; self.hometown = \u0026#34;Shandong, CN\u0026#34; self.current_location = \u0026#34;Beijing, CN\u0026#34; self.grad_school = \u0026#34;USTC\u0026#34; self.undergrad_school = \u0026#34;UPC\u0026#34; self.interests = [\u0026#34;ML\u0026#34;, \u0026#34;Robot\u0026#34;] self.hoppies = [\u0026#34;Reading\u0026#34;, \u0026#34;Yoga\u0026#34;, \u0026#34;Meditation\u0026#34;, \u0026#34;Taoist\u0026#34;, \u0026#34;Swimming\u0026#34;] ","date":"2024-05-19","externalUrl":null,"permalink":"/about/","section":"Welcome to Blowfish! 🎉","summary":"class Me: def __init__(self): self.names = \u0026#34;Victor Yang\u0026#34; self.prefer_name = \u0026#34;Victor\u0026#34; self.profession = \u0026#34;Programmer\u0026#34; self.hometown = \u0026#34;Shandong, CN\u0026#34; self.current_location = \u0026#34;Beijing, CN\u0026#34; self.","title":"About","type":"page"},{"content":"This is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing. 🚀\nYou can also use these content pages to define Hugo metadata like titles and descriptions that will be used for SEO and other purposes.\n","date":"0001-01-01","externalUrl":null,"permalink":"/tags/advanced/","section":"Tags","summary":"This is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing.","title":"Advanced","type":"tags"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/posts/ai-framework/","section":"Blogs","summary":"","title":"AI Framework","type":"posts"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/tags/_index.zh-cn/","section":"Tags","summary":"","title":"标签","type":"tags"},{"content":"Hell\n","date":"0001-01-01","externalUrl":null,"permalink":"/_index.zh-cn/","section":"Welcome to Blowfish! 🎉","summary":"Hell","title":"欢迎来到 Blowfish! 🎉","type":"page"},{"content":"这是高级标记。类似其他 Blowfish 中的其他列表页面，你可以在分类列表页添加自定义内容，这部分内容会显示在顶部。🚀\n你也可以用这些内容来定义 Hugo 的元数据，比如标题和描述。这些内容可以被用来增强 SEO 或其他目的。\n","date":"0001-01-01","externalUrl":null,"permalink":"/tags/advanced/_index.zh-cn/","section":"Tags","summary":"这是高级标记。类似其他 Blowfish 中的其他列表页面","title":"高级","type":"tags"}]