
[{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/posts/","section":"Blogs","summary":"","title":"Blogs","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/tags/cuda/","section":"Tags","summary":"","title":"CUDA","type":"tags"},{"content":"CUDA 程序获得高性能的必要（但不充分）条件有：\n数据传输比例较小 核函数的算术强度较高（计算访存比） 核函数中定义的线程数目较多 在编写与优化 CUDA 程序时，要想方设法（设计算法）做到：\n减少主机与设备之间的数据传输 提高核函数的算术强度（计算访存比） 增大核函数的并行规模 ","date":"2024-05-24","externalUrl":null,"permalink":"/posts/cuda/cuda_optimize/","section":"Blogs","summary":"CUDA 程序获得高性能的必要（但不充分）条件有","title":"CUDA Optimization","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/series/cuda-parallel-programming/","section":"Series","summary":"","title":"CUDA Parallel Programming","type":"series"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/posts/cuda/","section":"Blogs","summary":"","title":"CUDA Parallel Programming 学习笔记","type":"posts"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"2024-05-24","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"Hi, welcome to my blog.\n","date":"2024-05-24","externalUrl":null,"permalink":"/","section":"Welcome to My Blog","summary":"Hi, welcome to my blog.","title":"Welcome to My Blog","type":"page"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/attention/","section":"Tags","summary":"","title":"Attention","type":"tags"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/series/attention-and-optimization/","section":"Series","summary":"","title":"Attention and Optimization","type":"series"},{"content":"[WIP]\nFlashAttention V2:\n在V1的基础上减少了非矩阵乘法运算的FLOPs。 通过并行化和任务分配优化提高了计算速度和GPU利用率，性能提升了2-3倍。 Flash-Decoding借鉴了FlashAttention的优点，将并行化维度扩展到keys/values序列长度，提高了推理速度。 Flash-Decoding几乎不用额外存储大量数据到全局内存中，减少了内存开销。 Flash-Decoding++通过异步softmax和统一最大值、flat GEMM优化和双缓冲、启发式数据流和硬件资源适应等方法进一步提高了LLM推理的性能。 ","date":"2024-05-23","externalUrl":null,"permalink":"/posts/llm/flash_attention_2/","section":"Blogs","summary":"[WIP] FlashAttention V2: 在V1的基础上减少了非矩阵乘法运算","title":"Flash Attention V2","type":"posts"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/posts/llm/","section":"Blogs","summary":"","title":"LLM","type":"posts"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/nlp/","section":"Tags","summary":"","title":"NLP","type":"tags"},{"content":"","date":"2024-05-23","externalUrl":null,"permalink":"/tags/transformer/","section":"Tags","summary":"","title":"Transformer","type":"tags"},{"content":" Global Memory # 全局内存的访问模式，有合并（coalesced）与非合并（uncoalesced）之分。\n合并访问指的是一个线程束对全局内存的一次访问请求（读或者写）导致最少数量的数据传输，否则称访问是非合并的。\n合并度（degree of coalescing）等于线程束请求的字节数除以由该请求导致的所有数据传输处理的字节数。如果所有数据传输中处理的数据都是线程束所需要的，那么合并度就是 100%，即对应合并访问。也可以将合并度理解为一种资源利用率。利用率越高，核函数中与全局内存访问有关的部分的性能就更好\n顺序的合并访问:\nvoid __global__ add(float *x, float *y, float *z) { int n = threadIdx.x + blockIdx.x * blockDim.x; z[n] = x[n] + y[n]; } add\u0026lt;\u0026lt;\u0026lt;128, 32\u0026gt;\u0026gt;\u0026gt;(x, y, z); 第一个线程块中的线程束将访问数组 x 中第 0-31 个元素，对应 128 字节的连续内存，而且首地址一定是 256 字节的整数倍。这样的访问只需要 4 次数据传输即可完成，所以是合并访问，合并度为 100%\n乱序的合并访问:\nvoid __global__ add_permuted(float *x, float *y, float *z) { int tid_permuted = threadIdx.x ^ 0x1;//是将 0-31 的整数做某种置换（交换两个相邻的数） int n = tid_permuted + blockIdx.x * blockDim.x; z[n] = x[n] + y[n]; } add_permuted\u0026lt;\u0026lt;\u0026lt;128, 32\u0026gt;\u0026gt;\u0026gt;(x, y, z); 第一个线程块中的线程束将依然访问数组 x 中第 0-31 个元素，只不过线程号与数组元素指标不完全一致而已，合并度也为 100%\n不对齐的非合并访问:\nvoid __global__ add_offset(float *x, float *y, float *z) { int n = threadIdx.x + blockIdx.x * blockDim.x + 1; z[n] = x[n] + y[n]; } add_offset\u0026lt;\u0026lt;\u0026lt;128, 32\u0026gt;\u0026gt;\u0026gt;(x, y, z); 第一个线程块中的线程束将访问数组 x 中第 1-32 个元素。假如数组 x 的首地址为 256字节，该线程束将访问设备内存的 260-387 字节。这将触发 5 次数据传输，对应的内存地址分别是256-287 字节、288-319 字节、320-351 字节、352-383 字节和 384-415 字节，合并度为 4/5 = 80%\n跨越式的非合并访问\nvoid __global__ add_stride(float *x, float *y, float *z) { int n = blockIdx.x + threadIdx.x * gridDim.x; z[n] = x[n] + y[n]; } add_stride\u0026lt;\u0026lt;\u0026lt;128, 32\u0026gt;\u0026gt;\u0026gt;(x, y, z); 第一个线程块中的线程束将访问数组 x 中指标为 0、128、256、384 等的元素。每一对数据都不在一个连续的 32 字节的内存片段，故该线程束的访问将触发 32 次数据传输，合并度为 4/32 = 12.5%\nCUDA Kernel - Matrix Transpose # 对于多维数组，x 维度的线程指标 threadIdx.x 是最内层的（变化最快），所以相邻的 threadIdx.x 对应相邻的线程，即对threadIdx.x相邻的数据的访问是连续的。\nCUDA中，顺序读的性能理论上高于非顺序读，但是实际性能一致。因为从帕斯卡架构开始，如果编译器能够判断一个全局内存变量在整个核函数的范围都只可读（如这里的矩阵 A），则会自动用函数 __ldg() 读取全局内存，从而对数据的读取进行缓存，缓解非合并访问带来的影响。\n但是写操作没有这种自动配置，所以 顺序写的实际性能高于非顺序写。所以，CUDA中访问全局内存时，要注意优先做到顺序写。\n__global__ void transpose1(const real *A, real *B, const int N) { const int nx = blockIdx.x * blockDim.x + threadIdx.x; const int ny = blockIdx.y * blockDim.y + threadIdx.y; if (nx \u0026lt; N \u0026amp;\u0026amp; ny \u0026lt; N) { B[nx * N + ny] = A[ny * N + nx]; //顺序读，非顺序写 } } __global__ void transpose2(const real *A, real *B, const int N) { const int nx = blockIdx.x * blockDim.x + threadIdx.x; const int ny = blockIdx.y * blockDim.y + threadIdx.y; if (nx \u0026lt; N \u0026amp;\u0026amp; ny \u0026lt; N) { B[ny * N + nx] = A[nx * N + ny];//顺序写，非顺序读 } } __global__ void transpose3(const real *A, real *B, const int N) { const int nx = blockIdx.x * blockDim.x + threadIdx.x; const int ny = blockIdx.y * blockDim.y + threadIdx.y; if (nx \u0026lt; N \u0026amp;\u0026amp; ny \u0026lt; N) { B[ny * N + nx] = __ldg(\u0026amp;A[nx * N + ny]);//顺序写，自动化 } } 并行配置：\nint N=1024; int grid_x = (N+32)/N); //32 = float32的大小，一个thread访问一个float32数据 dim3 grid(grid_x, grid_x); dim3 block(32,32)//block最多1024个threads int M = sizeof(real) * N2; real *d_A, *d_B; cudaMallocManaged(\u0026amp;d_A, M); cudaMallocManaged(\u0026amp;d_B, M); 在Tesla T4上测试结果：\ntranspose with coalesced read: Time = 0.193677 +- 0.000643045 ms. transpose with coalesced write: Time = 0.129763 +- 0.00107203 ms. transpose with coalesced write and __ldg read: Time = 0.130074 +- 0.00122755 ms. Shared memory # 上面矩阵转置例子中，对全局内存的读和写这两个操作，总有一个是合并的，另—个是非合并的。利用共享内存可以改善全局内存的访问模式，使得对全局内存的读和写都是合并的。\n核心思路是用一个 block 处理 BLOCK_SIZE * BLOCK_SIZE 的矩阵块\nconst int block_size = 32; __global__ void matrix_trans(int* in, int* out){ __shared__ int buf[block_size]; int i = blockIdx.x * blockDim.x + threadIdx.x; int j = blockIdx.y * blockDim.y + threadIdx.y; int n = in.size(); if(i\u0026lt;n \u0026amp;\u0026amp; j\u0026lt;n){ buf[threadIdx.y][threadIdx.x] = in[i*n+j]; } __syncthreads(); i = blockIdx.y * blockDim.y + threadIdx.x; j = blockIdx.x * blockDim.x + threadIdx.y; if(i\u0026lt;n\u0026amp;\u0026amp;j\u0026lt;n\u0026gt;){ out[j*n+i] = buf[threadIdx.x][threadIdx.y]; } } CUDA Kernel - Array Reduce # 一个有 N（\\(10^8\\)） 个元素的数组 x，假如我们需要计算该数组中所有元素的和，即 sum = x[0] + x[1] + \u0026hellip; + x[N - 1]。\nTodo: CUDA编程：基础与实践 P83\nReference:\nCUDA编程：基础与实践 ","date":"2024-05-23","externalUrl":null,"permalink":"/posts/cuda/cuda_memory/","section":"Blogs","summary":"Global Memory # 全局内存的访问模式，有合并（coa","title":"CUDA Memory and Optimization","type":"posts"},{"content":"","date":"2024-05-21","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"2024-05-21","externalUrl":null,"permalink":"/tags/pytorch/","section":"Tags","summary":"","title":"Pytorch","type":"tags"},{"content":"","date":"2024-05-21","externalUrl":null,"permalink":"/series/pytorch/","section":"Series","summary":"","title":"PyTorch","type":"series"},{"content":"","date":"2024-05-21","externalUrl":null,"permalink":"/posts/pytorch/","section":"Blogs","summary":"","title":"PyTorch","type":"posts"},{"content":" PyTorch 代码结构 # PyTroch 主要由C10、ATen、torch三大部分组成：\ntorch/ 下包含 import 和使用的 Python 模块 torch/csrc/ 包含了 PyTorch 前端的 C++ 代码及C++前端代码。具体而言，它包含了 Python 和 C++ 之间转换的binding代码， autograd 引擎和 JIT 编译器等。 c10(Caffe Tensor Library), 包含 PyTorch 的核心抽象，存放最基础的Tensor库代码，包括 Tensor 和 Storage 数据结构的实际实现，可以运行在服务端和移动端。 最具代表性的class是 TensorImpl ，实现了Tensor的最基础框架。继承者和使用者有： Variable的Variable::Impl SparseTensorImpl detail::make_tensor(storage_impl, CUDATensorId(), false) Tensor(c10::intrusive_ptr\u0026lt;TensorImpl, UndefinedTensorImpl\u0026gt; tensor_impl) c10::make_intrusive\u0026lt;at::TensorImpl, at::UndefinedTensorImpl\u0026gt; ATen(A Tensor library for C++11)，包含声明和定义 Tensor 运算相关逻辑的代码，是实现张量运算的 C++ 库，kernel代码大多在这里 包含 C++ 实现的native算子和 C 实现的legacy算子(TH, THC, THNN, THCUNN) . aten/src/ATen/gen.py 用来动态生成一些ATen相关的代码 PyTroch 的编译过程 # 入口 setup.py； 提前检查依赖项； 使用 cmake 生成 Makefile Make: 产生中间源文件 Make: 编译三方库 Make: 生成静态库、动态库、可执行文件 Make: Copy文件到合适路径 setuptools, build_py setuptools, build_ext setuptools, install_lib PyTorch 工作流和计算图 # PyTorch 1.0 整体工作流：\n使用 imperative / eager 的范式，每一行代码都构建一个图作为完整计算图的一部分。即使完整的计算图还没有完成构建，也可以独立执行这些作为组件的小计算图，这种动态计算图被称为define-by-run Eager 模式适合块做原型、实验、debug，Script 模式(torch.jit)适合做优化与部署 动态图 # 假设PyTorch的autograd系统是一个 graph，那么每个 Function 实例就是 graph 中的节点，各个 Function 实例之间通过 Edge 连接。Edge 是个 struct，(Function, input_nr) 组合可以代表一个 edge\nstruct Edge { ... std::shared_ptr\u0026lt;Function\u0026gt; function; uint32_t input_nr; }; Function 的成员变量 next_edges_ 就是一组 Edge 实例，代表当前Function实例的返回值要输出到哪个Function\nFunction 的 input, ouput 都是 Variable实例，因此，当一个 graph 被执行时，Variable 实例就在这些 edge 之间来流动，传输信息\nFunction 的成员变量 sequence number，随着Function实例的不断构建而单调增长\nJIT # Code/AST -\u0026gt; Parsing-\u0026gt; Checking -\u0026gt; Optimization -\u0026gt; Translation -\u0026gt; Execution\nJIT 主要会输入代码或 Python 的抽象句法树（AST），其中 AST 会用树结构表征 Python 源代码的句法结构。 Parsing可能是解析句法结构和计算图，然后语法检测接连着代码优化过程，最后只要编译并执行就可以 优化计算图，如展开循环、指令转换等 执行，与 Python 解释器可以执行代码一样，PyTorch JIT 过程中也有一个解释器执行中间表征指令 PyTorch 从 Python 代码到 kernel # PyTorch 从 Python 代码到 kernel 的中间过程十分复杂, 在进入内核之前，所有代码都是自动生成的\n假设调用 torch.add()，流程如下：\nPython 域转换到 C++ 域（Python 参数解析） 处理 VariableType dispatch 处理 DeviceType/布局 dispatch 执行kernel(native kernel 或 TH kernel) ATen 动态生成的代码 # Type继承体系，包含头文件和源文件 Type继承体系是联系 Tensor op 与 legacy 的 TH 或 native kernel 的纽带 Type继承体系维护了2/3级分发机制 Declarations.yaml，会被Torch模块动态生成代码调用 生成 Tensor 类 生成Type家族注册初始化的代码 生成 legacy 的 TH/THC 的kernel声明 生成 native kernel 的声明 PyTroch Tensor # #在python中定义了Parameter类 class Parameter(torch.Tensor) #在python中定义了torch.Tensor类 class Tensor(torch._C._TensorBase) #在C++中定义了Variable类 struct TORCH_API Variable : public at::Tensor //PyObject* Py_InitModule(char *name, PyMethodDef *methods) //创建torch._C Py_InitModule(\u0026#34;torch._C\u0026#34;, methods.data()） //创建 torch._C._TensorBase PyModule_AddObject(module, \u0026#34;_TensorBase\u0026#34;, (PyObject *)\u0026amp;THPVariableType); Tensor运算 Dispatch 机制中的 Type 继承体系 # Type类派生出了TypeExtendedInterface，TypeExtendedInterface又派生了TypeDefault。TypeDefault又派生了CUDATypeDefault、CPUTypeDefault、VariableType（实现了autograd）、UndefinedType等。其中，根据 density 和 scaler type 的不同：\nCUDATypeDefault派生了：\nCUDAIntType CUDAShortType SparseCUDACharType CUDADoubleType CUDAByteType CUDACharType SparseCUDAByteType CUDAFloatType SparseCUDALongType CUDALongType CUDAHalfType SparseCUDAShortType SparseCUDADoubleType SparseCUDAIntType SparseCUDAFloatType CPUTypeDefault派生了：\nSparseCPUShortType CPUFloatType CPUHalfType CPUDoubleType CPUByteType SparseCPUFloatType SparseCPUIntType SparseCPUDoubleType CPUCharType SparseCPUByteType CPUIntType CPULongType SparseCPULongType SparseCPUCharType CPUShortType Type继承体系的作用\nPyTorch Kernel 组成 # Error checking, TORCH CHECK Output allocation Dtype dispatch Parallelization Data access 未完待续\u0026hellip;\n","date":"2024-05-21","externalUrl":null,"permalink":"/posts/pytorch/pytorch/","section":"Blogs","summary":"PyTorch 代码结构 # PyTroch 主要由C10、ATen、t","title":"PyTorch Architecture","type":"posts"},{"content":"","date":"2024-05-20","externalUrl":null,"permalink":"/tags/paged-attention/","section":"Tags","summary":"","title":"Paged Attention","type":"tags"},{"content":" vLLM # vLLM是吞吐性能卓越的大模型推理框架，PagedAttention是vLLM最大的创新点： Efficient Memory Management for Large Language Model Serving with PagedAttention\nvLLM中的attention计算，在推理的prefill阶段, 使用第三方库xformers的优化实现，decoding阶段使用 CUDA kernel 实现(csrc/attention/attention_kernels.cu，大约800多行)。\nAttention计算时使用页式管理 KV Cache 来提高内存利用率，进而提高吞吐量。\nPaged Attention(PA) # vLLM中有两个版本的 PA，其中：\nV1 源于 FasterTransformers 的 MHA，适用于 len(seq) \u0026lt; 8192 或 num_seqs * num_heads \u0026gt; 512 的情况。 V2 参考了 Flash Decoding方式，对 sequence 的维度进行切分来增加并行粒度 Paged Attention V1 # Block table in PA\n一个 req 中包含多个 seq 时，可以共享blocks\nPaged Attention V1 CUDA Kernel(vLLM) # csrc/attention/attention_kernels.cu\nsingle_query attention 函数\nDispatch逻辑：\nCALL_KERNEL_LAUNCHER_BLOCK_SIZE 根据存储的kv blocksize进行派发，分别是 8， 16， 32 LAUNCH_ATTENTION_KERNEL 根据注意力头大小HEADSIZE静态派发 并行任务的划分：\ndim3 grid(num_heads, num_seqs， 1) dim3 block(NUM_THREADS), 线程数是128，每个 block 负责完成 output 矩阵一行（head_size个元素）结果的 attention 计算 block 的线程划分为若干个 Warp, 每个 Warp 的32个线程划分为 blk_size 个 thread group Kernel 输入参数\nout[num_seqs, num_heads, head_size] q[num_seqs, num_heads, head_size] k_cache[num_blocks, num_kv_heads, head_size/x, block_size, x] # x表示一个向量化的大小，如float16 -\u0026gt; 16 / sizeof(float16) = 8 v_cache[num_blocks, num_kv_heads, head_size, block_size] head_mapping[num_heads] # 使用MQA, GQA时的kv_head block_tables[num_seqs, max_num_blocks_per_seq] # 维护各个Q对应KVCache的哪些block context_lens[num_seqs] # 用于变长 num_head： Q 的 head 数 num_kv_heads：K, V 的 head 数，MHA 的 num_kv_heads = num_head，GQA、MQA 的 num_kv_heads \u0026lt; num_head blk_size # block_size，每个page block存储的元素数量，每个page存(blk_size, num_head，head_size)个K、V的元素\nKernel 的常量定义：\nTHREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1) 通过WARPSIZE / BLOCKSIZE 得到一个thread_group大小。注意这里的BLOCKSIZE不是cuda blocksize，而是一个kv block的大小(默认值16) NUM_TOKENS_PER_THREAD_GROUP = (BLOCK_SIZE + WARP_SIZE - 1) / - WARP_SIZE 表示每个thread_group处理多少个token NUM_WARPS 表示一个threadblock有多少个warp VEC_SIZE 表示向量化大小，保证每个thread_group一次性获取16bytes，MAX(16 / (THREAD_GROUP_SIZE * sizeof(scalar_t)), 1) NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE 表示每个thread要负责多少个数据计算 NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE 表示每个thread负责的数据经过向量化后，一共有多少个vec V_VEC_SIZE = MIN(16 / sizeof(scalar_t), BLOCK_SIZE) 每个thread一次性读取16bytes NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE。对于v_cache[head_size, block_size]，表示一行需要几个V_VEC NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW 表示一个warp可以处理多少行 NUM_ROWS_PER_THREAD 表示每个thread需要负责多少行 Kernel 代码逻辑：\n（1）循环从显存读取\\(Q\\)到 shared memory：\n迭代读取，每 CUDA block 负责读取\\(Q\\)的一行（head_size 个元素）存入 shared memory。其中，block 的每个 Warp 负责读取 16blk_size 字节的 Q，即每个 thread group 会读取16字节的 Q，16blk_size 字节的 Q 对应 sequence 的一个 head。\nconst int thread_group_idx = thread_idx / THREAD_GROUP_SIZE; const int thread_group_offset = thread_idx % THREAD_GROUP_SIZE; // Load the query to registers. // Each thread in a thread group has a different part of the query. // For example, if the the thread group size is 4, then the first thread in // the group has 0, 4, 8, ... th vectors of the query, and the second thread // has 1, 5, 9, ... th vectors of the query, and so on. NOTE(woosuk): Because // q is split from a qkv tensor, it may not be contiguous. const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE; __shared__ Q_vec q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD]; #pragma unroll for (int i = thread_group_idx; i \u0026lt; NUM_VECS_PER_THREAD; i += NUM_THREAD_GROUPS) { const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE; q_vecs[thread_group_offset][i] = *reinterpret_cast\u0026lt;const Q_vec*\u0026gt;(q_ptr + vec_idx * VEC_SIZE); } __syncthreads(); （2）循环从显存读取\\(K\\)到 register，并计算QK：\n每个 seq 包含 cxt_length * num_kv_heads * head_size 个元素 每个 CUDA block 负责计算一个 seq 的一个 head 的 \\(QK^T\\)， 只需要读取 ctx_length * head_size 个 K 的元素 因为页式内存管理，K 在 ctx_length 维度的存储不连续，以 blk_size 个 token 为粒度分布在不同的内存地址，所以需要根据 Q 的 head_idx 和 seq_idx 访问 block_table 找到 K 的 physical_block_num K Cache的布局为 [num_blocks, num_kv_heads, head_size/x, block_size, x]， 目的是优化写入 shared memory。Q和K的同一行元素被读入寄存器并进行点乘运算后，结果要写入shared memory。如果一个 Warp 中所有线程都计算 Q、K 同一行数据，会导致写入 shared memory 的同一个位置，这将造成 warp 内不同线程顺序地写入。所以 warp 的线程最好计算 Q和K 的不同行数据。在设计 K 布局时，将 block_size 放在比 head_size 更低的维度。由于warp size大于block_size，我们需要将head_size拆分为head_size/x和x两个维度，借x到最低维度，以确保每个线程读入的数据量和计算量都足够大。最后，每个线程组派一个线程去写入shared memory，这样一个warp有blk_size个线程并行写入shared memory，从而增加了shared memory的访问带宽。这种设计策略是为了实现高效的并行计算和内存访问，以提高整体的计算性能。 读取 K 需要一个循环，循环中每个CUDA block中的所有 warp 依次访问num_blocks 个 page block。每次迭代： 每个 warp 负责访问连续的 blk_size 个 KCache 的行数据（blk_size * head_size个元素）。每个 thread group 负责访问 KCache 的一行，将head_size 个元素读入寄存器 寄存器中的Q和K元素进行点乘，结果写入shared memory。一个 CUDA block 的 shared memory 存储了一行 QK^T 的结果，共 ctx_length 个元素 CUDA block 对 shared memory 中元素进行 max，sum 方式 reduction，然后计算得到 softmax 的结果 代码步骤：\ngroup是由block大小决定的，当block\u0026gt;32时，每个warp实现了一个group,否则在一个warp中实现多个group\n每个warp负责计算一个block KCache，而每个block key shape为 [block_size, num_head, head_size]\n每个thread_group取一个key，即num_head个元素，计算QK dot\n只有thread_group的第一个thread负责将QK结果写入shared memory\nhead_idx标记GPU BLOCKs，也即每个GPU Blocks计算一个head\nnum_heads标记使用的GPU BLOCKs总数，也即head num\nseq_idx标记的是第二维GPU BLOCKs， 也即seq的位置\n分配red_smem[2*NUM_WARPS]为reduce所用，保留的是warp内的局部最大值。后面计算了qvec的dot结果保存为qk，先在group内reduce计算得到局部最大值，然后在每个warp内reduce计算得到全局最大值为qk_max。\n// 每个warp负责 blocksize * headsize个元素 // block_idx是block cache中的序号（逻辑序号） for (int block_idx = warp_idx; block_idx \u0026lt; num_blocks; block_idx += NUM_WARPS) { // TODO(Zhengzekang) // 定位物理块 const int physical_block_number = block_table[block_idx]; // ... K_vec k_vecs[NUM_VECS_PER_THREAD]; // 遍历每个thread_group处理多少个token for (int i = 0; i \u0026lt; NUM_TOKENS_PER_THREAD_GROUP; i++) { const int physical_block_offset = (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE; const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset; K_vec k_vecs[NUM_VECS_PER_THREAD]; // 遍历每个thread需要处理多少个VEC for (int j = 0; j \u0026lt; NUM_VECS_PER_THREAD; j++) { // vectorized取到key k_vecs[j] = xxxx; } // 计算QKdot，里面包含了一个thread_groupsize的WarpReduceSum， float qk = scale * Qk_dot\u0026lt;scalar_t, THREAD_GROUP_SIZE\u0026gt;::dot(q_vecs, k_vecs); // 只有thread_group的第一个thread负责将QK结果写入shared memory // 并且维护一个qk_max，用于后续softmax if (thread_group_offset == 0) { // Store the partial reductions to shared memory. // NOTE(woosuk): It is required to zero out the masked logits. const bool mask = token_idx \u0026gt;= context_len; logits[token_idx] = mask ? 0.f : qk; // Update the max value. qk_max = mask ? qk_max : fmaxf(qk_max, qk); } } } 此时各个thread_group已经完成了自己的qk_dot操作，并且都维护了qk_max。下面就需要和其他thread_group做warp shuffle操作，得到一个warp内的qk max值。\n由于每个thread_group里的thread内维护的qk_max是一样的，所以warp shuffle只需到 thread_group_size即可停止。并由lane_id = 0的线程将warp里的qk_max存储到smem，最后再做一次warpreduce，得到一个block里的qkmax值，通过shfl_sync广播操作，让每个线程都拿到max\n#pragma unroll for (int mask = WARP_SIZE / 2; mask \u0026gt;= THREAD_GROUP_SIZE; mask /= 2) { qk_max = fmaxf(qk_max, __shfl_xor_sync(uint32_t(-1), qk_max, mask)); } if (lane == 0) { red_smem[warp_idx] = qk_max; } __syncthreads(); // TODO(woosuk): Refactor this part. // Get the max qk value for the sequence. qk_max = lane \u0026lt; NUM_WARPS ? red_smem[lane] : -FLT_MAX; #pragma unroll for (int mask = NUM_WARPS / 2; mask \u0026gt;= 1; mask /= 2) { qk_max = fmaxf(qk_max, __shfl_xor_sync(uint32_t(-1), qk_max, mask)); } // Broadcast the max qk value to all threads. qk_max = __shfl_sync(uint32_t(-1), qk_max, 0); 接下来就是常规的softmax\n执行exp(x-qk_max)并得到每个warp上的exp_sum，规约得全局（所有warp）的exp_sum,计算每个节点上的softmax\n// Get the sum of the exp values. float exp_sum = 0.f; for (int i = thread_idx; i \u0026lt; context_len; i += NUM_THREADS) { float val = __expf(logits[i] - qk_max); logits[i] = val; exp_sum += val; } exp_sum = block_sum\u0026lt;NUM_WARPS\u0026gt;(\u0026amp;red_smem[NUM_WARPS], exp_sum); // Compute softmax. const float inv_sum = __fdividef(1.f, exp_sum + 1e-6f); for (int i = thread_idx; i \u0026lt; context_len; i += NUM_THREADS) { logits[i] *= inv_sum; } __syncthreads(); （3）从显存读取\\(V\\)到 register, 计算 softmax(QK^T)V\n和KCache一样，CUDA block 依次访问 num_blk 个 VCahce block 到寄存器，每个 warp 负责 1 个 VCache block，。不过这里不需要以 thread group 为单位访问16字节，而是每个 thread 读取16字节的元素到寄存器，然后与shared memory的 softmax(QK^T)中间结果 对应位置16字节的数据进行点乘，得到一个 float 结果，写到 output 的对应位置中。\n为了读写连续，将V_cache转置，shape为：[num_blocks, num_kv_heads, head_size, block_size]\n注意这里使用了fp32模式以防止累加过程中的精度损失\n// 每个线程一次性读16bytes数据 constexpr int V_VEC_SIZE = MIN(16 / sizeof(scalar_t), BLOCK_SIZE); using V_vec = typename Vec\u0026lt;scalar_t, V_VEC_SIZE\u0026gt;::Type; using L_vec = typename Vec\u0026lt;scalar_t, V_VEC_SIZE\u0026gt;::Type; using Float_L_vec = typename FloatVec\u0026lt;L_vec\u0026gt;::Type; // 每一行有多少个V_VEC，假设BLOCK_SIZE=8，那么NUM_V_VECS_PER_ROW=1 constexpr int NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE; // 一个WARP一次处理多少行，按照上面假设，这里是32 constexpr int NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW; // 每个thread需要负责多少行，假设headsize=128，那么每个thread要处理4行 constexpr int NUM_ROWS_PER_THREAD = (HEAD_SIZE + NUM_ROWS_PER_ITER - 1) / NUM_ROWS_PER_ITER; // 提前分配accumulate buffer，用float累加 float accs[NUM_ROWS_PER_THREAD]; #pragma unroll for (int i = 0; i \u0026lt; NUM_ROWS_PER_THREAD; i++) { accs[i] = 0.f; } for (int block_idx = warp_idx; block_idx \u0026lt; num_blocks; block_idx += NUM_WARPS) { // ... #pragma unroll for (int i = 0; i \u0026lt; NUM_ROWS_PER_THREAD; i++) { const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER; if (row_idx \u0026lt; HEAD_SIZE) { const int offset = row_idx * BLOCK_SIZE + physical_block_offset; V_vec v_vec = *reinterpret_cast\u0026lt;const V_vec*\u0026gt;(v_ptr + offset); accs[i] += dot(logits_vec, v_vec); } } } （4）更新最终的结果\n将一个block分成上半部分warp和下半部分warp。上半部分warp(warp_id \u0026gt; mid)将自己累加的结果写到shared memory。下半部分warp将之前上半部分warp存到shared_memory 的结果取出，进行累加。这样重复，当warp_idx==0时，将所有结果写回到每一行中。\n// Perform reduction across warps. float* out_smem = reinterpret_cast\u0026lt;float*\u0026gt;(shared_mem); #pragma unroll for (int i = NUM_WARPS; i \u0026gt; 1; i /= 2) { int mid = i / 2; // Upper warps write to shared memory. if (warp_idx \u0026gt;= mid \u0026amp;\u0026amp; warp_idx \u0026lt; i) { float* dst = \u0026amp;out_smem[(warp_idx - mid) * HEAD_SIZE]; #pragma unroll for (int i = 0; i \u0026lt; NUM_ROWS_PER_THREAD; i++) { const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER; if (row_idx \u0026lt; HEAD_SIZE \u0026amp;\u0026amp; lane % NUM_V_VECS_PER_ROW == 0) { dst[row_idx] = accs[i]; } } } } __syncthreads(); // Lower warps update the output. if (warp_idx \u0026lt; mid) { const float* src = \u0026amp;out_smem[warp_idx * HEAD_SIZE]; #pragma unroll for (int i = 0; i \u0026lt; NUM_ROWS_PER_THREAD; i++) { const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER; if (row_idx \u0026lt; HEAD_SIZE \u0026amp;\u0026amp; lane % NUM_V_VECS_PER_ROW == 0) { accs[i] += src[row_idx]; } } } __syncthreads(); // Write the final output. if (warp_idx == 0) { scalar_t* out_ptr = out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE + head_idx * max_num_partitions * HEAD_SIZE + partition_idx * HEAD_SIZE; #pragma unroll for (int i = 0; i \u0026lt; NUM_ROWS_PER_THREAD; i++) { const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER; if (row_idx \u0026lt; HEAD_SIZE \u0026amp;\u0026amp; lane % NUM_V_VECS_PER_ROW == 0) { from_float(*(out_ptr + row_idx), accs[i]); } } } 为什么 VCache 的 layout 是 [num_blocks, num_kv_heads, head_size, block_size]，和 KCache layout 不一样？ 因为 V 要去做点乘的对象在shared memory，只需要读，不涉及并行写。\nPA V1 和 Flash Attention 的区别 # 并行任务的划分方式不同\nFlashAttention 用了两层循环，每次写一个 Tile 的 output tensor，而 PA 只有一层循环，每次写一行 output tensor。因为每次迭代都有整行的 QK^T 中间结果，不需要online softmax PA V1 设计的 KCache layout 充分利用了 shared memory 写带宽 PA V1 的缺陷 # 不足：\n不适合 seq 很长的情况，因为没有沿着 ctx_length 或者 batch 维度做切分 和MHA相比，MQA和GAQ没有减少对KV Cache的读写次数。读K、V Cache时候只是做了一个head_idx的转换，会重复从显存读相同的head 未完待续\u0026hellip;\nReference:\nvllm Efficient Memory Management for Large Language Model Serving with PagedAttention PageAttention代码走读 vLLM kernel ","date":"2024-05-20","externalUrl":null,"permalink":"/posts/llm/paged_attention_v1/","section":"Blogs","summary":"vLLM # vLLM是吞吐性能卓越的大模型推理框","title":"Paged Attention V1(vLLM)","type":"posts"},{"content":"","date":"2024-05-20","externalUrl":null,"permalink":"/tags/vllm/","section":"Tags","summary":"","title":"Vllm","type":"tags"},{"content":"","date":"2024-05-20","externalUrl":null,"permalink":"/series/vllm/","section":"Series","summary":"","title":"VLLM","type":"series"},{"content":"","date":"2024-05-20","externalUrl":null,"permalink":"/tags/conv/","section":"Tags","summary":"","title":"Conv","type":"tags"},{"content":" CUDA Conv # 在 PyTorch 上实现Conv很简单\nimport torch from torch.nn.functional import conv2d device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) import time width = 1000 height = 1000 img =torch.randn([width,height]) img = img.to(device) kernel = torch.tensor([[-1.0, 0.0, 1.0], [-1.0, 0.0, 1.0], [-1.0, 0.0, 1.0]]) img = torch.reshape(img, (1, 1, width, height)) kernel = torch.reshape(kernel, (1, 1, 3, 3)) kernel = kernel.to(device) start = time.perf_counter() output = F.conv2d(img, kernel, stride=1).to(device) end = time.perf_counter() print(f\u0026#39;total_cost: {end-start} ms\u0026#39;) print(f\u0026#39;output_size: {output.shape}\u0026#39;) print(f\u0026#39;output_tensor: {output}\u0026#39;) 用CUDA实现Conv，Steps:\nCheck Get thread info Memory allocation Copy data to device Call conv kernel Copy data to host Free memory //file: conv.cu #include\u0026lt;stdint.h\u0026gt; #include\u0026lt;cuda.h\u0026gt; #include \u0026#34;cuda_runtime.h\u0026#34; #include \u0026#34;device_launch_parameters.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;math.h\u0026gt; const int NUM_REPEATS = 10; #define CHECK(call) \\ do \\ { \\ const cudaError_t error_code = call; \\ if (error_code != cudaSuccess) \\ { \\ printf(\u0026#34;CUDA Error:\\n\u0026#34;); \\ printf(\u0026#34; File: %s\\n\u0026#34;, __FILE__); \\ printf(\u0026#34; Line: %d\\n\u0026#34;, __LINE__); \\ printf(\u0026#34; Error code: %d\\n\u0026#34;, error_code); \\ printf(\u0026#34; Error text: %s\\n\u0026#34;, \\ cudaGetErrorString(error_code)); \\ exit(1); \\ } \\ } while (0) static void HandleError(cudaError_t err, const char* file, int line) { if (err != cudaSuccess) { printf(\u0026#34;%s in %s at line %d\\n\u0026#34;, cudaGetErrorString(err), file, line); exit(EXIT_FAILURE); } } #define HANDLE_ERROR(err) (HandleError(err, __FILE__, __LINE__)) int getThreadNum() { cudaDeviceProp prop; int count; CHECK(cudaGetDeviceCount(\u0026amp;count)); printf(\u0026#34;GPU num: %d\\n\u0026#34;, count); CHECK(cudaGetDeviceProperties(\u0026amp;prop, 0)); printf(\u0026#34;Max thread num per block: %d\\n\u0026#34;, prop.maxThreadsPerBlock); printf(\u0026#34;Max grid dim: %d, %d, %d)\\n\u0026#34;, prop.maxGridSize[0], prop.maxGridSize[1], prop.maxGridSize[2]); return prop.maxThreadsPerBlock; } __global__ void conv(float* img, float* kernel, float* result, int width, int height, int kernelSize) { int ti = threadIdx.x; int bi = blockIdx.x; int id = (bi * blockDim.x + ti); if (id \u0026gt;= width * height) { return; } int row = id / width; int col = id % width; for (int i = 0; i \u0026lt; kernelSize; ++i) { for (int j = 0; j \u0026lt; kernelSize; ++j) { float imgValue = 0; int curRow = row - kernelSize / 2 + i; int curCol = col - kernelSize / 2 + j; if (curRow \u0026lt; 0 || curCol \u0026lt; 0 || curRow \u0026gt;= height || curCol \u0026gt;= width) { } else { imgValue = img[curRow * width + curCol]; } result[id] += kernel[i * kernelSize + j] * imgValue; } } } int main() { int width = 1000; int height = 1000; float* img = new float[width * height]; for (int row = 0; row \u0026lt; height; ++row) { for (int col = 0; col \u0026lt; width; ++col) { img[col + row * width] = (col + row) % 256; } } int kernelSize = 3; float* kernel = new float[kernelSize * kernelSize]; for (int i = 0; i \u0026lt; kernelSize * kernelSize; ++i) { kernel[i] = i % kernelSize - 1; } float* imgGpu; float* kernelGpu; float* resultGpu; CHECK(cudaMalloc((void**)\u0026amp;imgGpu, width * height * sizeof(float))); CHECK(cudaMalloc((void**)\u0026amp;kernelGpu, kernelSize * kernelSize * sizeof(float))); CHECK(cudaMalloc((void**)\u0026amp;resultGpu, width * height * sizeof(float))); CHECK(cudaMemcpy(imgGpu, img, width * height * sizeof(float), cudaMemcpyHostToDevice)); CHECK(cudaMemcpy(kernelGpu, kernel, kernelSize * kernelSize * sizeof(float), cudaMemcpyHostToDevice)); int threadNum = getThreadNum(); int blockNum = (width * height - 0.5) / threadNum + 1; float t_sum = 0; float t2_sum = 0; for (int repeat = 0; repeat \u0026lt;= NUM_REPEATS; ++repeat) { cudaEvent_t start, stop; CHECK(cudaEventCreate(\u0026amp;start)); CHECK(cudaEventCreate(\u0026amp;stop)); CHECK(cudaEventRecord(start)); cudaEventQuery(start); conv \u0026lt;\u0026lt; \u0026lt;blockNum, threadNum \u0026gt;\u0026gt; \u0026gt; (imgGpu, kernelGpu, resultGpu, width, height, kernelSize); CHECK(cudaEventRecord(stop)); CHECK(cudaEventSynchronize(stop)); float elapsed_time; CHECK(cudaEventElapsedTime(\u0026amp;elapsed_time, start, stop)); printf(\u0026#34;Time = %g ms.\\n\u0026#34;, elapsed_time); if (repeat \u0026gt; 0) { t_sum += elapsed_time; t2_sum += elapsed_time * elapsed_time; } CHECK(cudaEventDestroy(start)); CHECK(cudaEventDestroy(stop)); } const float t_ave = t_sum / NUM_REPEATS; const float t_err = sqrt(t2_sum / NUM_REPEATS - t_ave * t_ave); printf(\u0026#34;Time = %g +- %g ms.\\n\u0026#34;, t_ave, t_err); float* result = new float[width * height]; CHECK(cudaMemcpy(result, resultGpu, width * height * sizeof(float), cudaMemcpyDeviceToHost)); // visualization printf(\u0026#34;img\\n\u0026#34;); for (int row = 0; row \u0026lt; 10; ++row) { for (int col = 0; col \u0026lt; 10; ++col) { printf(\u0026#34;%2.0f \u0026#34;, img[col + row * width]); } printf(\u0026#34;\\n\u0026#34;); } printf(\u0026#34;kernel\\n\u0026#34;); for (int row = 0; row \u0026lt; kernelSize; ++row) { for (int col = 0; col \u0026lt; kernelSize; ++col) { printf(\u0026#34;%2.0f \u0026#34;, kernel[col + row * kernelSize]); } printf(\u0026#34;\\n\u0026#34;); } printf(\u0026#34;result\\n\u0026#34;); for (int row = 0; row \u0026lt; 10; ++row) { for (int col = 0; col \u0026lt; 10; ++col) { printf(\u0026#34;%2.0f \u0026#34;, result[col + row * width]); } printf(\u0026#34;\\n\u0026#34;); } return 0; } ","date":"2024-05-20","externalUrl":null,"permalink":"/posts/cuda/cuda_conv/","section":"Blogs","summary":"CUDA Conv # 在 PyTorch 上实现Conv很简单 import torch from torch.nn.functional import","title":"CUDA Conv","type":"posts"},{"content":"","date":"2024-05-18","externalUrl":null,"permalink":"/tags/gpu/","section":"Tags","summary":"","title":"GPU","type":"tags"},{"content":" CPU 与 GPU 的不同 # CPU，4个 ALU，主要负责逻辑计算，1个控制单元 Control，1个 DRAM，1个 Cache GPU，绿色小方块看作 ALU，红色框看作一个 SM，SM 中的多个 ALU share 一个Control 和 Cache，SM 可以看作一个多核 CPU，但是 ALU 更多，control 更少，也就是算力提升，控制力减弱 所以，CPU 适合控制逻辑复杂的任务，GPU 适合逻辑简单、数据量大、计算量大的任务。\nGPU, CUDA, AI Framework 的关系 # Reference:\nNVIDIA CUDA Docs cuda编程学习 一张图了解GPU、CUDA、CUDA toolkit和pytorch的关系 GPU 内存概念浅析 GPU 内部结构 # 每一个 SM 有自己的 Wrap scheduler 、寄存器（Register）、指令缓存、L1缓存、共享内存。\nA100 中每个 SM 包括 4 个 SM partition（SMP），里边绿色的就是 Streaming Processor（SP），也叫 CUDA cores，它们是实际执行计算的基本单元。\n所有的 SM 共享 L2 缓存。整个 GPU 内存结构如下图所示\nGPU 内存结构 # 按照存储功能进行细分，GPU 内存可以分为：局部内存（local memory）、全局内存（global memory）、常量内存（constant memory）、共享内存（shared memory）、寄存器（register）、L1/L2 缓存等。\n其中全局内存、局部内存、常量内存都是片下内存(off-chip)，储存在 HBM 上。所以 HBM 的大部分作为全局内存。\non-chip：L1/L2 cache：多级缓存，在 GPU 芯片内部\noff-chip：GPU DRAM/HBM, global memory\nL2 缓存可以被所有 SM 访问，速度比全局内存快。Flash attention 的思路就是尽可能地利用 L2 缓存，减少 HBM 的数据读写时间\nL1 缓存用于存储 SM 内的数据，被 SM 内的 CUDA cores 共享，但是跨 SM 之间的 L1 不能相互访问\n局部内存 (local memory) 是线程独享的内存资源，线程之间不可以相互访问。局部内存属于off-chip，所以访问速度跟全局内存一样。它主要是用来应对寄存器不足时的场景，即在线程申请的变量超过可用的寄存器大小时，nvcc 会自动将一部数据放置到片下内存里。\n寄存器（register）是线程能独立访问的资源，它是片上（on chip）存储，用来存储一些线程的暂存数据。寄存器的速度是访问中最快的，但是它的容量较小，只有几百甚至几十 KB，而且要被许多线程均分\n共享内存（shared memory） 是一种在线程块内能访问的内存，是片上（on chip）存储，访问速度较快。共享内存主要是缓存一些需要反复读写的数据。共享内存与 L1 缓存的位置、速度极其类似，区别在于共享内存的控制与生命周期管理与 L1 不同：共享内存受用户控制，L1 受系统控制。共享内存更利于线程块之间数据交互。\n常量内存（constant memory）是片下（off chip）存储，但是通过特殊的常量内存缓存（constant cache）进行缓存读取，它是只读内存。常量内存主要是解决一个 warp scheduler 内多个线程访问相同数据时速度太慢的问题。假设所有线程都需要访问一个 constant_A 的常量，在存储介质上 constant_A 的数据只保存了一份，而内存的物理读取方式决定了多个线程不能在同一时刻读取到该变量，所以会出现先后访问的问题，这样使得并行计算的线程出现了运算时差。常量内存正是解决这样的问题而设置的，它有对应的 cache 位置产生多个副本，让线程访问时不存在冲突，从而保证并行度。\n内存类型 物理位置 访问权限 可见范围 生命周期 全局内存 在芯片外 可读可写 所有线程和主机端 由主机分配与释放 常量内存 在芯片外 仅可读 所有线程和主机端 由主机分配与释放 纹理和表面内存 在芯片外 一般仅可读 所有线程和主机端 由主机分配与释放 寄存器内存 在芯片内 可读可写 单个线程 所在线程 局部内存 在芯片外 可读可写 单个线程 所在线程 共享内存 在芯片内 可读可写 单个线程块 所在线程块 “全局内存”（global memory）的含义是核函数中的所有线程都能够访问其中的数 据. 一般用 cudaMalloc 函数为全局内存变量分配设备内存，用 cudaMemcpy(h_z, d_z, M, cudaMemcpyDeviceToHost) 或 cudaMemcpyDeviceToDevice 拷贝内存。在处理逻辑上的两维或三维问题时，可以用 cudaMallocPitch 和 cudaMalloc3D 函数分配内存，用 cudaMemcpy2D 和 cudaMemcpy3D 复制数据，释放时依然用 cudaFree 函数。\n静态全局内存变量： 所占内存数量是在编译期间就确定的。而且，这样的静态全局内存变量必须在所有主机与设备函数外部定义，所以是一种“全局的静态全局内存变量”。在核函数中，可直接对静态全局内存变量进行访问，并不需要将它们以参数的形式传给核 函数。静态全局内存变量由以下方式在任何函数外部定义：\n__device__ T x; // 单个变量 __device__ T y[N]; // 固定长度的数组 不可在主机函数中直接访问静态全局内存变量，但可以用 cudaMemcpyToSymbol 函 数和 cudaMemcpyFromSymbol 函数在静态全局内存与主机内存之间传输数据。\n常量内存（constant memory）是有常量缓存的全局内存，数量有限，一共仅有 64 KB。它的可见范围和生命周期与全局内存一样。常量内存仅可读、不可写。访问速度比全局内存高，但得到高访问速度的前提是一个线程束中的线程（一个线程块中相邻的 32 个线程）要读取相同的常量内存数据。常量内存的方法是在核函数外面用 constant 定义变量，用 cudaMemcpyToSymbol 将数据从主机端复制到设备的常量内存后 供核函数使用\n在核函数中定义的不加任何限定符的变量一般来说就存放于寄存器（register）中。核函数中定义的不加任何限定符的数组有可能存放于寄存器中，但也有可能存放于局部内存中。如 gridDim、blockDim、blockIdx、threadIdx 及 warpSize 都保存在特殊的寄存器中。这里的 n 就是一个寄存器变量:\nconst int n = blockDim.x * blockIdx.x + threadIdx.x; 寄存器变量仅仅被一个线程可见。\n局部内存： 寄存器中放不下的变量，以及索引值不能在编译时就确定的数组，都有可能放在局部内存中。\n共享内存和寄存器类似，存在于芯片上，具有仅次于寄存器的读写速度，数量也有限。共享内存对整个线程块可见，主要作用是减少对全局内存的访问。\nSM 的构成 # 一个GPU是由多个SM(Streaming Multiprocessor)构成的。一个SM包含如下资源：\n一定数量的寄存器 一定数量的共享内存 常量内存的缓存 纹理和表面内存的缓存 L1缓存 两个（计算能力6.0）或4个（其他计算能力）线程束调度器（warp scheduler）用于不同线程的上下文之间迅速地切换，以及为准备就绪的线程束发出执行指令 执行核心，包括： 若干整型数运算的核心（INT32） 若干单精度浮点数运算的核心（FP32） 若干双精度浮点数运算的核心（FP64） 若干单精度浮点数超越函数（transcendental functions）的特殊函数单元（special function units, SFUs） 若干混合精度的张量核心（tensor cores，由伏特架构引入，适用于机器学习中的低精度矩阵计算） SM的占有率： 在并行规模足够大（即核函数执行配置中定义的总线程数足够多）的前提下分几种情况来分析SM的理论占有率：\n(1) 寄存器和共享内存使用量很小的情况。此时，SM的占有率完全由执行配置中的线程块大小决定。关于线程块大小，读者也许注意到我们之前一直用128。这是因为，SM中线程的执行是以线程束为单位的，所以最好将线程块大小取为线程束大小（32个线程）的整数倍。例如，假设将线程块大小定义为100，那么一个线程块中将有3个完整的线程束（一共96个线程）和一个不完整的线程束（只有4个线程）。在执行核函数中的指令时，不完整的线程束花的时间和完整的线程束花费的时间一样，这就无形中浪费了计算资源。所以，建议将线程块大小取为32的整数倍。在该前提下，任何不小于$N_t/N_b$而且能整除$N_t$的线程块大小都能得到100%的占有率；线程块大小不小于64时其他架构能获得100%的占有率。根据我们列出的数据，线程块大小不小于128时开普勒架构能获得100%的占有率；线程块大小不小于64时其他架构能获得100%的占有率。作者近几年都用一块开普勒架构的Tesla K40开发程序，所以习惯了在一般情况下用128的线程块大小。\n(2) 有限寄存器数量对占有率的约束情况。我们只针对第三节中列出的几个计算能力进行分析，读者可以类似地分析其他未列出的计算能力。对于第三节中列出的所有计算能力，一个SM最多能使用的寄存器个数为64K（64 x 1024）。除图灵架构外，如果我们希望在一个SM中驻留最多的线程（2048个），核函数中的每个线程最多只能用32个寄存器。当每个线程所用寄存器个数大于64时，SM的占有率将小于50%；当每个线程所用寄存器个数大于128时，SM的占有率将小于25%。对于图灵架构，同样的占有率允许使用更多的寄存器。\n(3) 有限的共享内存对占有率的约束清理。因为共享内存的数量随着计算能力的上升没有显著的变化规律，所以我们这里仅对计算能力3.5进行分析，对其他计算能力可以类似地分析。如果线程块大小为128，那么每个SM要激活16个线程块才能有2048个线程，达到100%的占有率。此时，一个线程块最多能使用3KB的共享内存。在不改变线程块大小的情况下，要达到50%的占有率，一个线程块最多能使用6KB的共享内存；要达到25%的占有率，一个线程块最多能使用12KB的共享内存。如果一个线程块使用了超过48KB的共享内存，会直接导致核函数无法允许。对其他线程块大小可进行类似的分析。\n在 CUDA 工具箱中，有一个 CUDA_Occupancy_Calculator.xls，可用来计算各种情况下的 SM 占有率。\n## 重要的计算能力技术指标 SM 寄存器数上限、单个线程块寄存器数上限、单个线程寄存器数上限、SM 共享内存上限、单个线程块共享内存上限、SM 线程块上限、 SM 线程数上限 代码中查询技术指标： ```c++ #include \u0026#34;error.cuh\u0026#34; //CHECK #include \u0026lt;cstdio\u0026gt; int main(int argc, char *argv[]) { int device_id = 0; if (argc \u0026gt; 1) device_id = atoi(argv[1]); CHECK(cudaSetDevice(device_id)); cudaDeviceProp prop; CHECK(cudaGetDeviceProperties(\u0026amp;prop, device_id)); printf(\u0026#34;Device id: %d\\n\u0026#34;, device_id); printf(\u0026#34;Device name: %s\\n\u0026#34;, prop.name); printf(\u0026#34;Compute capability: %d.%d\\n\u0026#34;, prop.major, prop.minor); printf(\u0026#34;Amount of global memory: %g GB\\n\u0026#34;, prop.totalGlobalMem / (1024.0 * 1024 * 1024)); printf(\u0026#34;Amount of constant memory: %g KB\\n\u0026#34;, prop.totalConstMem / 1024.0); printf(\u0026#34;Maximum grid size: %d %d %d\\n\u0026#34;, prop.maxGridSize[0], prop.maxGridSize[1], prop.maxGridSize[2]); printf(\u0026#34;Maximum block size: %d %d %d\\n\u0026#34;, prop.maxThreadsDim[0], prop.maxThreadsDim[1], prop.maxThreadsDim[2]); printf(\u0026#34;Number of SMs: %d\\n\u0026#34;, prop.multiProcessorCount); printf(\u0026#34;Maximum amount of shared memory per block: %g KB\\n\u0026#34;, prop.sharedMemPerBlock / 1024.0); printf(\u0026#34;Maximum amount of shared memory per SM: %g KB\\n\u0026#34;, prop.sharedMemPerMultiprocessor / 1024.0); printf(\u0026#34;Maximum number of registers per block: %d K\\n\u0026#34;, prop.regsPerBlock / 1024); printf(\u0026#34;Maximum number of registers per SM: %d K\\n\u0026#34;, prop.regsPerMultiprocessor / 1024); printf(\u0026#34;Maximum number of threads per block: %d\\n\u0026#34;, prop.maxThreadsPerBlock); printf(\u0026#34;Maximum number of threads per SM: %d\\n\u0026#34;, prop.maxThreadsPerMultiProcessor); } ## Tensor Core CUDA core 和 Tensor core 的区别： - Tensor core 是在 Volta 以及之后的架构中才有的, 相比于CUDA core，它可以提供更高效的运算。 - 每个 GPU clock，CUDA core 可以进行一次单精度乘加运算，即：in fp32: x += y * z。 - 每个 GPU clock，Tensor core 可以完成 4 × 4 的混合精度矩阵乘加 (matrix multiply-accumulate, MMA)：D=A * B + C，其中 A、B、C、D 都是 4 × 4 矩阵。A 和 B是 FP16 矩阵，而累加矩阵 C 和 D 可以是 FP16 或 FP32 矩阵（FP16/FP16 或 FP16/FP32 两种模式。所以每个 GPU clock，Tensor core 可以执行 64 个浮点 FMA 混合精度运算（4 × 4 × 4）。 - Turing 架构中新增了 INT8/INT32, INT4/INT32, INT1/INT32 等模式 ![Tensor Core](https://img2024.cnblogs.com/blog/798398/202402/798398-20240219182403297-314074416.png) V100 中，一个 SM 中有 8 个 Tensor core，每个 GPU clock 共可以执行 1024 个浮点运算（64 × 8 × 2，乘以 2 因为乘加是两个浮点运算） Reference: - [TENSOR CORE DL PERFORMANCE GUIDE](https://link.zhihu.com/?target=https%3A//developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9926-tensor-core-performance-the-ultimate-guide.pdf) - [GPU内存概念浅析](https://www.cnblogs.com/ArsenalfanInECNU/p/18021724) ","date":"2024-05-18","externalUrl":null,"permalink":"/posts/cuda/gpu/","section":"Blogs","summary":"CPU 与 GPU 的不同 # CPU，4个 ALU，主要负","title":"GPU 结构","type":"posts"},{"content":" Kernel Function # gridDim\ngridDim.x、gridDim.y、gridDim.z分别表示 grid 各个维度的大小\nblockDim\nblockDim.x、blockDim.y、blockDim.z分别表示 block 各个维度的大小\nblockIdx\nblockIdx.x、blockIdx.y、blockIdx.z分别表示当前 block 在 grid 中的坐标\nthreadIdx\nthreadIdx.x、threadIdx.y、threadIdx.z分别表示当前 thread 在 block 的坐标\ngrid 里总的线程个数 N = gridDim.x * gridDim.y * gridDim.z * blockDim.x * blockDim.y * blockDim.z\n通过 blockIdx.x、blockIdx.y、blockIdx.z、threadIdx.x、threadIdx.y、threadIdx.z 可以定位一个线程的坐标。\n主流架构一个block三个唯独的设置最多为(1024， 1024， 64)，同时总线程数最多只能有 1024 个。\n将所有的线程排成一个序列，序列号为 0 , 1 , 2 , … , N ，如何找到当前 thread 的序列号 ?\n先找到该thread所在的 block的序号 blockId = blockIdx.x + blockIdx.ygridDim.x + blockIdx.zgridDim.x*gridDim.y 然后找到当前 thread 在 block 中的序号 threadId = threadIdx.x + threadIdx.yblockDim.x + threadIdx.zblockDim.x*blockDim.y 计算一个 block 中一共有多少个 thread， M = blockDim.xblockDim.yblockDim.z 求得当前的线程的序列号 idx = threadId + M*blockId Device function # kernel 可以调用不带执行配置的自定义函数，这样的自定义函数称为设备函数（devicefunction）。它是在设备中执行，并在设备中被调用的。与之相比，核函数是在设备中执行，但在主机端被调用的。\ndouble __device__ add1_device(const double x, const double y) { return (x + y); } void __global__ add1(const double *x, const double *y, double *z, const int N) { const int n = blockDim.x * blockIdx.x + threadIdx.x; if (n \u0026lt; N) { z[n] = add1_device(x[n], y[n]); } } CUDA 常用的函数 # 同步函数 __syncthreads: 只能用在核函数中 __syncthreads(),该函数可保证一个线程块中的所有线程（或者说所有线程束）在执行该语句后面的语句之前都完全执行了该语句前面的语句。然而，该函数只是针对同一个线程块中的线程的，不同线程块中线程的执行次序依然是不确定的。\nCUDA Event Record # 在 C++ 中，有多种可以对一段代码进行计时的方法，如使用 GCC 的 clock 函数和与头文件 对应的时间库、GCC 中的 gettimeofday 函数。\nCUDA 提供了一种基于 CUDA 事件（CUDA event）的计时方式，可用来给一段 CUDA 代码（可能包含了主机代码和设备代码）计时。下面的例子涵盖了计时的基本流程：\n//creat CHECK(cudaEventCreate(\u0026amp;start)); CHECK(cudaEventCreate(\u0026amp;stop)); //record CHECK(cudaEventRecord(start)); cudaEvent_t start, stop; cudaEventQuery(start) //+需要计时的代码块 CHECK(cudaEventRecord(stop)); CHECK(cudaEventSynchronize(stop)); float elapsed_time; //compute CHECK(cudaEventElapsedTime(\u0026amp;elapsed_time, start, stop)); printf(\u0026#34;Time = %g ms.\\n\u0026#34;, elapsed_time); //clean CHECK(cudaEventDestroy(start)); CHECK(cudaEventDestroy(stop)); Steps of CUDA Program # Config GPU(device), cudaSetDevice() Allocate device memory, cudaMalloc(), cudaMallocManaged() Allocate CPU(Host) memory Copy data from host to device, cudaMemcpy() Run kernel on device Copy result from device to host, cudaMemcpy() Print result on host Release host and device memory, cudaFree(), free() CPU is always called host, the GPU is called device\nC Example:\n// file: sample.cu #include\u0026lt;stdint.h\u0026gt; #include\u0026lt;cuda.h\u0026gt; #include \u0026#34;cuda_runtime.h\u0026#34; #include \u0026#34;device_launch_parameters.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; // This is a sample CUDA kernel, called on host and execte on device __global__ void add(float* a) { a[threadIdx.x] = 1; } int main(int argc, char** argv) { // 1. Set device cudaSetDevice(0); // 2. Allocate device memory float* dx; cudaMalloc((void**)\u0026amp;dx, 16 * sizeof(float)); // 3. Allocate host memory float hx[16] = { 0 }; // 4. Copy data from host to device cudaMemcpy(dx, hx, 16 * sizeof(float), cudaMemcpyHostToDevice); // 5. Run kernel on device add \u0026lt;\u0026lt; \u0026lt;1, 16 \u0026gt;\u0026gt; \u0026gt; (dx); // 6. Copy result from device to host cudaMemcpy(hx, dx, 16 * sizeof(float), cudaMemcpyDeviceToHost); // 7. Print result on host for (int i = 0; i \u0026lt; 16; i++) { printf(\u0026#34;%f \\n\u0026#34;, hx[i]); } 8. Release host and device memory cudaFree(dx); free(hx); //cudaDeviceReset(); return 0; } 使用nvcc编译,然后运行\nnvcc sample.cu - o sample ./sample CUDA 提供了统一内存: gpu和cpu可访问的单一内存空间. 调用cudaMallocManaged()，它返回一个指针，从host code或device code都可以访问。要释放数据，只需将指针传递给cudaFree()。\nCUDA Kernel and Parrallel Computing # 前置知识: 理解 GPU 结构, Grid, Block, Thread 这几个逻辑概念之间的关系\nCUDA kernel 的编程模型\n[Todo] Dim and size detail\n调用kernel: add \u0026lt;\u0026lt; \u0026lt;blockNumber, threadNumber \u0026gt;\u0026gt; \u0026gt; (dx);\n编写kernel：\n用关键字描述符 __global__ 声明kernel: __global__ void add(){} 调用 kernel 时的参数 \u0026lt;\u0026lt;\u0026lt;blockNumber per grid, threadNumber per block\u0026gt;\u0026gt;\u0026gt; 决定了共有 TotalThreadNum = blockNumber * threadNumber 个线程可以并行执行任务 kernel 内的每一次迭代，意味着 TotalThreadNum 个线程并行执行了一次循环体中的任务（即每个线程完成对一份数据的处理），也就是每次迭代能处理 TotalThreadNum 份数据，TotalThreadNum 也等价于跨步(stride)的大小 kernel 中 threadIdx.x 代表 the index of the thread within the block， blockDim.x 代表 the size of block（number of threads in block（假设 这里的 grid 和 block 的 dim 只有一维） kernel 内 threadIdx.x 和 blockIdx.x 的组合对应线程的唯一标识 以add_3这个 kernel 为例，可以用 index = blockIdx.x * blockDim.x + threadIdx.x 获得当前线程的要处理的数据的数组下标（见下图），\n__global__ void add_3(int n, float *x, float *y) { int index = blockIdx.x * blockDim.x + threadIdx.x; // stride为grid的线程总数:blockDim.x*gridDim.x int stride = blockDim.x * gridDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] = x[i] + y[i]; } Kernel examples(下面的 C++ Example)\n1个block,1个线程: add_1() 1个block,多个线程: add_2() 多个block,多个线程: add_3() 多个block，多个线程也称为网格跨步循环，其中每次循环的跨步(stride)为 grid 的线程总数: stride = blockDim.x * gridDim.x\nC++ Example:\n// file: add.cu #include \u0026lt;iostream\u0026gt; #include \u0026lt;math.h\u0026gt; // Kernel function to add the elements of two arrays // single thread __global__ void add_1(int n, float *x, float *y) { for (int i = 0; i \u0026lt; n; i++) y[i] = x[i] + y[i]; } // single block, multi threads __global__ void add_2(int n, float *x, float *y) { int index = threadIdx.x; int stride = blockDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] = x[i] + y[i]; } // multi block, multi threads // 网格跨步(stride)循环 __global__ void add_3(int n, float *x, float *y) { int index = blockIdx.x * blockDim.x + threadIdx.x; // stride为grid的线程总数:blockDim.x*gridDim.x int stride = blockDim.x * gridDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] = x[i] + y[i]; } int main(void) { int N = 1\u0026lt;\u0026lt;20; float *x, *y; // Allocate Unified Memory – accessible from CPU or GPU cudaMallocManaged(\u0026amp;x, N*sizeof(float)); cudaMallocManaged(\u0026amp;y, N*sizeof(float)); // initialize x and y arrays on the host for (int i = 0; i \u0026lt; N; i++) { x[i] = 1.0f; y[i] = 2.0f; } // Run kernel on 1M elements on the GPU // Just run with single thread :) add_1\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Run with 1 block and multi threads add_2\u0026lt;\u0026lt;\u0026lt;1, 256\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Run with multi block and multi threads int blockSize = 256;//并行线程数量 int numBlocks = (N + blockSize - 1) / blockSize;//线程块数量 add_3\u0026lt;\u0026lt;\u0026lt;numBlocks, blockSize\u0026gt;\u0026gt;\u0026gt;(N, x, y); // Wait for GPU to finish before accessing on host cudaDeviceSynchronize(); // Check for errors (all values should be 3.0f) float maxError = 0.0f; for (int i = 0; i \u0026lt; N; i++) maxError = fmax(maxError, fabs(y[i]-3.0f)); std::cout \u0026lt;\u0026lt; \u0026#34;Max error: \u0026#34; \u0026lt;\u0026lt; maxError \u0026lt;\u0026lt; std::endl; // Free memory cudaFree(x); cudaFree(y); return 0; } CUDA Code Profiling # nvprof是CUDA工具包附带的命令行GPU分析器\nReference:\nNVIDIA CUDA Docs CUDA C++ Programming Guide CUDA 矩阵乘法终极优化指南 ","date":"2024-05-10","externalUrl":null,"permalink":"/posts/cuda/cuda_basic/","section":"Blogs","summary":"Kernel Function # gridDim gridDim.x、gridDi","title":"CUDA Programming","type":"posts"},{"content":"","date":"2024-05-05","externalUrl":null,"permalink":"/tags/flash-attention/","section":"Tags","summary":"","title":"Flash Attention","type":"tags"},{"content":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n与标准 attention 相比，Flash Attention 有以下三点特点：\n运算速度更快 (Fast) 更节省显存 (Memory-Efficient) 计算结果相同 (Exact) FlashAttention 目的不是节约 FLOPs，而是减少对HBM的访问。它没有改变原有的计算公式，整体计算复杂度并未降低。\n背景 # GPU中存储单元主要有 HBM 和 SRAM：HBM 容量大但是访问速度慢，SRAM容量小却有着较高的访问速度。例如：A100 GPU有40-80GB的HBM，带宽为1.5-2.0TB/s；每108个流式多核处理器各有192KB的片上SRAM，带宽估计约为 19TB/s。可以看出，片上的SRAM比HBM快一个数量级，但尺寸要小许多数量级。\n当输入序列（sequence length）较长时，Transformer的计算过程缓慢且耗费内存，这是因为 self-attention 的 time 和 memory complexity 会随着 sequence length 的增加成二次增长。\n标准Attention的计算过程： $$ S=Q K^T \\in \\mathbb{R}^{N \\times N} $$ $$ P=\\operatorname{softmax}(S) \\in \\mathbb{R}^{N \\times N} $$ $$ O=P V \\in \\mathbb{R}^{N \\times N} $$\n标准Attention的中间结果 𝑆, 𝑃 通常需要通过高带宽内存（HBM）进行存取，两者所需内存空间复杂度为\\(O(Nd+N^2)\\), 对 HBM 的重复读写是主要瓶颈。要解决这个问题，需要做两件事：\n在不访问整个输入的情况下计算 softmax 不为反向传播存储大的中间 attention 矩阵(\\(N^2\\)) FlashAttention V1 # FlashAttention 提出了两种方法来解决上述问题：tiling 和 recomputation。\ntiling - 注意力计算被重新构造，将输入分割成块，并通过在输入块上进行多次传递来递增地执行softmax操作。 recomputation - 存储来自前向的 softmax 归一化因子，以便在反向中快速重新计算芯片上的 attention，这比从HBM读取中间矩阵的标准注意力方法更快。可以把它看作基于 tiling 的特殊的 gradient checkpointing 正常的softmax计算：\n$$m(x):=\\max _i x i$$ $$f(x):=\\left[e^{x_1-m(x)} \\ldots e^{x_B-m(x)}\\right]$$ $$\\ell(x):=\\sum_i f(x)_i$$ $$\\operatorname{softmax}(x):=\\frac{f(x)}{\\ell(x)}$$\nsoftmax 伪代码: softmax 函数需要三个循环，第一个循环计算数组的最大值，第二个循环计算 softmax 的分母，第三个循环计算 softmax 输出。\n分块的 softmax 计算(假设分2块并行计算)：\n$$m(x)=m\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)=\\max \\left(m\\left(x^{(1)}\\right), m\\left(x^{(2)}\\right)\\right) $$ $$f(x)=\\left[e^{m\\left(x^{(1)}\\right)-m(x)} f\\left(x^{(1)}\\right) \\quad e^{m\\left(x^{(2)}\\right)-m(x)} f\\left(x^{(2)}\\right)\\right] $$ $$\\ell(x)=\\ell\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)=e^{m\\left(x^{(1)}\\right)-m(x)} \\ell\\left(x^{(1)}\\right)+e^{m\\left(x^{(2)}\\right)-m(x)} \\ell\\left(x^{(2)}\\right) $$ $$\\operatorname{softmax}(x)=\\frac{f(x)}{\\ell(x)}$$\n复杂度分析：\n读取Q，K，写入\\(S=QK^T\\)，内存访问复杂度\\(O(Nd+N^2)\\) 读取S，写入\\(P=softmax(S)\\)，内存访问复杂度\\(O(N^2)\\) 读取V和P，写入\\(O=PV\\)，内存访问复杂度\\(O(Nd+N^2)\\) 综上，self-attention的 HBM 访问复杂度\\(O(Nd+N^2)\\)\n分块的 softmax 伪代码: 在第一个循环中同时对最大值\\(m\\)以及 softmax 的分母\\(d\\)进行更新，从而减少了一个循环。通过 tiling 的方式，softmax 的循环数从三个减到了两个，从而可以降低内存消耗。\nflashattention 伪代码： 中间变量：\\(O_i\\)(最终乘积)、\\(l_i\\)（softmax的分母，即累加和）、\\(m_i\\)（遍历到当前块为止的最大值），再也不用保存全部的S和P了。\n由于重新计算导致 FLOPs 增加，但是由于大量减少HBM访问，FlashAttention 运行速度更快\nFlashAttention的 FLOPs 为 \\(𝑂(𝑁^2𝑑)\\)，除了 input 和 output，额外需要的内存为 \\(𝑂(𝑁)\\), 对HBM访问的次数为 \\(𝑂(𝑁^2𝑑^2𝑀^{−1})\\), 其中 M 为SRAM的大小，当 \\(M=O(Nd)\\)时，对HBM访问的次数为\\(O(Nd)\\)， 远远小于标准 Attention 的 \\(O(Nd+N^2)\\)\nPyTorch 2.0已将 FlashAttention 集成到官方库中，可以直接调用 torch.nn.functional.scaled_dot_product_attention\n总结 # FlashAttention V1:\n通过切块技术减少了内存访问次数，提高了计算速度和内存利用率。 内存访问复杂度为 \\(𝑂(𝑁^2𝑑^2𝑀^{−1})\\), 比标准 Attention 的 \\(O(Nd+N^2)\\)更高效 Reference：\n一些改cuda加速的思路：FlashAttention、PagedAttention、LightSeq、ByteTransformer FlashAttention: 更快训练更长上下文的GPT 手撕Flash Attention ","date":"2024-05-05","externalUrl":null,"permalink":"/posts/llm/flash_attention/","section":"Blogs","summary":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness 与标准 attention 相比，Flash","title":"Flash Attention","type":"posts"},{"content":" Self-Attention # 对于self-attention，由于 Q, K, V 都来自输入 X ，在计算 \\(QT^T\\) 时，模型很容易关注到自身的位置上，也就是 \\(QT^T\\) 对角线上的激活值会明显比较大, 这会削弱模型关注其他高价值位置的能力，也就限制模型的理解和表达能力。MHA 对这个问题有一定的缓解作用。\nMHA # MHA（multi-head attention）\nQKV 经过线性变换后，将他们分别在 hidden states 维度上切分成 heads 份。\nMHA 相比单头的情况，相当于只是把 QKV 切成多份并行计算了，对于实际需要缓存的大小没有影响\nKV Cache # Decoding阶段，下一个step的输入其实包含了上一个step的内容，而且只在最后面多了一个token的数据，下一个step的计算应该也包含了上一个step的计算。\nKV Cache 的目的：空间换时间，用缓存把需要重复利用的中间计算结果存下来，减少重复计算。而 K 和 V 就是要缓存的对象。\nQ K V\n对于输入长度为 \\(s)\\ ，层数为 \\(L\\) ，hidden size为 \\(d\\) 的模型:\n当 batch size=1 时 需要缓存的参数量为: \\(2Ls*d\\)，其中 2 表示 K + V 需要的空间为（使用半精度浮点数 float16）：\\(22Lsd Bytes\\) ，其中第一个 2 表示 float16 占用 2 Bytes 当 batch size=B 时 需要缓存的参数量为: \\(2LsdB\\) 需要的空间为（使用半精度浮点数 float16）：\\(22Lsd*B Bytes\\) MHA 相比单头的情况，相当于只是把 QKV 切成多份并行计算了，对于实际需要缓存的大小没有影响 GQA、MQA [Todo] 以Llama2 7B为例，\\(L=32, d=4096\\)，此时每个 token 需要的 cache 空间为 524,288 Bytes(512 KB)，当 \\(s=1024, batch size=1)\\) 时，需要 512 MB\n主流显卡配置：\nNV A100(Ampere Arch)，HBM2e 40/80GB，L2 Cache 40MB, CUDA Cores - 16896/14592 NV H100(Hopper Arch), HBM2e/HBM3 80GB, L2 Cache 50MB, CUDA Cores 6912 NV V100(Volta Arch), HBM2 16/32GB, L2 Cache 6MB, CUDA Cores 5120 H100也只有50M的L2 Cache，只能支持Llama2 7B共100个tokens的seq，超出L2 Cache的部分只能走到显存中去了，但是 HBM 的 bandwidth 比 L2 Cache 小多了，A100 memory bandwidth 如下图所示：\nMQA # 《Fast Transformer Decoding: One Write-Head is All You Need》\nQ 经过线性变换后，MQA 只对 Q 进行 多个 head 的切分，每个 head 的 Q 的维度变为 \\(Q_s*(d/heads)\\)， K和V并不切分，而是线性变换时直接把hidden state维度降低为 d/heads， 然后 heads 个 Q 分别于 同一份 K，V 继续宁 attention 计算，最后将结果 concat 起来。\n比如在Llama2 7B中用了32个头，MQA后，1024个 tokens 需要 KVCache 就变成MHA 的 1/32，即 512MB/32=16MB，基本可以全部放入A100的L2 Cache\n由于共享了多个头的参数，限制了模型的表达能力，MQA虽然能好地支持推理加速，但是在效果上略略比MHA差一点\nGQA # 《GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》\nGQA（Grouped-Query Attention）提出了一个折中的办法，既能减少 MQA 的损失，又比 MHA 需要更少的缓存。\nGQA里， Q 还是按原来MHA/MQA的做法不变，但是 用多份 K和V，不过数量小于 Q 的 heads。相当于把 Q 的多个头给分了group，同一个group内的 Q 共享同一套 KV，不同group的 Q 所用的 KV 不同。\nGQA的速度相比MHA有明显提升，而效果上比MQA也好一些，能做到和MHA基本没差距。Llama2 70B 用的就是GQA。\n未完待续\u0026hellip;\nTodo: 代码，GQA量化计算\nReference:\n理解Attention:从起源到MHA,MQA和GQA self-attention code ","date":"2024-05-05","externalUrl":null,"permalink":"/posts/llm/attention/","section":"Blogs","summary":"Self-Attention # 对于self-attention，由","title":"Attention and KV Cache","type":"posts"},{"content":"","date":"2024-05-05","externalUrl":null,"permalink":"/tags/kvcache/","section":"Tags","summary":"","title":"KVCache","type":"tags"},{"content":" Me # class Me: def __init__(self): self.names = \u0026#34;Victor Yang\u0026#34; self.prefer_name = \u0026#34;Victor\u0026#34; self.profession = \u0026#34;Programmer\u0026#34; self.hometown = \u0026#34;Shandong, CN\u0026#34; self.current_location = \u0026#34;Beijing, CN\u0026#34; self.graduate = \u0026#34;University of Science and Technology of China(USTC), Software Engineering\u0026#34; self.undergraduate = \u0026#34;China University of Petroleum(UPC), Computer Science\u0026#34; self.interests = [\u0026#34;AI\u0026#34;, \u0026#34;Taoist\u0026#34;, \u0026#34;Traditional Chinese Medicine\u0026#34;, \u0026#34;History\u0026#34;, \u0026#34;Finance\u0026#34;, \u0026#34;Science Fiction\u0026#34;] self.hoppies = [\u0026#34;Reading\u0026#34;, \u0026#34;Yoga\u0026#34;, \u0026#34;Meditation\u0026#34;, \u0026#34;Swimming\u0026#34;] My Skills # Generative AI/AIGC: Transformer, LLM, VLM, Llama, Llava, MoE, Attention, vLLM, DeepSpeed, RAG, Langchain, Agent Parallel Computing: CUDA, GPU, Horovod, MPI Machine Learning: Deep Learning, PyTorch, Federated Learning Programming Language: Python, C++, GoLang Cloud Native: Knative, Ray, Docker Others: Linux ","date":"2024-05-01","externalUrl":null,"permalink":"/about/","section":"Welcome to My Blog","summary":"Me # class Me: def __init__(self): self.names = \u0026#34;Victor Yang\u0026#34; self.prefer_name = \u0026#34;Victor\u0026#34; self.profession = \u0026#34;Programmer\u0026#34; self.hometown = \u0026#34;Shandong, CN\u0026#34; self.","title":"About","type":"page"},{"content":"This is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing. 🚀\nYou can also use these content pages to define Hugo metadata like titles and descriptions that will be used for SEO and other purposes.\n","date":"0001-01-01","externalUrl":null,"permalink":"/tags/advanced/","section":"Tags","summary":"This is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing.","title":"Advanced","type":"tags"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/series/attention/","section":"Series","summary":"","title":"Attention","type":"series"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/tags/_index.zh-cn/","section":"Tags","summary":"","title":"标签","type":"tags"},{"content":"欢迎来到我的博客!\n","date":"0001-01-01","externalUrl":null,"permalink":"/_index.zh-cn/","section":"Welcome to My Blog","summary":"欢迎来到我的博客!","title":"欢迎来到我的博客","type":"page"},{"content":"这是高级标记。类似其他 Blowfish 中的其他列表页面，你可以在分类列表页添加自定义内容，这部分内容会显示在顶部。🚀\n你也可以用这些内容来定义 Hugo 的元数据，比如标题和描述。这些内容可以被用来增强 SEO 或其他目的。\n","date":"0001-01-01","externalUrl":null,"permalink":"/tags/advanced/_index.zh-cn/","section":"Tags","summary":"这是高级标记。类似其他 Blowfish 中的其他列表页面","title":"高级","type":"tags"}]