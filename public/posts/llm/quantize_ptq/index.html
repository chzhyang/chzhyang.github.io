<!DOCTYPE html>
<html lang="en" dir="ltr" class="scroll-smooth" data-default-appearance="light"
  data-auto-appearance="true"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  
  <meta http-equiv="content-language" content="en" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  
  <title>PTQ for LLM &middot; Victor&#39;s Blog</title>
  <meta name="title" content="PTQ for LLM &middot; Victor&#39;s Blog" />
  
  <meta name="description" content="LLM 常见的 PTQ 量化方法， 如 GPTQ, AWQ, PTQ, GGUF 等" />
  <meta name="keywords" content="NLP, Transformer, LLM, AI Quantization, " />
  
  
  <link rel="canonical" href="http://localhost:1313/posts/llm/quantize_ptq/" />
  
  
  
  
  
  
  
  
  
  
  <link type="text/css" rel="stylesheet" href="/css/main.bundle.min.ac59a1b8eaafef739c129d12787ac29f6a420bcb348c7aeec8949a8d0b7d6c2023766c169d14b9031b751504eb3c976efe786fec135b340a49868acd8caa4127.css"
    integrity="sha512-rFmhuOqv73OcEp0SeHrCn2pCC8s0jHruyJSajQt9bCAjdmwWnRS5Axt1FQTrPJdu/nhv7BNbNApJhorNjKpBJw==" />
  
  
  <script type="text/javascript" src="/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js"
    integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj&#43;e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script>
  
  
  
  
  
  
  
  
  
  <script defer type="text/javascript" id="script-bundle" src="/js/main.bundle.min.b36dda9ec4ec11e967fd341e7d57b8c484ac7a39f8f329f7bcf7ce4812569de8607db866a086d4789956ac0b74967e251c9a566d94d469dd28b8787fed4f46f5.js"
    integrity="sha512-s23ansTsEeln/TQefVe4xISsejn48yn3vPfOSBJWnehgfbhmoIbUeJlWrAt0ln4lHJpWbZTUad0ouHh/7U9G9Q==" data-copy="" data-copied=""></script>
  
  
  <script src="/js/zoom.min.js"></script>
  
  
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
  <link rel="manifest" href="/site.webmanifest" />
  
  
  
  
  
  
  
  <meta property="og:url" content="http://localhost:1313/posts/llm/quantize_ptq/">
  <meta property="og:site_name" content="Victor&#39;s Blog">
  <meta property="og:title" content="PTQ for LLM">
  <meta property="og:description" content="LLM 常见的 PTQ 量化方法， 如 GPTQ, AWQ, PTQ, GGUF 等">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-28T22:43:31+00:00">
    <meta property="article:modified_time" content="2024-06-01T00:00:00+00:00">
    <meta property="article:tag" content="NLP">
    <meta property="article:tag" content="Transformer">
    <meta property="article:tag" content="LLM">
    <meta property="article:tag" content="AI Quantization">
      <meta property="og:see_also" content="http://localhost:1313/posts/llm/quantize_datatype/">
      <meta property="og:see_also" content="http://localhost:1313/posts/llm/quantize_intro/">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="PTQ for LLM">
  <meta name="twitter:description" content="LLM 常见的 PTQ 量化方法， 如 GPTQ, AWQ, PTQ, GGUF 等">

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Blogs",
    "name": "PTQ for LLM",
    "headline": "PTQ for LLM",
    "description": "LLM 常见的 PTQ 量化方法， 如 GPTQ, AWQ, PTQ, GGUF 等",
    "abstract": "PTQ # PTQ(Post-Training Quantization)，即后训",
    "inLanguage": "en",
    "url" : "http:\/\/localhost:1313\/posts\/llm\/quantize_ptq\/",
    "author" : {
      "@type": "Person",
      "name": "Victor Yang"
    },
    "copyrightYear": "2024",
    "dateCreated": "2024-05-28T22:43:31\u002b00:00",
    "datePublished": "2024-05-28T22:43:31\u002b00:00",
    
    "dateModified": "2024-06-01T00:00:00\u002b00:00",
    
    "keywords": ["NLP","Transformer","LLM","AI Quantization"],
    
    "mainEntityOfPage": "true",
    "wordCount": "3788"
  }]
  </script>


  
  
  <meta name="author" content="Victor Yang" />
  
  
  
  <link href="https://github.com/chzhyang" rel="me" />
  
  
  <link href="https://www.linkedin.com/in/victor-yang-0101x" rel="me" />
  
  
  <link href="mailto:chzhyangchn@gmail.com" rel="me" />
  
  
  
  

<script src="/lib/jquery/jquery.slim.min.js" integrity=""></script>











<link type="text/css" rel="stylesheet" href="/lib/katex/katex.min.af6f85907cdfd6ed3e4906d93a8233d91c5638859f744cd1b98c9b1a3ccaab5f231bdcda49f132f6cdfe8cca86ff33ed468affab8a9a502610a664a34f0f0cfa.css" integrity="sha512-r2&#43;FkHzf1u0&#43;SQbZOoIz2RxWOIWfdEzRuYybGjzKq18jG9zaSfEy9s3&#43;jMqG/zPtRor/q4qaUCYQpmSjTw8M&#43;g==" />


<script defer src="/lib/katex/katex.min.20da6cf7343619410c0900fbc626506c65159ea9f312f9729d5cba7aa713707378f9a4222e8f7fb9a42a7240e9749f199b7334401b3e3e4b60e29cf490492552.js" integrity="sha512-INps9zQ2GUEMCQD7xiZQbGUVnqnzEvlynVy6eqcTcHN4&#43;aQiLo9/uaQqckDpdJ8Zm3M0QBs&#43;Pktg4pz0kEklUg=="></script>


<script defer src="/lib/katex/auto-render.min.6095714e3aadb63b14ddc4af69346ab12974c1b460654345f8d1860a0b68fcc51b22f68b757433193090bb80afc8965b65cb607e5541d0f5f0f4b2e64d69b9ff.js" integrity="sha512-YJVxTjqttjsU3cSvaTRqsSl0wbRgZUNF&#43;NGGCgto/MUbIvaLdXQzGTCQu4CvyJZbZctgflVB0PXw9LLmTWm5/w=="
  onload="renderMathInElement(document.body);"></script>







































































































































  
  


  
  
  <meta name="theme-color"/>
  
  
</head>
<body
  class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32">
  <div id="the-top" class="absolute flex self-center">
    <a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
      href="#main-content"><span
        class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a>
  </div>
  
  
  <div style="padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px"
    class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3">
    
    <div class="flex flex-1 items-center justify-between">
        <nav class="flex space-x-3">

            
            <a href="/" class="text-base font-medium text-gray-500 hover:text-gray-900">Victor&rsquo;s Blog</a>
            

        </nav>
        <nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12">

            
            
            
  <a href="/posts/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
    
    <p class="text-base font-medium" title="">
        Blogs
    </p>
</a>


            
            
  <a href="/series/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
    
    <p class="text-base font-medium" title="">
        Series
    </p>
</a>


            
            
  <a href="/tags/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
    
    <p class="text-base font-medium" title="">
        Tags
    </p>
</a>


            
            
  <a href="/about/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
    
    <p class="text-base font-medium" title="">
        About
    </p>
</a>


            
            

            


            
            <button id="search-button" aria-label="Search" class="text-base hover:text-primary-600 dark:hover:text-primary-400"
                title="">
                

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


            </button>
            


            
            
            <div
                class="ltr:mr-14 rtl:ml-14 flex items-center">
                <button id="appearance-switcher" aria-label="Dark mode switcher" type="button" class="text-base hover:text-primary-600 dark:hover:text-primary-400">
                    <div class="flex items-center justify-center dark:hidden">
                        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>

  </span>


                    </div>
                    <div class="items-center justify-center hidden dark:flex">
                        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>

  </span>


                    </div>
                </button>
            </div>
            

        </nav>
        <div class="flex md:hidden items-center space-x-5 md:ml-12 h-12">

            <span></span>

            


            
            <button id="search-button-mobile" aria-label="Search" class="text-base hover:text-primary-600 dark:hover:text-primary-400"
                title="">
                

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


            </button>
            

            
            
            <button id="appearance-switcher-mobile" aria-label="Dark mode switcher" type="button" class="text-base hover:text-primary-600 dark:hover:text-primary-400" style="margin-right:5px">
                <div class="flex items-center justify-center dark:hidden">
                    

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>

  </span>


                </div>
                <div class="items-center justify-center hidden dark:flex">
                    

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>

  </span>


                </div>
            </button>
            

        </div>
    </div>
    <div class="-my-2 -mr-2 md:hidden">

        <label id="menu-button" for="menu-controller" class="block">
            <input type="checkbox" id="menu-controller" class="hidden" />
            
            <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
                

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>

  </span>


            </div>
            <div id="menu-wrapper" style="padding-top:5px;"
                class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50">
                <ul
                    class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl">

                    <li>
                        <span
                            class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>

  </span>

</span>
                    </li>

                    

                    
  <li class="mt-1">
    <a href="/posts/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            Blogs
        </p>
    </a>
</li>



                    

                    
  <li class="mt-1">
    <a href="/series/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            Series
        </p>
    </a>
</li>



                    

                    
  <li class="mt-1">
    <a href="/tags/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            Tags
        </p>
    </a>
</li>



                    

                    
  <li class="mt-1">
    <a href="/about/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            About
        </p>
    </a>
</li>



                    

                </ul>
                
                

            </div>
        </label>
    </div>
</div>





  
  <div class="relative flex flex-col grow">
    <main id="main-content" class="grow">
      


<article>
  

  <header id="single_header" class="mt-5 max-w-prose">
    
    <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
      PTQ for LLM
    </h1>
    <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
      





  
  







  



  



  



  













<div class="flex flex-row flex-wrap items-center">
  
  
  <time datetime="2024-05-28 22:43:31 &#43;0000 UTC">2024-05-28</time><span class="px-2 text-primary-500">&middot;</span><time datetime="2024-06-01 00:00:00 &#43;0000 UTC">Updated: 2024-06-01</time><span class="px-2 text-primary-500">&middot;</span><span>3788 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">8 mins</span>
  

  
  
</div>





<div class="flex flex-row flex-wrap items-center">
  
  
  
  
  
  
  
  
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/nlp/&#34;,'_self');">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    NLP
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/transformer/&#34;,'_self');">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Transformer
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/llm/&#34;,'_self');">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    LLM
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/ai-quantization/&#34;,'_self');">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    AI Quantization
  </span>
</span>
  </span>
  
  
  
  
</div>



    </div>

    
    
    
    
    

    

    
      
      

      

      

    

  </header>
  
  <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
    
     <div
      class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8">
      <div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-10">

         <details open class="toc-right mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block">
  <summary
    class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">
    Table of Contents
  </summary>
  <div
    class="min-w-[220px] py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#ptq">PTQ</a>
      <ul>
        <li><a href="#权重量化">权重量化</a></li>
        <li><a href="#权重和激活全量化">权重和激活全量化</a></li>
      </ul>
    </li>
    <li><a href="#llmint8">LLM.int8()</a></li>
    <li><a href="#gptq">GPTQ</a>
      <ul>
        <li><a href="#使用-autogptq-量化模型">使用 AutoGPTQ 量化模型</a></li>
        <li><a href="#在-transformers-中加载-gptq-模型">在 Transformers 中加载 GPTQ 模型</a></li>
        <li><a href="#在-vllm-中加载-gptq-量化模型">在 vLLM 中加载 GPTQ 量化模型</a></li>
      </ul>
    </li>
    <li><a href="#smoothquant">SmoothQuant</a></li>
    <li><a href="#awq">AWQ</a>
      <ul>
        <li><a href="#autoawq">AutoAWQ</a></li>
      </ul>
    </li>
    <li><a href="#gguf">GGUF</a>
      <ul>
        <li><a href="#加载-gguf-模型">加载 GGUF 模型</a></li>
      </ul>
    </li>
    <li><a href="#总结">总结</a></li>
  </ul>
</nav>
  </div>
</details>
<details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden">
  <summary
    class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">
    Table of Contents
  </summary>
  <div
    class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#ptq">PTQ</a>
      <ul>
        <li><a href="#权重量化">权重量化</a></li>
        <li><a href="#权重和激活全量化">权重和激活全量化</a></li>
      </ul>
    </li>
    <li><a href="#llmint8">LLM.int8()</a></li>
    <li><a href="#gptq">GPTQ</a>
      <ul>
        <li><a href="#使用-autogptq-量化模型">使用 AutoGPTQ 量化模型</a></li>
        <li><a href="#在-transformers-中加载-gptq-模型">在 Transformers 中加载 GPTQ 模型</a></li>
        <li><a href="#在-vllm-中加载-gptq-量化模型">在 vLLM 中加载 GPTQ 量化模型</a></li>
      </ul>
    </li>
    <li><a href="#smoothquant">SmoothQuant</a></li>
    <li><a href="#awq">AWQ</a>
      <ul>
        <li><a href="#autoawq">AutoAWQ</a></li>
      </ul>
    </li>
    <li><a href="#gguf">GGUF</a>
      <ul>
        <li><a href="#加载-gguf-模型">加载 GGUF 模型</a></li>
      </ul>
    </li>
    <li><a href="#总结">总结</a></li>
  </ul>
</nav>
  </div>
</details>


   </div>
      </div>
      

      <div class="min-w-0 min-h-0 max-w-fit">
        
        
<details style="margin-left:0px" class="mt-2 mb-5 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5"  open >
    
    <summary
        class="py-1 text-lg font-semibold cursor-pointer bg-primary-200 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-primary-800 dark:text-neutral-100">
        AI Quantization - This article is part of a series.
    </summary>
    
    
    
    <div
        class="py-1 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600">
        <a href="http://localhost:1313/posts/llm/quantize_datatype/">
            Part 1: DataType in AI
        </a>
    </div>
    
    
    
    <div
        class="py-1 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600">
        <a href="http://localhost:1313/posts/llm/quantize_intro/">
            Part 2: Quantization Introduction
        </a>
    </div>
    
    
    
    <div
        class="py-1 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600">
        Part 3: This Article
    </div>
    
    


</details>



        <div class="article-content max-w-prose mb-20">
          



<h2 class="relative group">PTQ 
    <div id="ptq" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#ptq" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>PTQ(Post-Training Quantization)，即后训练量化，主要目标是无需对 LLM 架构进行修改或重新训练的前提下，通过量化减少 LLM 的存储和计算复杂度。</p>
<p>主要优势是简单高效，但会在量化过程中引入一定程度的精度损失。</p>
<p>PTQ 分为 权重量化 和 全量化。</p>


<h3 class="relative group">权重量化 
    <div id="%E6%9D%83%E9%87%8D%E9%87%8F%E5%8C%96" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%9D%83%E9%87%8D%E9%87%8F%E5%8C%96" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<ul>
<li><a href="https://arxiv.org/abs/2206.09557"   target="_blank">
    LUT-GEMM</a>, 仅对权重进行量化以及使用BCQ格式在LLM中优化矩阵乘法，通过提高计算效率来增强延迟降低和性能</li>
<li><a href="https://arxiv.org/abs/2208.07339"   target="_blank">
    LLM.int8</a>, 采用混合精度分解的量化方法。先做了一个矩阵分解，对绝大部分权重和激活用8bit量化（vector-wise）。对离群特征的几个维度保留16bit，对其做高精度的矩阵乘法</li>
<li><a href="https://arxiv.org/abs/2206.01861"   target="_blank">
    ZeroQuant</a>, 对权重做group-wise，对激活值做token-wise。用逐层知识蒸馏缓解精度损失（原网络做老师），量化后的网络做学生。和W8A8的普通方法做比较，在BERT和GPT3-style模型上精度更好，还能把权重量化到4bit，但加速效果糟糕</li>
<li><a href="https://arxiv.org/abs/2210.17323"   target="_blank">
    GPTQ</a>, 对某个 block 内的所有参数逐个量化，每个参数量化后，需要适当调整这个 block 内其他未量化的参数，以弥补量化造成的精度损失。 <strong>GPTQ 需要准备校准数据集</strong></li>
<li><a href="https://arxiv.org/abs/2306.00978"   target="_blank">
    AWQ</a>, 发现对于LLM的性能，权重并不是同等重要的，通过保留1%的显著权重可以大大减少量化误差。在此基础上，AWQ采用了激活感知方法，考虑与较大激活幅度对应的权重通道的重要性，这在处理重要特征时起着关键作用。该方法采用逐通道缩放技术来确定最佳缩放因子，从而在量化所有权重的同时最小化量化误差</li>
</ul>


<h3 class="relative group">权重和激活全量化 
    <div id="%E6%9D%83%E9%87%8D%E5%92%8C%E6%BF%80%E6%B4%BB%E5%85%A8%E9%87%8F%E5%8C%96" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%9D%83%E9%87%8D%E5%92%8C%E6%BF%80%E6%B4%BB%E5%85%A8%E9%87%8F%E5%8C%96" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>LLM中激活往往由于异常值的存在而变得更加复杂</p>
<ul>
<li><a href="https://arxiv.org/abs/2211.10438"   target="_blank">
    SmoothQuant</a>，观察到不同的token在它们的通道上展示出类似的变化，引入了逐通道缩放变换，有效地平滑了幅度，解决了量化激活的挑战。</li>
</ul>


<h2 class="relative group">LLM.int8() 
    <div id="llmint8" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#llmint8" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>LLM.int8()发现当 LLMs 的模型参数量超过 6.7B 的时候，激活中会成片的出现大幅的离群点(outliers)，朴素且高效的量化方法（W8A8、ZeroQuant等）会导致量化误差增大，精度下降。但是离群特征（Emergent Features）的分布是有规律的，通常分布在 Transformer 层的少数几个维度。针对这个问题，LLM.int8() 采用了混合精度分解计算的方式（离群点和其对应的权重使用 FP16 计算，其他量化成 INT8 后计算）。虽然能确保精度损失较小，但由于需要运行时进行异常值检测、scattering 和 gathering，导致它比 FP16 推理慢。</p>
<p>




  <figure>
    <img class="my-0 rounded-md" loading="lazy" src="https://robot9.me/wp-content/uploads/2023/12/p50.png" alt="llm.int8" />
    
  </figure>

</p>
<p>步骤：</p>
<ul>
<li>从输入的隐含状态中，按列提取异常值 (离群特征，即大于某个阈值的值)。</li>
<li>对离群特征进行 FP16 矩阵运算，对非离群特征进行量化，做 INT8 矩阵运算；</li>
<li>反量化非离群值的矩阵乘结果，并与离群值矩阵乘结果相加，获得最终的 FP16 结果</li>
</ul>
<p>虽然 LLM.in8() 带来的性能下降微乎其微，但是这种分离计算的方式拖慢了推理速度。对于 BLOOM-176B，相比于 FP16，LLM.int8() 慢了大约 15% 到 23%；对于更小的模型（3B 和 11B），速度差距更为明显，LLM.int8() 慢三倍以上</p>
<p>LLM.int8() 的实现主要在 bitsandbytes 库，transformers 库已经集成了 bitsandbytes 这个量化库，其优点是不需要量化校准数据集，任何模型只要含有 torch.nn.Linear 模块，就可以对其进行开箱即用的量化。</p>
<p>8-bit:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;decapoda-research/llama-7b-hf&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="n">device_map</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="n">load_in_8bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="n">max_memory</span><span class="o">=</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">i</span><span class="p">:</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">mem_get_info</span><span class="p">(</span><span class="n">i</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span><span class="o">-</span><span class="mi">2</span><span class="si">}</span><span class="s1">GB&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></div><p>4-bit:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BitsAndBytesConfig</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">nf4_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">   <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">&#34;nf4&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model_nf4</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">nf4_config</span><span class="p">)</span>
</span></span></code></pre></div>

<h2 class="relative group">GPTQ 
    <div id="gptq" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#gptq" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>GPTQ (Generalized Post-Training Quantization)，是一种训练后量化 (PTQ) 方法，采用 INT4/FP16 (W4A16) 的混合量化方案，其中模型权重被量化为 int4，激活值保留在 FP16，是一种仅权重量化方法。通过最小化权重的均方误差（基于近似二阶信息）将所有权重压缩到 INT4。推理时，动态地将权重反量化为 FP16。</p>
<p>GPTQ 将权重分组（如：128列为一组）为多个子矩阵（block）。具体的迭代方案是：对某个 block 内的所有参数逐个量化，每个参数量化后，适当调整这个 block 内其他未量化的参数，以弥补量化造成的精度损失，该算法由90年代的剪枝算法发展而来：</p>
<p>OBD (1990)：引入 H 矩阵进行神经网络剪枝
OBS (1993)：新增权重删除补偿
OBQ (2022)：将 OBS 应用到模型量化，并增加分行计算
GPTQ (2023)：进一步提升量化速度</p>
<p>GPTQ 量化需要准备校准数据集。</p>
<p>GPTQ 把量化问题视作优化问题，逐层寻找最优的量化权重，使用 Cholesky 分解 Hessian 矩阵的逆，在给定的step中对连续列的块进行量化，并在step结束时更新剩余的权重。</p>
<p>优势：</p>
<ul>
<li>int4 量化能够节省接近4倍的内存</li>
<li>主要针对 GPU 推理和性能，对 GPU 进行了优化</li>
<li>不需要对模型进行重训练</li>
</ul>
<p>缺陷：</p>
<ul>
<li>对 GPU 要求较高</li>
<li>量化预训练模型带来量化误差</li>
</ul>
<p>量化和反量化的步骤：</p>
<ul>
<li>缩放：将输入张量x除以缩放因子scale。这一步是为了将x的值范围调整到预期的量化范围</li>
<li>四舍五入：将缩放后的结果四舍五入到最近的整数。这一步将x的值离散化，即将其转换为整数</li>
<li>限制范围：使用torch.clamp函数将四舍五入后的结果限制在0和maxq之间，确保量化后的值不会超出预期的量化范围</li>
<li>反量化：将量化后的张量减去零点zero，然后乘以缩放因子scale。这一步是为了将量化后的值恢复到原始的值范围</li>
</ul>
<p>一般来说，GPTQ推荐使用 8-bit 量化及 groupsize = 128。</p>


<h3 class="relative group">使用 AutoGPTQ 量化模型 
    <div id="%E4%BD%BF%E7%94%A8-autogptq-%E9%87%8F%E5%8C%96%E6%A8%A1%E5%9E%8B" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E4%BD%BF%E7%94%A8-autogptq-%E9%87%8F%E5%8C%96%E6%A8%A1%E5%9E%8B" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p><a href="https://github.com/AutoGPTQ/"   target="_blank">
    AutoGPTQ</a> 是基于 GPTQ 算法、有用户友好型接口的 LLM 量化 toolkit，AutoGPTQ 代码库已被集成到 Transformers 中，可以使用 GPTQ 算法在 8 bit、4 bit、3 bit、2 bit 精度下量化和运行模型</p>
<p><a href="https://github.com/AutoGPTQ/AutoGPTQ?tab=readme-ov-file#installation"   target="_blank">
    安装 GPTQ</a></p>
<pre tabindex="0"><code>git clone https://github.com/AutoGPTQ/AutoGPTQ &amp;&amp; cd AutoGPTQ
pip install -e .
</code></pre><p>构建 GPTQ 量化模型需要使用<strong>训练数据</strong>进行校准。以单卡 GPU 进行量化为例：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">auto_gptq</span> <span class="kn">import</span> <span class="n">AutoGPTQForCausalLM</span><span class="p">,</span> <span class="n">BaseQuantizeConfig</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">logging</span>
</span></span><span class="line"><span class="cl"><span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="nb">format</span><span class="o">=</span><span class="s2">&#34;</span><span class="si">%(asctime)s</span><span class="s2"> </span><span class="si">%(levelname)s</span><span class="s2"> [</span><span class="si">%(name)s</span><span class="s2">] </span><span class="si">%(message)s</span><span class="s2">&#34;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span> <span class="n">datefmt</span><span class="o">=</span><span class="s2">&#34;%Y-%m-</span><span class="si">%d</span><span class="s2"> %H:%M:%S&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model_path</span> <span class="o">=</span> <span class="s2">&#34;model_path&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">quant_path</span> <span class="o">=</span> <span class="s2">&#34;quantized_model_path&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">quantize_config</span> <span class="o">=</span> <span class="n">BaseQuantizeConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">bits</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="c1"># INT4 or INT8</span>
</span></span><span class="line"><span class="cl">    <span class="n">group_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="c1"># 量化 group</span>
</span></span><span class="line"><span class="cl">    <span class="n">damp_percent</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">desc_act</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># set to False can significantly speed up inference but the perplexity may slightly bad</span>
</span></span><span class="line"><span class="cl">    <span class="n">static_groups</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">sym</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">true_sequential</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">model_name_or_path</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">model_file_base_name</span><span class="o">=</span><span class="s2">&#34;model&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">max_len</span> <span class="o">=</span> <span class="mi">8192</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoGPTQForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">quantize_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 使用训练数据进行校准</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 样本的数据类型应该为 List[Dict]，其中字典的键有且仅有 input_ids 和 attention_mask</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="p">[{</span><span class="n">input_ids</span><span class="p">:</span><span class="s2">&#34;&#34;</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">:</span> <span class="s2">&#34;&#34;</span><span class="p">}]</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">cache_examples_on_gpu</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 保存模型, 不支持模型分片</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">save_quantized</span><span class="p">(</span><span class="n">quant_path</span><span class="p">,</span> <span class="n">use_safetensors</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">quant_path</span><span class="p">)</span>
</span></span></code></pre></div><p>如果使用多个 GPU，需要配置 使用 max_memory 而不是 device_map：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoGPTQForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">model_path</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">quantize_config</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">max_memory</span><span class="o">=</span><span class="p">{</span><span class="n">i</span><span class="p">:</span><span class="s2">&#34;20GB&#34;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)}</span> <span class="c1"># 每个 GPU 的内存配置</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></div>

<h3 class="relative group">在 Transformers 中加载 GPTQ 模型 
    <div id="%E5%9C%A8-transformers-%E4%B8%AD%E5%8A%A0%E8%BD%BD-gptq-%E6%A8%A1%E5%9E%8B" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%9C%A8-transformers-%E4%B8%AD%E5%8A%A0%E8%BD%BD-gptq-%E6%A8%A1%E5%9E%8B" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>Transformers, optimum, peft 已支持 AutoGPTQ，可以直接在 Transformers 中使用量化后的模型。以 Qwen1.5-7B-Chat-GPTQ-Int8 为例：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
</span></span><span class="line"><span class="cl"><span class="n">device</span> <span class="o">=</span> <span class="s2">&#34;cuda&#34;</span> <span class="c1"># the device to load the model onto</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;Qwen/Qwen1.5-7B-Chat-GPTQ-Int8&#34;</span><span class="p">,</span> <span class="c1"># the quantized model</span>
</span></span><span class="line"><span class="cl">    <span class="n">device_map</span><span class="o">=</span><span class="s2">&#34;auto&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&#34;Qwen/Qwen1.5-7B-Chat-GPTQ-Int8&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">prompt</span> <span class="o">=</span> <span class="s2">&#34;What is AI?&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="s2">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;system&#34;</span><span class="p">,</span> <span class="s2">&#34;content&#34;</span><span class="p">:</span> <span class="s2">&#34;You are a helpful assistant.&#34;</span><span class="p">},</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="s2">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;user&#34;</span><span class="p">,</span> <span class="s2">&#34;content&#34;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">messages</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="n">text</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&#34;pt&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">model_inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">generated_ids</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="n">output_ids</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">):]</span> <span class="k">for</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">output_ids</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">model_inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">generated_ids</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">response</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span></span></code></pre></div>

<h3 class="relative group">在 vLLM 中加载 GPTQ 量化模型 
    <div id="%E5%9C%A8-vllm-%E4%B8%AD%E5%8A%A0%E8%BD%BD-gptq-%E9%87%8F%E5%8C%96%E6%A8%A1%E5%9E%8B" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%9C%A8-vllm-%E4%B8%AD%E5%8A%A0%E8%BD%BD-gptq-%E9%87%8F%E5%8C%96%E6%A8%A1%E5%9E%8B" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p><code>python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen1.5-7B-Chat-GPTQ-Int8</code></p>


<h2 class="relative group">SmoothQuant 
    <div id="smoothquant" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#smoothquant" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>LLM 量化的挑战之一是激活值比权重更难量化，因为权重数据分布一般比较均匀，而激活的异常值多且大让激活值量化变得更艰难，但是异常值只存在少数通道。单一 token 方差很大（异常值会存在于每一个 token 中），单一 channel 方差会小很多。</p>
<p>SmoothQuant 是一种同时确保准确率且推理高效的训练后量化 (PTQ) 方法，可实现 8bit 权重量化（W8A16）、8bit 全量化(W8A8)。核心思想是缩小激活，放大权重，使得激活更容易量化，通常来说由于各类 Norm 的存在，激活的波动范围会远大于权重，因此 SmoothQuant 从激活的参数中提取一个缩放系数，再乘到权重中，结果不变但压缩了激活的变换范围，从而减少了量化误差。它引入平滑因子 s 来平滑激活的异常值，通过数学等效变换将量化难度从激活转移到权重上。</p>
<p>SmoothQuant 对激活进行平滑，按通道（列）除以 smoothing factor，同时为了保持 liner layer 数学上的等价性，以相反的方式对权重进行对应调整。</p>
<p>




  <figure>
    <img class="my-0 rounded-md" loading="lazy" src="https://robot9.me/wp-content/uploads/2023/12/p51.jpg" alt="smoothquant1" />
    
  </figure>

</p>
<p>




  <figure>
    <img class="my-0 rounded-md" loading="lazy" src="https://robot9.me/wp-content/uploads/2023/12/p52.jpg" alt="smoothquant2" />
    
  </figure>

</p>
<p>SmoothQuant 证明自己可以无损地量化（8bit）所有超过100B参数的开源LLM。通过集成到PyTorch和FasterTransformer中，与 FP16 相比，获得高达1.56倍的推理加速，并将内存占用减半，并且模型越大，加速效果越明显。</p>
<p>与其他量化方法相比，SmoothQuant 可以保持较高的精度，同时具有更低的延迟。</p>
<p>目前，SmoothQuant 已经被集成到 TensorRT-LLM(NVIDIA) 和 Neural-Compressor(Intel) 中。</p>


<h2 class="relative group">AWQ 
    <div id="awq" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#awq" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>AWQ(Activation-aware Weight Quantization), 即激活感知权重量化，是一种硬件友好的、低比特权重量化方法，同时支持 CPU、GPU。</p>
<p>AWQ 源于一个观察，即权重对于LLM的性能并不同等重要：存在约（0.1%-1%）的显著权重（salient weight）对大模型性能影响很大，<strong>跳过这1%的显著权重</strong>（不量化），可大大减少量化误差。</p>
<p>AWQ 通过观察<strong>激活分布</strong>而非权重分布来寻找保护显著权重的最佳每通道缩放比例（per-channel），在量化过程中会对特殊权重进行特殊处理以减轻量化过程中的精度损失，在和GPTQ量化保持类似推理速度的同时可以具备更好的精度。</p>
<p>除了官方支持<a href="https://github.com/mit-han-lab/llm-awq"   target="_blank">
    llm-awq</a>以外，AutoAWQ、vLLM、HuggingFace TGI、LMDeploy、TensorRT-LLM、FastChat 等都支持 AWQ</p>


<h3 class="relative group">AutoAWQ 
    <div id="autoawq" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#autoawq" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>基于 AWQ 的 量化工具包，与 FP16 相比，AutoAWQ 使用4bit量化将模型速度提高了 3 倍，并将对内存需求降低了 3 倍。Transformers 已经集成了 AutoAWQ</p>
<p>使用autoawq量化模型</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">awq</span> <span class="kn">import</span> <span class="n">AutoAWQForCausalLM</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
</span></span><span class="line"><span class="cl"><span class="n">model_path</span> <span class="o">=</span> <span class="s2">&#34;facebook/opt-125m&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">quant_path</span> <span class="o">=</span> <span class="s2">&#34;opt-125m-awq&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">quant_config</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;zero_point&#34;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;q_group_size&#34;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;w_bit&#34;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;version&#34;</span><span class="p">:</span><span class="s2">&#34;GEMM&#34;</span><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Load model</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoAWQForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Quantize</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">quant_config</span><span class="o">=</span><span class="n">quant_config</span><span class="p">)</span>
</span></span></code></pre></div><p>加载 awq 模型</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
</span></span><span class="line"><span class="cl"><span class="n">quant_path</span><span class="o">=</span><span class="s2">&#34;opt-125m-awq&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">quant_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">quant_path</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">text</span> <span class="o">=</span> <span class="s2">&#34;What is AI?&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&#34;pt&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</span></span></code></pre></div>

<h2 class="relative group">GGUF 
    <div id="gguf" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#gguf" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>GGUF(GPT-Generated Unified Format)，以前称为 GGML(General Matrix Multiply Library)，GGUF格式较新，可以保留模型版本等其他自定义信息。这两种格式也是PTQ形式的量化算法。允许用户使用 CPU 来运行 LLM，它专注于优化矩阵乘，以提高量化后的计算效率，适用于在资源受限的设备。</p>


<h3 class="relative group">加载 GGUF 模型 
    <div id="%E5%8A%A0%E8%BD%BD-gguf-%E6%A8%A1%E5%9E%8B" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%8A%A0%E8%BD%BD-gguf-%E6%A8%A1%E5%9E%8B" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p><code>pip install ctransformers[cuda]</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">ctransformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">pipeline</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Use `gpu_layers` to specify how many layers will be offloaded to the GPU.</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;TheBloke/zephyr-7B-beta-GGUF&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">model_file</span><span class="o">=</span><span class="s2">&#34;zephyr-7b-beta.Q4_K_M.gguf&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">model_type</span><span class="o">=</span><span class="s2">&#34;mistral&#34;</span><span class="p">,</span> <span class="n">gpu_layers</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">hf</span><span class="o">=</span><span class="kc">True</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;HuggingFaceH4/zephyr-7b-beta&#34;</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="kc">True</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a pipeline</span>
</span></span><span class="line"><span class="cl"><span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s1">&#39;text-generation&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Inference</span>
</span></span><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&#34;generated_text&#34;</span><span class="p">])</span>
</span></span></code></pre></div>

<h2 class="relative group">总结 
    <div id="%E6%80%BB%E7%BB%93" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%80%BB%E7%BB%93" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Type</th>
<th>Need Dataset</th>
<th>Scale 粒度</th>
<th>DateType</th>
<th>Hardware</th>
</tr>
</thead>
<tbody>
<tr>
<td>LLM.int8()</td>
<td>PTQ</td>
<td>No</td>
<td>per-channel</td>
<td>8bit,4bit</td>
<td>GPU</td>
</tr>
<tr>
<td>GPTQ</td>
<td>PTQ</td>
<td>Yes</td>
<td>per-group</td>
<td>8bit,4bit</td>
<td>GPU</td>
</tr>
<tr>
<td>SmothQuant</td>
<td>PTQ</td>
<td>No</td>
<td>per-tensor, per-token</td>
<td>8bit</td>
<td>GPU</td>
</tr>
<tr>
<td>AWQ</td>
<td>PTQ</td>
<td>No</td>
<td>per-channel</td>
<td>4bit</td>
<td>GPU,CPU</td>
</tr>
</tbody>
</table>
<p>Reference</p>
<ul>
<li><a href="https://arxiv.org/abs/2210.17323"   target="_blank">
    GPTQ</a></li>
<li><a href="https://github.com/AutoGPTQ"   target="_blank">
    AutoGPTQ</a></li>
<li><a href="https://github.com/mit-han-lab/smoothquant"   target="_blank">
    smoothquant</a></li>
<li><a href="https://arxiv.org/abs/2306.00978"   target="_blank">
    AWQ</a></li>
<li><a href="https://www.maartengrootendorst.com/blog/quantization/"   target="_blank">
    Which Quantization Method is Right for You? (GPTQ vs. GGUF vs. AWQ)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/680212402"   target="_blank">
    大模型量化技术原理-LLM.int8,GPTQ</a></li>
<li><a href="https://juejin.cn/post/7330079146515611687"   target="_blank">
    大模型量化技术原理-SmoothQuant</a></li>
<li><a href="https://www.cnblogs.com/ting1/p/18217395"   target="_blank">
    量化技术解析</a></li>
<li><a href="https://developer.aliyun.com/article/1376963"   target="_blank">
    大语言模型量化方法对比：GPTQ、GGUF、AWQ</a></li>
</ul>

        </div>
        
        

        
<details style="margin-left:0px" class="mt-2 mb-5 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5">
    
    <summary
        class="py-1 text-lg font-semibold cursor-pointer bg-primary-200 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-primary-800 dark:text-neutral-100">
        AI Quantization - This article is part of a series.
    </summary>
    
    
    
    <div
        class="py-1 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600">
        <a href="http://localhost:1313/posts/llm/quantize_datatype/">
            Part 1: DataType in AI
        </a>
    </div>
    
    
    
    <div
        class="py-1 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600">
        <a href="http://localhost:1313/posts/llm/quantize_intro/">
            Part 2: Quantization Introduction
        </a>
    </div>
    
    
    
    <div
        class="py-1 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600">
        Part 3: This Article
    </div>
    
    


</details>

        

          
      </div>
     
      
      
        
        
          
          
        
      <script>
        var oid = "views_posts\/llm\/quantize_ptq.md"
        var oid_likes = "likes_posts\/llm\/quantize_ptq.md"
      </script>
      
      
      
      <script type="text/javascript" src="/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js" integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q&#43;oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script>
  
    </section>
  <footer class="pt-8 max-w-prose print:hidden">

    
  
    
    
    
      
      
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600" />
      <div class="flex justify-between pt-3">
        <span>
          
        </span>
        <span>
          
            <a class="flex text-right group ml-3" href="/posts/llm/llama3/">
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >Implement Llama3 in Python and Quantitative Analysis</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2024-05-28 14:16:06 &#43;0000 UTC">2024-05-28</time>
                  
                </span>
              </span>
              <span
                class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400"
                >&rarr;</span
              >
              <span
                class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400"
                >&larr;</span
              >
            </a>
          
        </span>
      </div>
    </div>
  


    
  </footer>
</article>

      <div id="top-scroller" class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0">
  <a href="#the-top"
    class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="Scroll to top" title="Scroll to top">
    &uarr;
  </a>
</div>
    </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
  
  <div class="flex items-center justify-between">

    
    
    <p class="text-sm text-neutral-500 dark:text-neutral-400">
      &copy;
      2024
      Victor Yang
    </p>
    

    
    
    <p class="text-xs text-neutral-500 dark:text-neutral-400">
      
      
      Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
        href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
        href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
    </p>
    

  </div>
  <script>
    
    mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
      margin: 24,
      background: 'rgba(0,0,0,0.5)',
      scrollOffset: 0,
    })
    
  </script>
  
  
  <script type="text/javascript" src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js" integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer><div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]"
  data-url="http://localhost:1313/"
  style="z-index:500"
>
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"
  >
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0"
        />
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)"
      >
        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>

  </span>


      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

  </div>
</body>

</html>
