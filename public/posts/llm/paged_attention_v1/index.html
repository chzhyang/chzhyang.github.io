<!DOCTYPE html>
<html lang="en" dir="ltr" class="scroll-smooth" data-default-appearance="light"
  data-auto-appearance="true"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  
  <meta http-equiv="content-language" content="en" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  
  <title>Paged Attention V1(vLLM) &middot; Victor&#39;s Blog</title>
  <meta name="title" content="Paged Attention V1(vLLM) &middot; Victor&#39;s Blog" />
  
  
  <meta name="keywords" content="NLP, Transformer, LLM, vLLM, Paged Attention, " />
  
  
  <link rel="canonical" href="http://localhost:1313/posts/llm/paged_attention_v1/" />
  
  
  
  
  
  
  
  
  
  
  <link type="text/css" rel="stylesheet" href="/css/main.bundle.min.ac59a1b8eaafef739c129d12787ac29f6a420bcb348c7aeec8949a8d0b7d6c2023766c169d14b9031b751504eb3c976efe786fec135b340a49868acd8caa4127.css"
    integrity="sha512-rFmhuOqv73OcEp0SeHrCn2pCC8s0jHruyJSajQt9bCAjdmwWnRS5Axt1FQTrPJdu/nhv7BNbNApJhorNjKpBJw==" />
  
  
  <script type="text/javascript" src="/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js"
    integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj&#43;e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script>
  
  
  
  
  
  
  
  
  
  <script defer type="text/javascript" id="script-bundle" src="/js/main.bundle.min.b36dda9ec4ec11e967fd341e7d57b8c484ac7a39f8f329f7bcf7ce4812569de8607db866a086d4789956ac0b74967e251c9a566d94d469dd28b8787fed4f46f5.js"
    integrity="sha512-s23ansTsEeln/TQefVe4xISsejn48yn3vPfOSBJWnehgfbhmoIbUeJlWrAt0ln4lHJpWbZTUad0ouHh/7U9G9Q==" data-copy="" data-copied=""></script>
  
  
  <script src="/js/zoom.min.js"></script>
  
  
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
  <link rel="manifest" href="/site.webmanifest" />
  
  
  
  
  
  
  
  <meta property="og:url" content="http://localhost:1313/posts/llm/paged_attention_v1/">
  <meta property="og:site_name" content="Victor&#39;s Blog">
  <meta property="og:title" content="Paged Attention V1(vLLM)">
  <meta property="og:description" content="vLLM # vLLM是吞吐性能卓越的大模型推理框">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-20T15:15:49+00:00">
    <meta property="article:modified_time" content="2024-05-24T00:00:00+00:00">
    <meta property="article:tag" content="NLP">
    <meta property="article:tag" content="Transformer">
    <meta property="article:tag" content="LLM">
    <meta property="article:tag" content="Vllm">
    <meta property="article:tag" content="Paged Attention">
      <meta property="og:see_also" content="http://localhost:1313/posts/llm/flash_attention_2/">
      <meta property="og:see_also" content="http://localhost:1313/posts/llm/flash_attention/">
      <meta property="og:see_also" content="http://localhost:1313/posts/llm/attention/">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Paged Attention V1(vLLM)">
  <meta name="twitter:description" content="vLLM # vLLM是吞吐性能卓越的大模型推理框">

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Blogs",
    "name": "Paged Attention V1(vLLM)",
    "headline": "Paged Attention V1(vLLM)",
    
    "abstract": "vLLM # vLLM是吞吐性能卓越的大模型推理框",
    "inLanguage": "en",
    "url" : "http:\/\/localhost:1313\/posts\/llm\/paged_attention_v1\/",
    "author" : {
      "@type": "Person",
      "name": "Victor Yang"
    },
    "copyrightYear": "2024",
    "dateCreated": "2024-05-20T15:15:49\u002b00:00",
    "datePublished": "2024-05-20T15:15:49\u002b00:00",
    
    "dateModified": "2024-05-24T00:00:00\u002b00:00",
    
    "keywords": ["NLP","Transformer","LLM","vLLM","Paged Attention"],
    
    "mainEntityOfPage": "true",
    "wordCount": "4685"
  }]
  </script>


  
  
  <meta name="author" content="Victor Yang" />
  
  
  
  <link href="https://github.com/chzhyang" rel="me" />
  
  
  <link href="https://www.linkedin.com/in/victor-yang-0101x" rel="me" />
  
  
  <link href="mailto:chzhyangchn@gmail.com" rel="me" />
  
  
  
  

<script src="/lib/jquery/jquery.slim.min.js" integrity=""></script>











<link type="text/css" rel="stylesheet" href="/lib/katex/katex.min.af6f85907cdfd6ed3e4906d93a8233d91c5638859f744cd1b98c9b1a3ccaab5f231bdcda49f132f6cdfe8cca86ff33ed468affab8a9a502610a664a34f0f0cfa.css" integrity="sha512-r2&#43;FkHzf1u0&#43;SQbZOoIz2RxWOIWfdEzRuYybGjzKq18jG9zaSfEy9s3&#43;jMqG/zPtRor/q4qaUCYQpmSjTw8M&#43;g==" />


<script defer src="/lib/katex/katex.min.20da6cf7343619410c0900fbc626506c65159ea9f312f9729d5cba7aa713707378f9a4222e8f7fb9a42a7240e9749f199b7334401b3e3e4b60e29cf490492552.js" integrity="sha512-INps9zQ2GUEMCQD7xiZQbGUVnqnzEvlynVy6eqcTcHN4&#43;aQiLo9/uaQqckDpdJ8Zm3M0QBs&#43;Pktg4pz0kEklUg=="></script>


<script defer src="/lib/katex/auto-render.min.6095714e3aadb63b14ddc4af69346ab12974c1b460654345f8d1860a0b68fcc51b22f68b757433193090bb80afc8965b65cb607e5541d0f5f0f4b2e64d69b9ff.js" integrity="sha512-YJVxTjqttjsU3cSvaTRqsSl0wbRgZUNF&#43;NGGCgto/MUbIvaLdXQzGTCQu4CvyJZbZctgflVB0PXw9LLmTWm5/w=="
  onload="renderMathInElement(document.body);"></script>







































































































































  
  


  
  
  <meta name="theme-color"/>
  
  
</head>
<body
  class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32">
  <div id="the-top" class="absolute flex self-center">
    <a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
      href="#main-content"><span
        class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a>
  </div>
  
  
  <div style="padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px"
    class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3">
    
    <div class="flex flex-1 items-center justify-between">
        <nav class="flex space-x-3">

            
            <a href="/" class="text-base font-medium text-gray-500 hover:text-gray-900">Victor&rsquo;s Blog</a>
            

        </nav>
        <nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12">

            
            
            
  <a href="/posts/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
    
    <p class="text-base font-medium" title="">
        Blogs
    </p>
</a>


            
            
  <a href="/series/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
    
    <p class="text-base font-medium" title="">
        Series
    </p>
</a>


            
            
  <a href="/tags/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
    
    <p class="text-base font-medium" title="">
        Tags
    </p>
</a>


            
            
  <a href="/about/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
    
    <p class="text-base font-medium" title="">
        About
    </p>
</a>


            
            

            


            
            <button id="search-button" aria-label="Search" class="text-base hover:text-primary-600 dark:hover:text-primary-400"
                title="">
                

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


            </button>
            


            
            
            <div
                class="ltr:mr-14 rtl:ml-14 flex items-center">
                <button id="appearance-switcher" aria-label="Dark mode switcher" type="button" class="text-base hover:text-primary-600 dark:hover:text-primary-400">
                    <div class="flex items-center justify-center dark:hidden">
                        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>

  </span>


                    </div>
                    <div class="items-center justify-center hidden dark:flex">
                        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>

  </span>


                    </div>
                </button>
            </div>
            

        </nav>
        <div class="flex md:hidden items-center space-x-5 md:ml-12 h-12">

            <span></span>

            


            
            <button id="search-button-mobile" aria-label="Search" class="text-base hover:text-primary-600 dark:hover:text-primary-400"
                title="">
                

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


            </button>
            

            
            
            <button id="appearance-switcher-mobile" aria-label="Dark mode switcher" type="button" class="text-base hover:text-primary-600 dark:hover:text-primary-400" style="margin-right:5px">
                <div class="flex items-center justify-center dark:hidden">
                    

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>

  </span>


                </div>
                <div class="items-center justify-center hidden dark:flex">
                    

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>

  </span>


                </div>
            </button>
            

        </div>
    </div>
    <div class="-my-2 -mr-2 md:hidden">

        <label id="menu-button" for="menu-controller" class="block">
            <input type="checkbox" id="menu-controller" class="hidden" />
            
            <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
                

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>

  </span>


            </div>
            <div id="menu-wrapper" style="padding-top:5px;"
                class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50">
                <ul
                    class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl">

                    <li>
                        <span
                            class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>

  </span>

</span>
                    </li>

                    

                    
  <li class="mt-1">
    <a href="/posts/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            Blogs
        </p>
    </a>
</li>



                    

                    
  <li class="mt-1">
    <a href="/series/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            Series
        </p>
    </a>
</li>



                    

                    
  <li class="mt-1">
    <a href="/tags/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            Tags
        </p>
    </a>
</li>



                    

                    
  <li class="mt-1">
    <a href="/about/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            About
        </p>
    </a>
</li>



                    

                </ul>
                
                

            </div>
        </label>
    </div>
</div>





  
  <div class="relative flex flex-col grow">
    <main id="main-content" class="grow">
      


<article>
  

  <header id="single_header" class="mt-5 max-w-prose">
    
    <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
      Paged Attention V1(vLLM)
    </h1>
    <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
      





  
  







  



  



  



  













<div class="flex flex-row flex-wrap items-center">
  
  
  <time datetime="2024-05-20 15:15:49 &#43;0000 UTC">2024-05-20</time><span class="px-2 text-primary-500">&middot;</span><time datetime="2024-05-24 00:00:00 &#43;0000 UTC">Updated: 2024-05-24</time><span class="px-2 text-primary-500">&middot;</span><span>4685 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">10 mins</span>
  

  
  
</div>





<div class="flex flex-row flex-wrap items-center">
  
  
  
  
  
  
  
  
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/nlp/&#34;,'_self');">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    NLP
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/transformer/&#34;,'_self');">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Transformer
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/llm/&#34;,'_self');">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    LLM
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/vllm/&#34;,'_self');">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Vllm
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/paged-attention/&#34;,'_self');">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Paged Attention
  </span>
</span>
  </span>
  
  
  
  
</div>



    </div>

    
    
    
    
    

    

    
      
      

      

      

    

  </header>
  
  <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
    
     <div
      class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8">
      <div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-10">

         <details open class="toc-right mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block">
  <summary
    class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">
    Table of Contents
  </summary>
  <div
    class="min-w-[220px] py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#vllm">vLLM</a></li>
    <li><a href="#paged-attentionpa">Paged Attention(PA)</a></li>
    <li><a href="#paged-attention-v1">Paged Attention V1</a></li>
    <li><a href="#paged-attention-v1-cuda-kernelvllm">Paged Attention V1 CUDA Kernel(vLLM)</a></li>
    <li><a href="#pa-v1-和-flash-attention-的区别">PA V1 和 Flash Attention 的区别</a></li>
    <li><a href="#pa-v1-的缺陷">PA V1 的缺陷</a></li>
  </ul>
</nav>
  </div>
</details>
<details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden">
  <summary
    class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">
    Table of Contents
  </summary>
  <div
    class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#vllm">vLLM</a></li>
    <li><a href="#paged-attentionpa">Paged Attention(PA)</a></li>
    <li><a href="#paged-attention-v1">Paged Attention V1</a></li>
    <li><a href="#paged-attention-v1-cuda-kernelvllm">Paged Attention V1 CUDA Kernel(vLLM)</a></li>
    <li><a href="#pa-v1-和-flash-attention-的区别">PA V1 和 Flash Attention 的区别</a></li>
    <li><a href="#pa-v1-的缺陷">PA V1 的缺陷</a></li>
  </ul>
</nav>
  </div>
</details>


   </div>
      </div>
      

      <div class="min-w-0 min-h-0 max-w-fit">
        
        
<details style="margin-left:0px" class="mt-2 mb-5 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5"  open >
    
    <summary
        class="py-1 text-lg font-semibold cursor-pointer bg-primary-200 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-primary-800 dark:text-neutral-100">
        vLLM - This article is part of a series.
    </summary>
    
    
    
    <div
        class="py-1 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600">
        Part [1 4]: This Article
    </div>
    
    


</details>



        <div class="article-content max-w-prose mb-20">
          



<h2 class="relative group">vLLM 
    <div id="vllm" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#vllm" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>vLLM是吞吐性能卓越的大模型推理框架，PagedAttention是vLLM最大的创新点： <a href="https://link.zhihu.com/?target=https%3A//dl.acm.org/doi/abs/10.1145/3600006.3613165"   target="_blank">
    Efficient Memory Management for Large Language Model Serving with PagedAttention</a></p>
<p>vLLM中的attention计算，在推理的prefill阶段, 使用第三方库xformers的优化实现，decoding阶段使用 CUDA kernel 实现(csrc/attention/attention_kernels.cu，大约800多行)。</p>
<p>Attention计算时使用页式管理 KV Cache 来提高内存利用率，进而提高吞吐量。</p>


<h2 class="relative group">Paged Attention(PA) 
    <div id="paged-attentionpa" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#paged-attentionpa" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>vLLM中有两个版本的 PA，其中：</p>
<ul>
<li>V1 源于 FasterTransformers 的 MHA，适用于 len(seq) &lt; 8192 或 num_seqs * num_heads &gt; 512 的情况。</li>
<li>V2 参考了 Flash Decoding方式，对 sequence 的维度进行切分来增加并行粒度</li>
</ul>


<h2 class="relative group">Paged Attention V1 
    <div id="paged-attention-v1" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#paged-attention-v1" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>Block table in PA</p>
<p>




  <figure>
    <img class="my-0 rounded-md" loading="lazy" src="https://blog.vllm.ai/assets/figures/annimation1.gif" alt="Example generation process for a request with PagedAttention" />
    
  </figure>

</p>
<p>一个 req 中包含多个 seq 时，可以共享blocks</p>
<p>




  <figure>
    <img class="my-0 rounded-md" loading="lazy" src="https://blog.vllm.ai/assets/figures/annimation3.gif" alt="Example generation process for a request that samples multiple outputs" />
    
  </figure>

</p>


<h2 class="relative group">Paged Attention V1 CUDA Kernel(vLLM) 
    <div id="paged-attention-v1-cuda-kernelvllm" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#paged-attention-v1-cuda-kernelvllm" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p><a href="https://github1s.com/vllm-project/vllm/blob/main/csrc/attention/attention_kernels.cu#L90-L91"   target="_blank">
    csrc/attention/attention_kernels.cu</a></p>
<p>single_query attention 函数</p>
<p>Dispatch逻辑：</p>
<ul>
<li>CALL_KERNEL_LAUNCHER_BLOCK_SIZE 根据存储的kv blocksize进行派发，分别是 8， 16， 32</li>
<li>LAUNCH_ATTENTION_KERNEL 根据注意力头大小HEADSIZE静态派发</li>
</ul>
<p>并行任务的划分：</p>
<ul>
<li>dim3 grid(num_heads, num_seqs， 1)</li>
<li>dim3 block(NUM_THREADS), 线程数是128，每个 block 负责完成 output 矩阵一行（head_size个元素）结果的 attention 计算</li>
<li>block 的线程划分为若干个 Warp, 每个 Warp 的32个线程划分为 blk_size 个 thread group</li>
</ul>
<p>Kernel 输入参数</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">out</span><span class="p">[</span><span class="n">num_seqs</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_size</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">q</span><span class="p">[</span><span class="n">num_seqs</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_size</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">k_cache</span><span class="p">[</span><span class="n">num_blocks</span><span class="p">,</span> <span class="n">num_kv_heads</span><span class="p">,</span> <span class="n">head_size</span><span class="o">/</span><span class="n">x</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">x</span><span class="p">]</span> <span class="c1"># x表示一个向量化的大小，如float16 -&gt; 16 / sizeof(float16) = 8</span>
</span></span><span class="line"><span class="cl"><span class="n">v_cache</span><span class="p">[</span><span class="n">num_blocks</span><span class="p">,</span> <span class="n">num_kv_heads</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">block_size</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">head_mapping</span><span class="p">[</span><span class="n">num_heads</span><span class="p">]</span> <span class="c1"># 使用MQA, GQA时的kv_head</span>
</span></span><span class="line"><span class="cl"><span class="n">block_tables</span><span class="p">[</span><span class="n">num_seqs</span><span class="p">,</span> <span class="n">max_num_blocks_per_seq</span><span class="p">]</span> <span class="c1"># 维护各个Q对应KVCache的哪些block</span>
</span></span><span class="line"><span class="cl"><span class="n">context_lens</span><span class="p">[</span><span class="n">num_seqs</span><span class="p">]</span> <span class="c1"># 用于变长</span>
</span></span></code></pre></div><p>num_head： Q 的 head 数
num_kv_heads：K, V 的 head 数，MHA 的 num_kv_heads = num_head，GQA、MQA 的 num_kv_heads &lt; num_head
blk_size # block_size，每个page block存储的元素数量，每个page存(blk_size, num_head，head_size)个K、V的元素</p>
<p>Kernel 的常量定义：</p>
<ul>
<li>THREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1) 通过WARPSIZE / BLOCKSIZE 得到一个thread_group大小。注意这里的BLOCKSIZE不是cuda blocksize，而是一个kv block的大小(默认值16)</li>
<li>NUM_TOKENS_PER_THREAD_GROUP = (BLOCK_SIZE + WARP_SIZE - 1) / - WARP_SIZE 表示每个thread_group处理多少个token</li>
<li>NUM_WARPS 表示一个threadblock有多少个warp</li>
<li>VEC_SIZE 表示向量化大小，保证每个thread_group一次性获取16bytes，MAX(16 / (THREAD_GROUP_SIZE * sizeof(scalar_t)), 1)</li>
<li>NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE 表示每个thread要负责多少个数据计算</li>
<li>NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE 表示每个thread负责的数据经过向量化后，一共有多少个vec</li>
<li>V_VEC_SIZE = MIN(16 / sizeof(scalar_t), BLOCK_SIZE) 每个thread一次性读取16bytes</li>
<li>NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE。对于v_cache[head_size, block_size]，表示一行需要几个V_VEC</li>
<li>NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW 表示一个warp可以处理多少行</li>
<li>NUM_ROWS_PER_THREAD 表示每个thread需要负责多少行</li>
</ul>
<p>Kernel 代码逻辑：</p>
<p>（1）循环从显存读取\(Q\)到 shared memory：</p>
<p>迭代读取，每 CUDA block 负责读取\(Q\)的一行（head_size 个元素）存入 shared memory。其中，block 的每个 Warp 负责读取 16<em>blk_size 字节的 Q，即每个 thread group 会读取16字节的 Q，16</em>blk_size 字节的 Q 对应 sequence 的一个 head。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="k">const</span> <span class="kt">int</span> <span class="n">thread_group_idx</span> <span class="o">=</span> <span class="n">thread_idx</span> <span class="o">/</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">const</span> <span class="kt">int</span> <span class="n">thread_group_offset</span> <span class="o">=</span> <span class="n">thread_idx</span> <span class="o">%</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Load the query to registers.
</span></span></span><span class="line"><span class="cl"><span class="c1">// Each thread in a thread group has a different part of the query.
</span></span></span><span class="line"><span class="cl"><span class="c1">// For example, if the the thread group size is 4, then the first thread in
</span></span></span><span class="line"><span class="cl"><span class="c1">// the group has 0, 4, 8, ... th vectors of the query, and the second thread
</span></span></span><span class="line"><span class="cl"><span class="c1">// has 1, 5, 9, ... th vectors of the query, and so on. NOTE(woosuk): Because
</span></span></span><span class="line"><span class="cl"><span class="c1">// q is split from a qkv tensor, it may not be contiguous.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="n">scalar_t</span><span class="o">*</span> <span class="n">q_ptr</span> <span class="o">=</span> <span class="n">q</span> <span class="o">+</span> <span class="n">seq_idx</span> <span class="o">*</span> <span class="n">q_stride</span> <span class="o">+</span> <span class="n">head_idx</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">__shared__</span> <span class="n">Q_vec</span> <span class="n">q_vecs</span><span class="p">[</span><span class="n">THREAD_GROUP_SIZE</span><span class="p">][</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_group_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREAD_GROUPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">const</span> <span class="kt">int</span> <span class="n">vec_idx</span> <span class="o">=</span> <span class="n">thread_group_offset</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">q_vecs</span><span class="p">[</span><span class="n">thread_group_offset</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">    <span class="o">*</span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="n">Q_vec</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">q_ptr</span> <span class="o">+</span> <span class="n">vec_idx</span> <span class="o">*</span> <span class="n">VEC_SIZE</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">__syncthreads</span><span class="p">();</span>
</span></span></code></pre></div><p>（2）循环从显存读取\(K\)到 register，并计算QK：</p>
<ul>
<li>每个 seq 包含 cxt_length * num_kv_heads * head_size 个元素</li>
<li>每个 CUDA block 负责计算一个 seq 的一个 head 的 \(QK^T\)， 只需要读取 ctx_length * head_size 个 K 的元素</li>
<li>因为页式内存管理，K 在 ctx_length 维度的存储不连续，以 blk_size 个 token 为粒度分布在不同的内存地址，所以需要根据 Q 的 head_idx 和 seq_idx 访问 block_table 找到 K 的 physical_block_num</li>
<li>K Cache的布局为 [num_blocks, num_kv_heads, head_size/x, block_size, x]， 目的是优化写入 shared memory。Q和K的同一行元素被读入寄存器并进行点乘运算后，结果要写入shared memory。如果一个 Warp 中所有线程都计算 Q、K 同一行数据，会导致写入 shared memory 的同一个位置，这将造成 warp 内不同线程顺序地写入。所以 warp 的线程最好计算 Q和K 的不同行数据。在设计 K 布局时，将 block_size 放在比 head_size 更低的维度。由于warp size大于block_size，我们需要将head_size拆分为head_size/x和x两个维度，借x到最低维度，以确保每个线程读入的数据量和计算量都足够大。最后，每个线程组派一个线程去写入shared memory，这样一个warp有blk_size个线程并行写入shared memory，从而增加了shared memory的访问带宽。这种设计策略是为了实现高效的并行计算和内存访问，以提高整体的计算性能。</li>
<li>读取 K 需要一个循环，循环中每个CUDA block中的所有 warp 依次访问num_blocks 个 page block。每次迭代：
<ul>
<li>每个 warp 负责访问连续的 blk_size 个 KCache 的行数据（blk_size * head_size个元素）。每个 thread group 负责访问 KCache 的一行，将head_size 个元素读入寄存器</li>
<li>寄存器中的Q和K元素进行点乘，结果写入shared memory。一个 CUDA block 的 shared memory 存储了一行 QK^T 的结果，共 ctx_length 个元素</li>
<li>CUDA block 对 shared memory 中元素进行 max，sum 方式 reduction，然后计算得到 softmax 的结果</li>
</ul>
</li>
</ul>
<p>代码步骤：</p>
<ul>
<li>
<p>group是由block大小决定的，当block&gt;32时，每个warp实现了一个group,否则在一个warp中实现多个group</p>
</li>
<li>
<p>每个warp负责计算一个block KCache，而每个block key shape为 [block_size, num_head, head_size]</p>
</li>
<li>
<p>每个thread_group取一个key，即num_head个元素，计算QK dot</p>
</li>
<li>
<p>只有thread_group的第一个thread负责将QK结果写入shared memory</p>
</li>
<li>
<p>head_idx标记GPU BLOCKs，也即每个GPU Blocks计算一个head</p>
</li>
<li>
<p>num_heads标记使用的GPU BLOCKs总数，也即head num</p>
</li>
<li>
<p>seq_idx标记的是第二维GPU BLOCKs， 也即seq的位置</p>
</li>
</ul>
<p>分配red_smem[2*NUM_WARPS]为reduce所用，保留的是warp内的局部最大值。后面计算了qvec的dot结果保存为qk，先在group内reduce计算得到局部最大值，然后在每个warp内reduce计算得到全局最大值为qk_max。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-C++" data-lang="C++"><span class="line"><span class="cl"><span class="c1">// 每个warp负责 blocksize * headsize个元素
</span></span></span><span class="line"><span class="cl"><span class="c1">// block_idx是block cache中的序号（逻辑序号）
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">num_blocks</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// TODO(Zhengzekang)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 定位物理块
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">physical_block_number</span> <span class="o">=</span> <span class="n">block_table</span><span class="p">[</span><span class="n">block_idx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">K_vec</span> <span class="n">k_vecs</span><span class="p">[</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1">// 遍历每个thread_group处理多少个token
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">const</span> <span class="kt">int</span> <span class="n">physical_block_offset</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">            <span class="p">(</span><span class="n">thread_group_idx</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">WARP_SIZE</span><span class="p">)</span> <span class="o">%</span> <span class="n">BLOCK_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">const</span> <span class="kt">int</span> <span class="n">token_idx</span> <span class="o">=</span> <span class="n">block_idx</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">physical_block_offset</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">K_vec</span> <span class="n">k_vecs</span><span class="p">[</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// 遍历每个thread需要处理多少个VEC
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">//  vectorized取到key
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">k_vecs</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">xxxx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// 计算QKdot，里面包含了一个thread_groupsize的WarpReduceSum，
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="kt">float</span> <span class="n">qk</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">Qk_dot</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span> <span class="n">THREAD_GROUP_SIZE</span><span class="o">&gt;::</span><span class="n">dot</span><span class="p">(</span><span class="n">q_vecs</span><span class="p">,</span> <span class="n">k_vecs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// 只有thread_group的第一个thread负责将QK结果写入shared memory
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// 并且维护一个qk_max，用于后续softmax
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span> <span class="p">(</span><span class="n">thread_group_offset</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Store the partial reductions to shared memory.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// NOTE(woosuk): It is required to zero out the masked logits.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="k">const</span> <span class="kt">bool</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">token_idx</span> <span class="o">&gt;=</span> <span class="n">context_len</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">logits</span><span class="p">[</span><span class="n">token_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">?</span> <span class="mf">0.f</span> <span class="o">:</span> <span class="n">qk</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Update the max value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">qk_max</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">?</span> <span class="nl">qk_max</span> <span class="p">:</span> <span class="n">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">qk</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>此时各个thread_group已经完成了自己的qk_dot操作，并且都维护了qk_max。下面就需要和其他thread_group做warp shuffle操作，得到一个warp内的qk max值。</p>
<p>由于每个thread_group里的thread内维护的qk_max是一样的，所以warp shuffle只需到 thread_group_size即可停止。并由lane_id = 0的线程将warp里的qk_max存储到smem，最后再做一次warpreduce，得到一个block里的qkmax值，通过shfl_sync广播操作，让每个线程都拿到max</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">WARP_SIZE</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">mask</span> <span class="o">&gt;=</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span> <span class="n">mask</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">qk_max</span> <span class="o">=</span> <span class="n">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">__shfl_xor_sync</span><span class="p">(</span><span class="kt">uint32_t</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">qk_max</span><span class="p">,</span> <span class="n">mask</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">lane</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">red_smem</span><span class="p">[</span><span class="n">warp_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">qk_max</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// TODO(woosuk): Refactor this part.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// Get the max qk value for the sequence.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">qk_max</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">&lt;</span> <span class="n">NUM_WARPS</span> <span class="o">?</span> <span class="n">red_smem</span><span class="p">[</span><span class="n">lane</span><span class="p">]</span> <span class="o">:</span> <span class="o">-</span><span class="n">FLT_MAX</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">NUM_WARPS</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">mask</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">mask</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">qk_max</span> <span class="o">=</span> <span class="n">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">__shfl_xor_sync</span><span class="p">(</span><span class="kt">uint32_t</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">qk_max</span><span class="p">,</span> <span class="n">mask</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Broadcast the max qk value to all threads.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">qk_max</span> <span class="o">=</span> <span class="n">__shfl_sync</span><span class="p">(</span><span class="kt">uint32_t</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">qk_max</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
</span></span></code></pre></div><p>接下来就是常规的softmax</p>
<p>执行exp(x-qk_max)并得到每个warp上的exp_sum，规约得全局（所有warp）的exp_sum,计算每个节点上的softmax</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">// Get the sum of the exp values.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">float</span> <span class="n">exp_sum</span> <span class="o">=</span> <span class="mf">0.f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">context_len</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREADS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">__expf</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">qk_max</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">exp_sum</span> <span class="o">+=</span> <span class="n">val</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">exp_sum</span> <span class="o">=</span> <span class="n">block_sum</span><span class="o">&lt;</span><span class="n">NUM_WARPS</span><span class="o">&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="n">red_smem</span><span class="p">[</span><span class="n">NUM_WARPS</span><span class="p">],</span> <span class="n">exp_sum</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Compute softmax.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">float</span> <span class="n">inv_sum</span> <span class="o">=</span> <span class="n">__fdividef</span><span class="p">(</span><span class="mf">1.f</span><span class="p">,</span> <span class="n">exp_sum</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="n">f</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">context_len</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREADS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*=</span> <span class="n">inv_sum</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">__syncthreads</span><span class="p">();</span>
</span></span></code></pre></div><p>（3）从显存读取\(V\)到 register, 计算 softmax(QK^T)V</p>
<p>和KCache一样，CUDA block 依次访问 num_blk 个 VCahce block 到寄存器，每个 warp 负责 1 个 VCache block，。不过这里不需要以 thread group 为单位访问16字节，而是每个 thread 读取16字节的元素到寄存器，然后与shared memory的 softmax(QK^T)中间结果 对应位置16字节的数据进行点乘，得到一个 float 结果，写到 output 的对应位置中。</p>
<blockquote>
<p>为了读写连续，将V_cache转置，shape为：[num_blocks, num_kv_heads, head_size, block_size]</p>
</blockquote>
<blockquote>
<p>注意这里使用了fp32模式以防止累加过程中的精度损失</p>
</blockquote>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">// 每个线程一次性读16bytes数据
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">constexpr</span> <span class="kt">int</span> <span class="n">V_VEC_SIZE</span> <span class="o">=</span> <span class="n">MIN</span><span class="p">(</span><span class="mi">16</span> <span class="o">/</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">scalar_t</span><span class="p">),</span> <span class="n">BLOCK_SIZE</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">using</span> <span class="n">V_vec</span> <span class="o">=</span> <span class="k">typename</span> <span class="n">Vec</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span> <span class="n">V_VEC_SIZE</span><span class="o">&gt;::</span><span class="n">Type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">using</span> <span class="n">L_vec</span> <span class="o">=</span> <span class="k">typename</span> <span class="n">Vec</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span> <span class="n">V_VEC_SIZE</span><span class="o">&gt;::</span><span class="n">Type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">using</span> <span class="n">Float_L_vec</span> <span class="o">=</span> <span class="k">typename</span> <span class="n">FloatVec</span><span class="o">&lt;</span><span class="n">L_vec</span><span class="o">&gt;::</span><span class="n">Type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1">// 每一行有多少个V_VEC，假设BLOCK_SIZE=8，那么NUM_V_VECS_PER_ROW=1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">=</span> <span class="n">BLOCK_SIZE</span> <span class="o">/</span> <span class="n">V_VEC_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 一个WARP一次处理多少行，按照上面假设，这里是32
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_ROWS_PER_ITER</span> <span class="o">=</span> <span class="n">WARP_SIZE</span> <span class="o">/</span> <span class="n">NUM_V_VECS_PER_ROW</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 每个thread需要负责多少行，假设headsize=128，那么每个thread要处理4行
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_ROWS_PER_THREAD</span> <span class="o">=</span> <span class="p">(</span><span class="n">HEAD_SIZE</span> <span class="o">+</span> <span class="n">NUM_ROWS_PER_ITER</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">NUM_ROWS_PER_ITER</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// 提前分配accumulate buffer，用float累加
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">float</span> <span class="n">accs</span><span class="p">[</span><span class="n">NUM_ROWS_PER_THREAD</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">num_blocks</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">const</span> <span class="kt">int</span> <span class="n">row_idx</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">/</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">NUM_ROWS_PER_ITER</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="p">(</span><span class="n">row_idx</span> <span class="o">&lt;</span> <span class="n">HEAD_SIZE</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">const</span> <span class="kt">int</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">row_idx</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">physical_block_offset</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">V_vec</span> <span class="n">v_vec</span> <span class="o">=</span> <span class="o">*</span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="n">V_vec</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">v_ptr</span> <span class="o">+</span> <span class="n">offset</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">dot</span><span class="p">(</span><span class="n">logits_vec</span><span class="p">,</span> <span class="n">v_vec</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span></code></pre></div><p>（4）更新最终的结果</p>
<p>将一个block分成上半部分warp和下半部分warp。上半部分warp(warp_id &gt; mid)将自己累加的结果写到shared memory。下半部分warp将之前上半部分warp存到shared_memory 的结果取出，进行累加。这样重复，当warp_idx==0时，将所有结果写回到每一行中。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl">  <span class="c1">// Perform reduction across warps.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">float</span><span class="o">*</span> <span class="n">out_smem</span> <span class="o">=</span> <span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">shared_mem</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">NUM_WARPS</span><span class="p">;</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">mid</span> <span class="o">=</span> <span class="n">i</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Upper warps write to shared memory.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">warp_idx</span> <span class="o">&gt;=</span> <span class="n">mid</span> <span class="o">&amp;&amp;</span> <span class="n">warp_idx</span> <span class="o">&lt;</span> <span class="n">i</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="kt">float</span><span class="o">*</span> <span class="n">dst</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">out_smem</span><span class="p">[(</span><span class="n">warp_idx</span> <span class="o">-</span> <span class="n">mid</span><span class="p">)</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">const</span> <span class="kt">int</span> <span class="n">row_idx</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">/</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">NUM_ROWS_PER_ITER</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">row_idx</span> <span class="o">&lt;</span> <span class="n">HEAD_SIZE</span> <span class="o">&amp;&amp;</span> <span class="n">lane</span> <span class="o">%</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="n">dst</span><span class="p">[</span><span class="n">row_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Lower warps update the output.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">warp_idx</span> <span class="o">&lt;</span> <span class="n">mid</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">src</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">out_smem</span><span class="p">[</span><span class="n">warp_idx</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">const</span> <span class="kt">int</span> <span class="n">row_idx</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">/</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">NUM_ROWS_PER_ITER</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">row_idx</span> <span class="o">&lt;</span> <span class="n">HEAD_SIZE</span> <span class="o">&amp;&amp;</span> <span class="n">lane</span> <span class="o">%</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">src</span><span class="p">[</span><span class="n">row_idx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Write the final output.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">warp_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">scalar_t</span><span class="o">*</span> <span class="n">out_ptr</span> <span class="o">=</span> <span class="n">out</span> <span class="o">+</span> <span class="n">seq_idx</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">*</span> 
</span></span><span class="line"><span class="cl">        <span class="n">max_num_partitions</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span> <span class="o">+</span> <span class="n">head_idx</span> <span class="o">*</span> 
</span></span><span class="line"><span class="cl">        <span class="n">max_num_partitions</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span> <span class="o">+</span> <span class="n">partition_idx</span> <span class="o">*</span> 
</span></span><span class="line"><span class="cl">        <span class="n">HEAD_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">const</span> <span class="kt">int</span> <span class="n">row_idx</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">/</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">NUM_ROWS_PER_ITER</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="p">(</span><span class="n">row_idx</span> <span class="o">&lt;</span> <span class="n">HEAD_SIZE</span> <span class="o">&amp;&amp;</span> <span class="n">lane</span> <span class="o">%</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">from_float</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">out_ptr</span> <span class="o">+</span> <span class="n">row_idx</span><span class="p">),</span> <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span></code></pre></div><blockquote>
<p>为什么 VCache 的 layout 是 [num_blocks, num_kv_heads, head_size, block_size]，和 KCache layout 不一样？ 因为 V 要去做点乘的对象在shared memory，只需要读，不涉及并行写。</p>
</blockquote>


<h2 class="relative group">PA V1 和 Flash Attention 的区别 
    <div id="pa-v1-%E5%92%8C-flash-attention-%E7%9A%84%E5%8C%BA%E5%88%AB" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#pa-v1-%E5%92%8C-flash-attention-%E7%9A%84%E5%8C%BA%E5%88%AB" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>并行任务的划分方式不同</p>
<ul>
<li>FlashAttention 用了两层循环，每次写一个 Tile 的 output tensor，而 PA 只有一层循环，每次写一行 output tensor。因为每次迭代都有整行的 QK^T 中间结果，不需要online softmax</li>
<li>PA V1 设计的 KCache layout 充分利用了 shared memory 写带宽</li>
</ul>


<h2 class="relative group">PA V1 的缺陷 
    <div id="pa-v1-%E7%9A%84%E7%BC%BA%E9%99%B7" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#pa-v1-%E7%9A%84%E7%BC%BA%E9%99%B7" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>不足：</p>
<ul>
<li>不适合 seq 很长的情况，因为没有沿着 ctx_length 或者 batch 维度做切分</li>
<li>和MHA相比，MQA和GAQ没有减少对KV Cache的读写次数。读K、V Cache时候只是做了一个head_idx的转换，会重复从显存读相同的head</li>
</ul>
<p>未完待续&hellip;</p>
<p>Reference:</p>
<ul>
<li><a href="https://www.zhihu.com/question/633412311/answer/3332907958"   target="_blank">
    vllm</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//dl.acm.org/doi/abs/10.1145/3600006.3613165"   target="_blank">
    Efficient Memory Management for Large Language Model Serving with PagedAttention</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/668736097"   target="_blank">
    PageAttention代码走读</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/657114963"   target="_blank">
    vLLM kernel</a></li>
</ul>

        </div>
        
        

        
<details style="margin-left:0px" class="mt-2 mb-5 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5">
    
    <summary
        class="py-1 text-lg font-semibold cursor-pointer bg-primary-200 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-primary-800 dark:text-neutral-100">
        vLLM - This article is part of a series.
    </summary>
    
    
    
    <div
        class="py-1 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600">
        Part [1 4]: This Article
    </div>
    
    


</details>

        

          
      </div>
     
      
      
        
        
          
          
        
      <script>
        var oid = "views_posts\/llm\/paged_attention_v1.md"
        var oid_likes = "likes_posts\/llm\/paged_attention_v1.md"
      </script>
      
      
      
      <script type="text/javascript" src="/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js" integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q&#43;oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script>
  
    </section>
  <footer class="pt-8 max-w-prose print:hidden">

    
  
    
    
    
      
      
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600" />
      <div class="flex justify-between pt-3">
        <span>
          
            <a class="flex group mr-3" href="/posts/llm/flash_attention_2/">
              <span
                class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400"
                >&larr;</span
              >
              <span
                class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400"
                >&rarr;</span
              >
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >Flash Attention V2</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2024-05-23 22:43:31 &#43;0000 UTC">2024-05-23</time>
                  
                </span>
              </span>
            </a>
          
        </span>
        <span>
          
            <a class="flex text-right group ml-3" href="/posts/llm/flash_attention/">
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >Flash Attention</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2024-05-05 22:43:31 &#43;0000 UTC">2024-05-05</time>
                  
                </span>
              </span>
              <span
                class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400"
                >&rarr;</span
              >
              <span
                class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400"
                >&larr;</span
              >
            </a>
          
        </span>
      </div>
    </div>
  


    
  </footer>
</article>

      <div id="top-scroller" class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0">
  <a href="#the-top"
    class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="Scroll to top" title="Scroll to top">
    &uarr;
  </a>
</div>
    </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
  
  <div class="flex items-center justify-between">

    
    
    <p class="text-sm text-neutral-500 dark:text-neutral-400">
      &copy;
      2024
      Victor Yang
    </p>
    

    
    
    <p class="text-xs text-neutral-500 dark:text-neutral-400">
      
      
      Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
        href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
        href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
    </p>
    

  </div>
  <script>
    
    mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
      margin: 24,
      background: 'rgba(0,0,0,0.5)',
      scrollOffset: 0,
    })
    
  </script>
  
  
  <script type="text/javascript" src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js" integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer><div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]"
  data-url="http://localhost:1313/"
  style="z-index:500"
>
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"
  >
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0"
        />
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)"
      >
        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>

  </span>


      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

  </div>
</body>

</html>
